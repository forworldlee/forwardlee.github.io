<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Nginx+keepalived实现高可用]]></title>
    <url>%2F2018%2F09%2F19%2FNginx-keepalived%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[安装openrestyopenresty安装步骤参照这里 安装KeepalivedCentos 6编译安装ipvsadm-1.26 1234567891011121314151617181920212223242526272829[root@vpsback software]# uname -r2.6.32-696.30.1.el6.x86_64yum install -y kernel-devel-2.6.32-696.30.1.el6.x86_64ln -s /usr/src/kernels/2.6.32-696.30.1.el6.x86_64/ /usr/src/linuxyum -y install popt popt-devel libnl libnl-devel popt-static tar xzvf ipvsadm-1.26.tar.gzcd ipvsadm-1.26makemake install[root@vpsback ipvsadm-1.26]# ipvsadm --versionipvsadm v1.26 2008/5/15 (compiled with popt and IPVS v1.2.1)[root@vpsback ipvsadm-1.26]# ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn 安装keepalived软件 12345 tar zxvf keepalived-1.3.5.tar.gz cd ikeepalived-1.3.5 ./configure --prefix=/usr/local/keepalived --with-kernel-dir=/usr/src/kernels/2.6.32-696.30.1.el6.x86_64 makemake install 配置和启动keepalived 12mkdir /etc/keepalivedvi /etc/keepalived/keepalived.conf keepalived.conf 1234567891011121314151617181920212223242526272829303132! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125;vrrp_script chk_nginx &#123; script "/etc/keepalived/check_nginx.sh" interval 2 weight -5 fall 3 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER ##备用keepalived配置为BACKUP interface eth0 virtual_router_id 50 #虚拟路由标示，相同实例，需相同标示。 nopreempt priority 100 #优先级 数字越大 优先级越高 MASTER的优先级高于BACKUP优先级（如master 100,backup 50） advert_int 1 virtual_ipaddress &#123; 192.168.1.230 ##设定虚拟IP地址 可以设置多个 每行一个 &#125;&#125; /etc/keepalived/nginx_check.sh内容如下 123456789#!/bin/bashA=`ps -C nginx –no-header |wc -l`if [ $A -eq 0 ];then /usr/servers/nginx/sbin/nginx sleep 2 if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then killall keepalived fifi 启动keepalived 1/usr/servers/keepalived/sbin/keepalived 查看虚拟IP和真实IP的状态 1234[root@localhost keepalived]# ip a | grep eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 inet 192.168.1.231/24 brd 192.168.1.255 scope global eth0 inet 192.168.1.230/32 scope global eth0 重启keepalived 1/usr/servers/keepalived/sbin/keepalived --signum=reload 通过浏览器访问http://192.168.1.230:81/查看是否请求到了http://192.168.1.231:81/]]></content>
      <tags>
        <tag>Keepalived</tag>
        <tag>OpenResty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis和Zookeeper实现分布式锁的原理对比]]></title>
    <url>%2F2018%2F09%2F07%2FRedis%E5%92%8CZookeeper%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%8E%9F%E7%90%86%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[Redis实现分布式锁获取锁基于Redis内部是单线程执行指令，利用Redis命令支持的setNX EX原子操作指令给某个key赋值并设置过期时间，实现获取分布式锁，成功设置key的值并返回value值作为这把锁的拥有者，表示成功获取到锁。容易犯错的地方在于把setNX EX操作分解成两步setNX和setEX操作，破坏了原子性，有可能导致setEX失败后setNX设置的key永不过期，那么其他线程将永远无法获得这把锁。在获取分布式锁时，如果一次获取锁失败，就允许一定时间范围内的重试获取锁，这样能保证线程获取分布式锁的成功率。在每次获取锁失败后，要让当前线程睡一小会儿，防止短时间内过多的线程争夺分布式锁。 12345678910111213141516171819202122232425262728293031323334@Override public String acquireDistributeLock(String key) &#123; String keyOwner = UUID.randomUUID().toString(); long now = System.currentTimeMillis(); long end = now + 1000; while (now&lt;end)&#123; boolean b = setDistributeLockValue(key, keyOwner); if (b) &#123; break; &#125;else &#123; try &#123; Thread.sleep(20L); &#125; catch (InterruptedException e) &#123; log.error("",e); &#125; &#125; &#125; return keyOwner; &#125; private boolean setDistributeLockValue(final String key,final String keyOwner)&#123; return stringRedisTemplate.execute(new RedisCallback&lt;Boolean&gt;() &#123; @Override public Boolean doInRedis(RedisConnection connection) throws DataAccessException &#123; JedisCommands jedisCommands = ((JedisCommands) connection.getNativeConnection()); String value = jedisCommands.set(key, keyOwner, "NX", "PX", 1000); if ("OK".equals(value)) &#123; return true; &#125;else &#123; return false; &#125; &#125; &#125;); &#125; 释放锁Redis实现释放分布式锁，需要这把锁的拥有者才有权删除key达到释放锁的目的，为了保证释放锁过程指令执行的原子性，使用Redis(&gt;=2.6)的eval指令执行Lua脚本的方式释放锁。redis.call(&#39;get&#39;, KEYS[1])得到key对应的value，ARGV[1]表示传入的keyOwner值，如果value==keyOwner，说明当前线程是分布式锁的拥有者，有权释放锁，则执行redis.call(&#39;del&#39;, KEYS[1])，执行成功返回1，实行失败返回0。 1234567891011121314151617@Override public Boolean releaseDistributeLock(String key,String keyOwner) &#123; return stringRedisTemplate.execute(new RedisCallback&lt;Boolean&gt;() &#123; @Override public Boolean doInRedis(RedisConnection connection) throws DataAccessException &#123; String unlockScript = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end"; Jedis jedis = ((Jedis) connection.getNativeConnection()); Object result = jedis.eval(unlockScript, Collections.singletonList(key), Collections.singletonList(keyOwner)); if (new Long(1L).equals(result)) &#123; return true; &#125; return false; &#125; &#125;); &#125; Zookeeper实现分布式锁获取锁Zookeeper可以创建有生命周期的不同类型节点（持久节点、临时节点、持久顺序节点、临时顺序节点），实现高性能分布式协调和调度。通过Zookeeper同一个临时节点（CreateMode.EPHEMERAL）只能创建一次的原理可以实现分布式锁。跟Redis获取分布式锁的方式相似，在第一次获取分布式锁失败后，进行若干次重试失败后发送预警通知。 123456789101112131415161718192021222324252627282930313233/** * 获取分布式锁 */ public void acquireDistributedLock(String objectId)&#123; try &#123; createNode(objectId); log.info("=======分布式锁创建成功========="); &#125; catch (Exception e) &#123; //如果临时节点已经存在，则会抛出异常，临时创建失败（分布式锁没获取到）,再延时尝试 int count = 0; while(true)&#123; if(count&gt;100)&#123; log.error("重试"+count+"次获取zookeeper分布式锁失败"); break; //send msg 发送预警 &#125; try &#123; Thread.sleep(20); createNode(objectId); &#125; catch (Exception e1) &#123; log.error("重试获取zookeeper分布式锁失败",e1); count++; continue; &#125; log.info("经过"+count+"次尝试，获取到分布式锁["+objectId+"]"); break; &#125; &#125; &#125; private void createNode(String objectId) throws KeeperException, InterruptedException &#123; zooKeeper.create("/"+objectId,"".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); &#125; 释放锁删除创建的临时节点，达到释放锁的目的。 12345678910/** * 释放锁 */ public void deleteDistribituedLock(String objectId)&#123; try &#123; zooKeeper.delete("/"+objectId,-1); &#125; catch (Exception e) &#123; log.info("释放锁出错",e); &#125; &#125; Redis与Zookeeper分布式锁实现对比Redis实现分布式锁，难点在于key的原子性操作，需要借助与redis原生指令创建，需要Redis客户端支持执行原生命令。Redis分布式锁value可以保存锁的创建者，只有创建者才有权释放锁。 Zookeeper实现分布式锁，只需要编写创建和删除分布式锁的临时节点少量代码即可完成，要实现对分布式锁拥有者的管理，可以通过对节点的path格式进行规划，保存锁拥有者信息。]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenResty生产环境反向代理和七层负载均衡配置策略]]></title>
    <url>%2F2018%2F09%2F06%2FOpenResty%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%85%8D%E7%BD%AE%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[Nginx配置手册《Nginx配置手册》看这里 Nginx反向代理缓存配置降低上游服务器压力代理分为正向代理和反向代理，简言之，正向代理是指代理客户端请求，反向代理是指代理服务器端服务。Nginx作为七层负载均衡服务器，主要任务是分发从客户端到达的http请求到上游服务器集群。为了减少到达上游服务器的并发请求数量，可以在Nginx进行反向代理的缓存全局配置。 12345678910111213proxy_buffering on;proxy_buffer_size 4k;proxy_buffers 512 4k;proxy_busy_buffers_size 64k;proxy_temp_file_write_size 256k;proxy_cache_lock on;proxy_cache_lock_timeout 200ms;#/tmpfs为内存文件系统proxy_temp_path /tmpfs/proxy_temp;proxy_cache_path /tmpfs/proxy_cache levels=1:2 keys_zone=cache:512m inactive=5m max_size=8g;proxy_connect_timeout 3s;proxy_read_timeout 5s;proxy_send_timeout 5s; Nginx常用的七层负载均衡配置策略Step1.配置upstream上游服务器和负载均衡算法要实现负载均衡，首先，要配置Nginx反向代理的上游服务器地址，并配置响应的负载均衡算法。Nginx支持的负载均衡算法有：轮询（round-robin）、IP哈希（ip_hash）、特定值哈希（hash $key）、特定值一致性哈希（hash $key consistent）、最少活跃连接（least_conn），商业版Nginx还支持最少平均响应时间（least_time）。其次，Nginx负载均衡支持对每个代理服务器的请求权重（weight=n）、失败重试（max_fails=n）、连接超时（fail_timeout=ns）、备用（backup)、下线（down）、心跳检测的可选配置项，配合负载均衡算法，实现高可用、高性能的负载均衡。 12345678910111213#在nginx配置文件的http模块中配置upstrem模块，并定义一个名字name，在location指向name进行负载均衡。upstream backend &#123; #配置负载均衡算法 hash $consistent_key consistent; #反向代理的上游服务器集群地址 server 192.168.1.210:80 max_fails=2 fail_timeout=30s weight=1; server 192.168.1.211:80 max_fails=2 fail_timeout=30s weight=2; #反向代理的上游服务器集群宕机，连接到备用机提供服务 server 192.168.1.213:80 max_fails=2 fail_timeout=30s weight=3 backup; #心跳检测，interval检测间隔时间(s)，rise被检测到存活的次数即可正常提供服务， #fall被检测到失败的次数标记为服务器不存活，timeout检测请求超时配置 check interval=5000 rise=1 fall=3 timeout=2000 type=tcp;&#125; Step2.使用反向代理开启负载均衡创建一个针对URL的反向代理配置文件/usr/local/common/lua_scripts/hello/hello.conf。 1234567891011121314151617181920212223242526272829#/xxx是需要进行负载均衡访问的pathlocation ~ /xxx/(.*)$ &#123; #backend是配置对上游服务器进行负载均衡算法的名字,$1表示正则表达式(.*)匹配到的内容 #$is_args如果是get请求有参数代表?,并且用$args传送参数值 proxy_pass http://backend/$1$is_args$args; #如果backend需要根据某一个变量参数（$consistent_key)进行负载均衡,则在此处设置；否则，忽略； #OpenResty支持lua脚本，可以通过lua脚本对参数进行计算 set_by_lua_file $consistent_key "/usr/local/common/lua_scripts/hello/lua/lua_balancing.lua"; #失败重试配置 proxy_next_upstream error timeout http_500 http_502 http_504 proxy_next_upstream_timeout 2s; proxy_next_upstream_tries 2; #请求上游服务器的Http Request Method proxy_method GET; #是否向上游服务器器传递请求体 proxy_pass_request_body off; #是否向上游服务器传递请求头 proxy_pass_request_headers off; #设置上游服务器的哪些请求头信息不发送给客户端 proxy_hide_header Vary; proxy_http_version 1.1; #按照需要传递Referer、Cookie、Host proxy_set_header Connection ""; proxy_set_header Referer $http_referer; proxy_set_header Cookie $http_cookie; proxy_set_header Host 192.168.1.212:8080 &#125; Step3.对负载均衡的$key进行计算Nginx添加ngx_http_core模块后，在Nginx配置文件中就可以使用相关的Http参数值参与负载均衡算法的计算。$key变量的值可以是Nginx支持的内建变量有$uri（解码后不带参数）、$request_uri（未解码包含参数）、$arg_XXX（未解码的参数XXX，XXX不区分大小写），还支持Nginx内建的常量。OpenResty除了支持Nginx包含的各模块中的变量和常量，还支持Lua脚本对变量进行计算，生产环境中常用。 在nginx.conf中http模块中使用lua模块提供的功能配置缓存blcache，在lua脚本中可以使用这个缓存： 12345678http &#123; lua_shared_dict blcache 128m; lua_package_path "/usr/local/common/lua_scripts/hello/lualib/?.lua;;"; lua_package_cpath "/usr/local/common/lua_scripts/hello/lualib/?.so;;"; include /usr/local/common/lua_scripts/hello/hello.conf; ... /usr/local/common/lua_scripts/hello/lua/lua_balancing.lua脚本内容如下： 12345678910111213141516local uri_args = ngx.req.get_uri_args()local consistent_key = uri_args.ecourseIdif not consistent_key or consistent_key == '' then consistent_key = ngx.var.request_uriendlocal balancing_cache = ngx.shared.blcachelocal value = balancing_cache:get(consistent_key)if not value then success,err=balancing_cache:set(consistent_key,1,60)else newval,err=balancing_cache:incr(consistent_key,1)endif newval and newval &gt; 5000 then consistent_key = consistent_key..'_'..newvalendreturn consistent_key Nginx内建全局常量：转自Nginx变量使用方法详解 12345678910111213141516171819202122232425262728293031arg_PARAMETER #这个变量包含GET请求中，如果有变量PARAMETER时的值。args #这个变量等于请求行中(GET请求)的参数，如：foo=123&amp;bar=blahblah;binary_remote_addr #二进制的客户地址。body_bytes_sent #响应时送出的body字节数数量。即使连接中断，这个数据也是精确的。content_length #请求头中的Content-length字段。content_type #请求头中的Content-Type字段。cookie_COOKIE #cookie COOKIE变量的值document_root #当前请求在root指令中指定的值。document_uri #与uri相同。host #请求主机头字段，否则为服务器名称。hostname #Set to themachine’s hostname as returned by gethostnamehttp_HEADERis_args #如果有args参数，这个变量等于”?”，否则等于”"，空值。http_user_agent #客户端agent信息http_cookie #客户端cookie信息limit_rate #这个变量可以限制连接速率。query_string #与args相同。request_body_file #客户端请求主体信息的临时文件名。request_method #客户端请求的动作，通常为GET或POST。remote_addr #客户端的IP地址。remote_port #客户端的端口。remote_user #已经经过Auth Basic Module验证的用户名。request_completion #如果请求结束，设置为OK. 当请求未结束或如果该请求不是请求链串的最后一个时，为空(Empty)。request_method #GET或POSTrequest_filename #当前请求的文件路径，由root或alias指令与URI请求生成。request_uri #包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。不能修改。scheme #HTTP方法（如http，https）。server_protocol #请求使用的协议，通常是HTTP/1.0或HTTP/1.1。server_addr #服务器地址，在完成一次系统调用后可以确定这个值。server_name #服务器名称。server_port #请求到达服务器的端口号。]]></content>
      <tags>
        <tag>OpenResty</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA垃圾回收策略与内存泄漏]]></title>
    <url>%2F2018%2F08%2F27%2FJAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AD%96%E7%95%A5%E4%B8%8E%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%2F</url>
    <content type="text"><![CDATA[【转自】https://www.ibm.com/developerworks/cn/java/l-JavaMemoryLeak/index.html 问题背景JVM进行垃圾回收时判断对象是否存活的算法有引用计数算法和可达性分析算法，更多使用中会使用对象可达性分析算法。如果某个对象JVM运行时一直处于GCROOT可达，GC不会回收这个对象，但是一直没有被JVM方法栈所使用，这就可能出现内存泄漏。 问题的提出Java的一个重要优点就是通过垃圾收集器(Garbage Collection，GC)自动管理内存的回收，程序员不需要通过调用函数来释放内存。因此，很多程序员认为Java不存在内存泄漏问题，或者认为即使有内存泄漏也不是程序的责任，而是GC或JVM的问题。其实，这种想法是不正确的，因为Java也存在内存泄露，但它的表现与C++不同。 随着越来越多的服务器程序采用Java技术，例如JSP，Servlet， EJB等，服务器程序往往长期运行。另外，在很多嵌入式系统中，内存的总量非常有限。内存泄露问题也就变得十分关键，即使每次运行少量泄漏，长期运行之后，系统也是面临崩溃的危险。 Java是如何管理内存为了判断Java中是否有内存泄露，我们首先必须了解Java是如何管理内存的。Java的内存管理就是对象的分配和释放问题。在Java中，程序员需要通过关键字new为每个对象申请内存空间 (基本类型除外)，所有的对象都在堆 (Heap)中分配空间。另外，对象的释放是由GC决定和执行的。在Java中，内存的分配是由程序完成的，而内存的释放是有GC完成的，这种收支两条线的方法确实简化了程序员的工作。但同时，它也加重了JVM的工作。这也是Java程序运行速度较慢的原因之一。因为，GC为了能够正确释放对象，GC必须监控每一个对象的运行状态，包括对象的申请、引用、被引用、赋值等，GC都需要进行监控。 监视对象状态是为了更加准确地、及时地释放对象，而释放对象的根本原则就是该对象不再被引用。 为了更好理解GC的工作原理，我们可以将对象考虑为有向图的顶点，将引用关系考虑为图的有向边，有向边从引用者指向被引对象。另外，每个线程对象可以作为一个图的起始顶点，例如大多程序从main进程开始执行，那么该图就是以main进程顶点开始的一棵根树。在这个有向图中，根顶点可达的对象都是有效对象，GC将不回收这些对象。如果某个对象 (连通子图)与这个根顶点不可达(注意，该图为有向图)，那么我们认为这个(这些)对象不再被引用，可以被GC回收。 以下，我们举一个例子说明如何用有向图表示内存管理。对于程序的每一个时刻，我们都有一个有向图表示JVM的内存分配情况。以下右图，就是左边程序运行到第6行的示意图。 Java使用有向图的方式进行内存管理，可以消除引用循环的问题，例如有三个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的。这种方式的优点是管理内存的精度很高，但是效率较低。另外一种常用的内存管理技术是使用计数器，例如COM模型采用计数器方式管理构件，它与有向图相比，精度行低(很难处理循环引用的问题)，但执行效率很高。 什么是Java中的内存泄露下面，我们就可以描述什么是内存泄漏。在Java中，内存泄漏就是存在一些被分配的对象，这些对象有下面两个特点，首先，这些对象是可达的，即在有向图中，存在通路可以与其相连；其次，这些对象是无用的，即程序以后不会再使用这些对象。如果对象满足这两个条件，这些对象就可以判定为Java中的内存泄漏，这些对象不会被GC所回收，然而它却占用内存。 在C++中，内存泄漏的范围更大一些。有些对象被分配了内存空间，然后却不可达，由于C++中没有GC，这些内存将永远收不回来。在Java中，这些不可达的对象都由GC负责回收，因此程序员不需要考虑这部分的内存泄露。 通过分析，我们得知，对于C++，程序员需要自己管理边和顶点，而对于Java程序员只需要管理边就可以了(不需要管理顶点的释放)。通过这种方式，Java提高了编程的效率。 因此，通过以上分析，我们知道在Java中也有内存泄漏，但范围比C++要小一些。因为Java从语言上保证，任何对象都是可达的，所有的不可达对象都由GC管理。对于程序员来说，GC基本是透明的，不可见的。虽然，我们只有几个函数可以访问GC，例如运行GC的函数System.gc()，但是根据Java语言规范定义， 该函数不保证JVM的垃圾收集器一定会执行。因为，不同的JVM实现者可能使用不同的算法管理GC。通常，GC的线程的优先级别较低。JVM调用GC的策略也有很多种，有的是内存使用到达一定程度时，GC才开始工作，也有定时执行的，有的是平缓执行GC，有的是中断式执行GC。但通常来说，我们不需要关心这些。除非在一些特定的场合，GC的执行影响应用程序的性能，例如对于基于Web的实时系统，如网络游戏等，用户不希望GC突然中断应用程序执行而进行垃圾回收，那么我们需要调整GC的参数，让GC能够通过平缓的方式释放内存，例如将垃圾回收分解为一系列的小步骤执行，Sun提供的HotSpot JVM就支持这一特性。下面给出了一个简单的内存泄露的例子。在这个例子中，我们循环申请Object对象，并将所申请的对象放入一个Vector中，如果我们仅仅释放引用本身，那么Vector仍然引用该对象，所以这个对象对GC来说是不可回收的。因此，如果对象加入到Vector后，还必须从Vector中删除，最简单的方法就是将Vector对象设置为null。 123456Vector v=new Vector(10);for (int i=1;i&lt;100; i++)&#123; Object o=new Object(); v.add(o); o=null;//此时，所有的Object对象都没有被释放，因为变量v引用这些对象。&#125; 如何检测内存泄漏最后一个重要的问题，就是如何检测Java的内存泄漏。目前，我们通常使用一些工具来检查Java程序的内存泄漏问题。市场上已有几种专业检查Java内存泄漏的工具，它们的基本工作原理大同小异，都是通过监测Java程序运行时，所有对象的申请、释放等动作，将内存管理的所有信息进行统计、分析、可视化。开发人员将根据这些信息判断程序是否有内存泄漏问题。这些工具包括Optimizeit Profiler，JProbe Profiler，JinSight , Rational 公司的Purify等。 下面，我们将简单介绍Optimizeit的基本功能和工作原理。 Optimizeit Profiler版本4.11支持Application，Applet，Servlet和Romote Application四类应用，并且可以支持大多数类型的JVM，包括SUN JDK系列，IBM的JDK系列，和Jbuilder的JVM等。并且，该软件是由Java编写，因此它支持多种操作系统。Optimizeit系列还包括Thread Debugger和Code Coverage两个工具，分别用于监测运行时的线程状态和代码覆盖面。 当设置好所有的参数了，我们就可以在OptimizeIt环境下运行被测程序，在程序运行过程中，Optimizeit可以监视内存的使用曲线(如下图)，包括JVM申请的堆(heap)的大小，和实际使用的内存大小。另外，在运行过程中，我们可以随时暂停程序的运行，甚至强行调用GC，让GC进行内存回收。通过内存使用曲线，我们可以整体了解程序使用内存的情况。这种监测对于长期运行的应用程序非常有必要，也很容易发现内存泄露。 在运行过程中，我们还可以从不同视角观查内存的使用情况，Optimizeit提供了四种方式：堆视角。 这是一个全面的视角，我们可以了解堆中的所有的对象信息(数量和种类)，并进行统计、排序，过滤。了解相关对象的变化情况。方法视角。通过方法视角，我们可以得知每一种类的对象，都分配在哪些方法中，以及它们的数量。对象视角。给定一个对象，通过对象视角，我们可以显示它的所有出引用和入引用对象，我们可以了解这个对象的所有引用关系。引用图。 给定一个根，通过引用图，我们可以显示从该顶点出发的所有出引用。在运行过程中，我们可以随时观察内存的使用情况，通过这种方式，我们可以很快找到那些长期不被释放，并且不再使用的对象。我们通过检查这些对象的生存周期，确认其是否为内存泄露。在实践当中，寻找内存泄露是一件非常麻烦的事情，它需要程序员对整个程序的代码比较清楚，并且需要丰富的调试经验，但是这个过程对于很多关键的Java程序都是十分重要的。综上所述，Java也存在内存泄露问题，其原因主要是一些对象虽然不再被使用，但它们仍然被引用。为了解决这些问题，我们可以通过软件工具来检查内存泄露，检查的主要原理就是暴露出所有堆中的对象，让程序员寻找那些无用但仍被引用的对象。]]></content>
      <tags>
        <tag>GC</tag>
        <tag>内存泄漏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis主从异步复制和Sentinel脑裂导致数据丢失的问题与解决方案]]></title>
    <url>%2F2018%2F08%2F13%2FRedis-Sentinel%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[Redis主从异步复制导致数据丢失因为Redis主从之间复制数据是异步进行的，那么就有可能在某一时刻Redis Master没有保存数据就突然宕机，此时还没来得及向Slave节点同步数据，就导致Master节点新写入的数据丢失。 Redis Sentinel脑裂问题导致数据丢失Redis Master节点每次写入新数据，都会异步发送给Slave节点，在Redis Sentinel集群环境中，如果某一个时刻Redis Master节点出现网络故障，Sentinel与Master之间、Master与Slave之间不能通信，但是Sentinel和两个Slave节点还能正常通信，此时Sentinel会根据Slave节点判断Master sdown的数量是否达到qorum数量，达到条件就认为Master odown了，Sentinel会选举出一个新的Master节点提供服务，并且把另一个Slave节点的主从配置指向新的Master节点。与此同时，Client端与原有的Redis Master之间的通信并未中断，那么此时就会出现两个Master节点同时接收写入数据，而且两者之间没有主从同步。这就是Sentinel集群中的脑裂现象。当原有的Redis Master节点网络恢复后，Sentinel会把它变成Slave节点指向新的Master节点，那么原来的Master节点中的数据会被清空，重新从新的Master节点同步全量数据。 解决这两类问题的思路Master节点记录了上次与Slave节点数据同步的offset值，并把新增数据写入本地缓冲区，异步发送给Slave节点。如果Master节点与Slave节点网络上断开，那么Master节点的新增数据会产生堆积，Master可以判断每个Slave节点从上次同步到现在的延迟时间，如果延迟时间达到一定值T，就可以认为与Slave节点断开连接，不再提供数据写服务。从客户端再写入的新数据，需要通过代码进行特殊处理。 配置Redis Master达到与Slave的断开条件后，Master停止提供数据写入服务 对Redis Master数据写入接口进行限流、降级处理，把无法写入Redis的数据，临时写到消息队列中，保全数据，并再进行数据写入的重试 redis配置及代码实现Redis配置min-slaves-to-write 1 min-slaves-max-lag 10 两个选项相互关联，如果min-slaves-to-write配置为0，则Redis此特性表示关闭。 用此处的值解释为，至少有1个slave节点的数据同步延迟&lt;=10s，否则master节点将停止接收一切请求。 Redis接口隔离和消息队列降级处理采用hystrix对Redis Master接口进行隔离，当Master节点不可用时，Redis服务接口降级为消息队列异步处理。定时（例如5分钟）从消息队列中获取未保存到Redis中的数据重新尝试写入redis。 消息发送端： 1234567891011121314151617181920212223242526@Slf4jpublic class SaveECourseInfo2RedisCmmd extends HystrixCommand&lt;ECourseInfo&gt; &#123; private ECourseInfo eCourseInfo; public SaveECourseInfo2RedisCmmd(ECourseInfo eCourseInfo)&#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("RedisGroup"))); this.eCourseInfo = eCourseInfo; &#125; @Override protected ECourseInfo run() throws Exception &#123; RedisService redisService = (RedisService) SpringContext.getContext().getBean(RedisService.class); redisService.setValue(ECourseInfoKey.ID,eCourseInfo.getPkid()+"", JSONObject.toJSONString(eCourseInfo)); log.info("======="+ JSONObject.toJSONString(eCourseInfo)+"========="); return eCourseInfo; &#125; @Override protected ECourseInfo getFallback() &#123; JSONObject jsonObject = JSONObject.parseObject(JSONObject.toJSONString(eCourseInfo)); jsonObject.put("serviceId","rewriteEcourseRedisDataService"); EcKafkaSender sender = SpringContext.getContext().getBean(EcKafkaSender.class); sender.sendMsg("redisDataRewrite",JSONObject.toJSONString(jsonObject)); return eCourseInfo; &#125;&#125; 消息接收端： 12345678910111213141516171819202122232425@Slf4j@Componentpublic class RedisDataRewriteScheduleKafkaConsumer &#123; @KafkaListener(topics="redisDataRewrite") public void listen(ConsumerRecord&lt;?, ?&gt; cr) throws Exception&#123; log.info("=======收到消息["+cr.toString()+"]========"); //收到课程管理服务数据更新的消息message，根据message对象返回的数据判断要进行的缓存处理操作 JSONObject jsonObject = JSONObject.parseObject(JSONObject.toJSONString(cr)); String serviceId = ((String) jsonObject.get("serviceId")); if("rewriteEcourseRedisDataService".equals(serviceId))&#123; rewriteEcourseRedisData(jsonObject); &#125; &#125; private void rewriteEcourseRedisData(JSONObject jsonObject) &#123; jsonObject.remove("serviceId"); ECourseInfo eCourseInfo = JSONObject.parseObject(JSONObject.toJSONString(jsonObject), ECourseInfo.class); ReSaveECourseInfo2RedisCmmd reSaveECourseInfo2RedisCmmd = new ReSaveECourseInfo2RedisCmmd(eCourseInfo); reSaveECourseInfo2RedisCmmd.execute(); &#125;&#125; 备用方案Redis配置min-slaves-to-write和min-slaves-max-lag两个选项，在代码层，把从Client端提交到Redis-Master的数据先保存到磁盘文件，然后定时从磁盘文件中读取数据重新写入Redis。 补充这种解决方案尽可能减少Redis主从复制和Sentinel脑裂后Master节点丢失的数据，不是100%不丢失数据。]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>Sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多级缓存实现服务高可用背后缓存雪崩和缓存穿透问题的解决方案]]></title>
    <url>%2F2018%2F07%2F26%2F%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E9%AB%98%E5%8F%AF%E7%94%A8%E8%83%8C%E5%90%8E%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%92%8C%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[多级缓存架构实现服务高可用传统web服务架构模式 请求从浏览器发起，到达Nginx代理服务器，可以加载静态资源，如html、js、css等；如果是请求动态内容，就会请求tomcat服务，从缓存、数据库、文件服务器加载。有一部分访问量大的数据，可以使用模板引擎技术，在数据发生变更时把动态数据加载到模板中，生成静态页面内容，当请求访问这部分数据时就可以直接加载静态页面内容了。 问题：随着业务量的增加，需要静态化处理的页面达到成千上万个时，每次数据变化都要静态化成千上万个页面，这个静态化操作过程的耗时量可想而知，而且会因为页面静态化过程大大降低Tomcat容器的吞吐量，降低性能。 双层Nginx+多级缓存架构设计 双层Nginx架构为了避免大量页面使用模板技术进行静态化带来的严重性能问题，使用Nginx+LUA脚本作为分发层，LUA脚本对请求的特殊参数进行与Nginx应用层数量取模运算，把请求分发到Nginx应用层；使用Nginx+LUA+Cache+模板进行应用层缓存处理，LUA脚本完成对应用层Tomcat接口的请求，并把返回数据写入Nginx本地缓存中，Nginx本地缓存的数据读取出来后渲染到模板页面，完成页面显示；通过以上两步操作，完成双层Nginx架构设计。这样频繁访问的热数据加载到应用层Nginx缓存中，并使用模板技术把缓存内容显示出来，请求从Nginx应用层中获取数据就返回了，不需要到达后端缓存服务或者数据库服务，提高了系统架构的健壮性。 分发层Nginx的LUA脚本代码： 123456789101112131415161718192021222324252627local uri_args = ngx.req.get_uri_args()local ecourseId = uri_args["ecourseId"]local hosts = &#123;"192.168.1.210", "192.168.1.211"&#125;local hash = ngx.crc32_long(ecourseId)local index = (hash % 2) + 1backend = "http://"..hosts[index]local requestPath = uri_args["requestPath"]requestPath = "/"..requestPath.."?ecourseId="..ecourseIdlocal http = require("resty.http")local httpc = http.new()local resp, err = httpc:request_uri(backend,&#123; method = "GET", path = requestPath&#125;)if not resp then ngx.say("request error: ", err) returnendngx.say(resp.body)httpc:close() 应用层Nginx的LUA脚本代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061local uri_args = ngx.req.get_uri_args()local ecourseId = uri_args["ecourseId"]local authorId = uri_args["authorId"]local cache_ngx = ngx.shared.my_cachelocal ecourseCacheKey = "ecourse_info_"..ecourseIdlocal authorCacheKey = "author_info_"..authorIdlocal ecourseCache = cache_ngx:get(ecourseCacheKey)local authorCache = cache_ngx:get(authorCacheKey)if ecourseCache == "" or ecourseCache == nil then local http = require("resty.http") local httpc = http.new() local resp, err = httpc:request_uri("http://192.168.31.179:8080",&#123; method = "GET", path = "/getEcourseInfo?ecourseId="..ecourseId &#125;) ecourseCache = resp.body math.randomseed(tostring(os.time()):reverse():sub(1,7)) local expireTime = math.random(600,1200) cache_ngx:set(ecourseCacheKey, ecourseCache, expireTime)endif authorCache == "" or authorCache == nil then local http = require("resty.http") local httpc = http.new() local resp, err = httpc:request_uri("http://192.168.31.179:8080",&#123; method = "GET", path = "/getEcourseAuthorInfo?authorId="..authorId &#125;) authorCache = resp.body cache_ngx:set(authorCacheKey, authorCache, 10 * 60)endlocal cjson = require("cjson")local ecourseCacheJSON = cjson.decode(ecourseCache)local authorCacheJSON = cjson.decode(authorCache)local context = &#123; ecourseId = ecourseCacheJSON.id, ecourseName = ecourseCacheJSON.name, ecoursePrice = ecourseCacheJSON.price, ecoursePictureList = ecourseCacheJSON.pictureList, ecourseSpecification = ecourseCacheJSON.specification, ecourseService = ecourseCacheJSON.service, ecourseColor = ecourseCacheJSON.color, ecourseSize = ecourseCacheJSON.size, authorId = authorCacheJSON.id, authorName = authorCacheJSON.name, authorLevel = authorCacheJSON.level, authorGoodCommentRate = authorCacheJSON.goodCommentRate&#125;local template = require("resty.template")template.render("ecourse.html", context) PS: OpenResty是打包了resty和lua模块的Nginx版本，能直接使用LUA脚本进行简单开发动态程序。 多级缓存架构为避免缓存失效后大量的数据访问请求到达MySQL数据库服务层导致MySQL服务被冲垮，需要在数据访问请求到达MySQL服务层之前进行多级缓存处理。在Nginx应用层已经实现了Cache缓存模板数据，在后端应用层中对数据缓存服务进行多层化设计，例如拆分成Redis共享缓存和Ehcache本地缓存，Ehcache缓存作为Redis共享缓存的补充，当应用层Nginx的请求到达缓存服务时，先查询Redis共享缓存中的数据，如果Redis中没有数据，再查询Ehcache本地缓存数据；如果Redis中有数据，就要写入Ehcache本地缓存，并返回请求；如果Redis共享缓存和Ehcache本地缓存中都没有数据，再调用数据生产服务接口从MySQL数据库中获取。 Redis共享缓存+Ehcache本地缓存代码： 12345678910ECourseInfo eCourseInfo = ecCacheService.getECoureseInfoFromRedis(ecourseId);log.info("=======从redis中查询课程信息=========");if (eCourseInfo == null) &#123; log.info("=======从ehcache中查询课程信息========="); eCourseInfo = ecCacheService.getECoureseInfoFromEhcache(ecourseId);&#125;if (eCourseInfo == null) &#123; log.info("=======调用MySQL数据生产服务接口获取eCourseInfo数据=======")&#125; Ehcache本地缓存配置ehcache.xml: 12345678910111213141516171819202122232425262728293031&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ehcache xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://ehcache.org/ehcache.xsd" updateCheck="false"&gt; &lt;diskStore path="java.io.tmpdir/Tmp_EhCache" /&gt; &lt;!-- 可以配置多个缓存策略，但是如果没有其他的缓存策略，则使用defaultCache缓存策略--&gt; &lt;!--eternal过期是否可用，false有过期时间，true数据永不过期--&gt; &lt;!--maxElementsInMemory 存储对象的个数，需要存储对象的大小计算可存储多少个对象 --&gt; &lt;!-- memoryStoreEvictionPolicy 当达到maxElementsInMemory限制时，Ehcache将会根据指定的策略去清理内存。默认策略是LRU（最近最少使用）。--&gt; &lt;defaultCache eternal="false" maxElementsInMemory="1000" overflowToDisk="false" diskPersistent="false" timeToIdleSeconds="0" timeToLiveSeconds="0" memoryStoreEvictionPolicy="LRU" /&gt; &lt;cache name="local" eternal="false" maxElementsInMemory="1000" overflowToDisk="false" diskPersistent="false" timeToIdleSeconds="0" timeToLiveSeconds="0" memoryStoreEvictionPolicy="LRU" /&gt;&lt;/ehcache&gt; 按照这样的设计，由Nginx本地缓存+Redis共享缓存+缓存服务Ehcache本地缓存构成三级缓存架构，也可以根据实际业务需求，增加更多的缓存层，保护后端MySQL服务。 缓存雪崩问题及对策缓存雪崩通过多级缓存策略降低请求对后端Mysql服务的请求数量，但是当多级缓存在高并发场景下出现某个缓存服务接口不可用，大量的请求Blocking在这个接口上，耗费大量的缓存服务器资源，出现连锁反应，导致其他的缓存服务接口不可用，甚至导致缓存服务层全部瘫痪，所有的网络请求如果都被缓存服务层Blocking住，那么处于同一web容器内的所有服务会直接瘫痪；如果网络请求能够绕过缓存服务，所有的请求流量到达MySQL服务接口，那么就会使MySQL的瞬时连接数飙升，很快出现服务宕机，这样的场景是灾难性的，公司的经济损失也是不可估量的。 Redis数据接口请求隔离和服务降级防止Redis服务接口因为网络问题导致接口服务阻塞或者宕机，可以对Redis的服务接口使用Hystrix进行请求资源隔离和服务降级。 Hystrix Dashboard配置Hystrix Dashboard可以对Hystrix Command接口监控，收集接口的执行性能数据，例如TP90、TP99、TP99.5的QPS数据。 在Hystrix Command所在工程内添加Hystrix Metrics依赖包，实现对接口的扫描监控 12345&lt;dependency&gt; &lt;groupId&gt;com.netflix.hystrix&lt;/groupId&gt; &lt;artifactId&gt;hystrix-metrics-event-stream&lt;/artifactId&gt; &lt;version&gt;1.5.12&lt;/version&gt;&lt;/dependency&gt; 注册Hystrix Dashboard收集数据使用的HystrixMeticsStreamServlet 1234567@Bean public ServletRegistrationBean servletRegistrationBean()&#123; ServletRegistrationBean servletRegistrationBean = new ServletRegistrationBean(); servletRegistrationBean.setServlet(new HystrixMetricsStreamServlet()); servletRegistrationBean.addUrlMappings("/hystrix.stream"); return servletRegistrationBean; &#125; 配置Hystrix应用集群监控的Turbine，在Turbine的TURBINE-HOME/WEB-INF/classes下创建config.properties 12turbine.ConfigPropertyBasedDiscovery.default.instances=192.68.1.211,192.68.1.212turbine.instanceUrlSuffix=:8081/hystrix.stream 启动Hystrix Dashboard 和Turbine两个web应用，Hystrix Dashboard用来显示收集的数据，自身实现单点监控，Turbine实现对Hystrix应用集群的监控。 从图片中可以看出每个HystrixCommand执行的TP90、TP99、TP99.5耗时，从而也能计算出每个Command的QPS，根据Dashboard提供的数据，可以进一步优化HystrixCommandThreadPool配置。Hystrix生产环境配置实践 Redis接口限流和降级使用Hystrix多级降级，并且在二级降级的HystrixCommand中实现fallback silent 或者 fallback stubbed，将Redis的服务接口隔离在单独的线程池内执行，即使当前Redis接口宕机，也不会占用其他接口资源，保证整个web服务的高可用。 特别注意：多级降级时，嵌套的内部Command不能使用外层Command的GroupKey，因为外层Command进行降级请求，很有可能是它所在的线程池资源已耗尽，如果嵌套的内部Command继续使用相同点的GroupKey，嵌套的内部Command就无法获取空闲的线程资源执行Command，导致内嵌Command对应的Hystrix CircuitBreaker直接短路，所有请求都被Reject，执行内嵌Command的fallback降级逻辑，这样多级降级就不发生作用了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182@Slf4jpublic class GetECourseInfoFromRedisCmmd extends HystrixCommand&lt;ECourseInfo&gt; &#123; private int pkid; public GetECourseInfoFromRedisCmmd(int pkid)&#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("RedisGroup")) .andCommandKey(HystrixCommandKey.Factory.asKey("GetECourseInfoFromRedisCmmd")) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.defaultSetter() //高峰时段QPS*TP99耗时+buffer，例如30QPS*0.2S+4=10; .withCoreSize(10) .withAllowMaximumSizeToDivergeFromCoreSize(true) .withMaximumSize(30) .withKeepAliveTimeMinutes(1) .withMaxQueueSize(10) .withQueueSizeRejectionThreshold(15)) .andCommandPropertiesDefaults(HystrixCommandProperties.defaultSetter() //TP99.5延迟+buffer,例如100ms+50ms .withExecutionTimeoutInMilliseconds(150) //根据业务而定 .withCircuitBreakerErrorThresholdPercentage(30) //10秒内一个窗口处理的请求数，高峰QPS*10; .withCircuitBreakerRequestVolumeThreshold(40) .withCircuitBreakerSleepWindowInMilliseconds(60*1000))); this.pkid = pkid; &#125; @Override protected ECourseInfo run() throws Exception &#123; JedisCluster jedisCluster = (JedisCluster) SpringContext.getContext().getBean("jedisClusterFactory"); String key = "ecourse_info_"+pkid; String s = jedisCluster.get(key); ECourseInfo eCourseInfo = JSONObject.parseObject(s, ECourseInfo.class); return eCourseInfo; &#125; @Override protected ECourseInfo getFallback() &#123; return new GetECourseInfoFromBakRedisCmmd(pkid).execute(); &#125; private static class GetECourseInfoFromBakRedisCmmd extends HystrixCommand&lt;ECourseInfo&gt; &#123; private int pkid; public GetECourseInfoFromBakRedisCmmd(int pkid)&#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("RedisBakGroup")) .andCommandKey(HystrixCommandKey.Factory.asKey("GetECourseInfoFromBakRedisCmmd")) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.defaultSetter() //高峰时段QPS*TP99耗时+buffer，例如30QPS*0.2S+4=10; .withCoreSize(10) .withAllowMaximumSizeToDivergeFromCoreSize(true) .withMaximumSize(30) .withKeepAliveTimeMinutes(1) .withMaxQueueSize(10) .withQueueSizeRejectionThreshold(15)) .andCommandPropertiesDefaults(HystrixCommandProperties.defaultSetter() //TP99.5延迟+buffer,例如100ms+50ms .withExecutionTimeoutInMilliseconds(150) //根据业务而定 .withCircuitBreakerErrorThresholdPercentage(30) //10秒内一个窗口处理的请求数，高峰QPS*10; .withCircuitBreakerRequestVolumeThreshold(40) .withCircuitBreakerSleepWindowInMilliseconds(60*1000))); this.pkid = pkid; &#125; @Override protected ECourseInfo run() throws Exception &#123; JedisCluster jedisCluster = (JedisCluster) SpringContext.getContext().getBean("jedisClusterFactory"); String key = "ecourse_info_"+pkid; String s = jedisCluster.get(key); ECourseInfo eCourseInfo = JSONObject.parseObject(s, ECourseInfo.class); return eCourseInfo; &#125; /** * fail silent * @return */ @Override protected ECourseInfo getFallback() &#123; return null; &#125; &#125;&#125; 数据生产服务层请求隔离和服务降级数据缓存服务层，除了要跨网络请求Redis接口数据，还要跨网络请求数据生产服务的接口数据。如果大量的请求涌入数据生产服务，除了会给后端数据库带来高并发压力外，还有可能因为缓存层的高并发占用缓存服务接口的资源，导致缓存层服务宕机。 数据生产服务接口多级降级和最终Fallback Stubbed内外两层Command不能使用相同的GroupKey，原理与缓存服务层的Redis数据接口的多级降级配置原理相同。外层Command调用数据生产服务接口获取数据，第一次降级请求云端历史数据（或其他冷备历史数据），第二次降级实现残缺数据降级，根据请求参数中的数据组装一些初级数据返回。 第二级降级一定要实现Fallback Stubbed，这样才能保证客户端请求有数据返回。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687@Slf4jpublic class GetECourseInfoFromServiceCmmd extends HystrixCommand&lt;ECourseInfo&gt; &#123; private Integer ecourseId; public GetECourseInfoFromServiceCmmd(Integer ecourseId)&#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ECourseInfoServiceGroup")) .andCommandKey(HystrixCommandKey.Factory.asKey("GetECourseInfoFromServiceCmmd")) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.defaultSetter() //高峰时段QPS*TP99耗时+buffer，例如30QPS*0.2S+4=10; .withCoreSize(10) .withAllowMaximumSizeToDivergeFromCoreSize(true) .withMaximumSize(30) .withKeepAliveTimeMinutes(1) .withMaxQueueSize(10) .withQueueSizeRejectionThreshold(15)) .andCommandPropertiesDefaults(HystrixCommandProperties.defaultSetter() //TP99.5延迟+buffer,例如100ms+50ms .withExecutionTimeoutInMilliseconds(150) //根据业务而定 .withCircuitBreakerErrorThresholdPercentage(30) //10秒内一个窗口处理的请求数，高峰QPS*10; .withCircuitBreakerRequestVolumeThreshold(40) .withCircuitBreakerSleepWindowInMilliseconds(60*1000))); this.ecourseId = ecourseId; &#125; @Override protected ECourseInfo run() throws Exception &#123; RestTemplate restTemplate = (RestTemplate) SpringContext.getContext().getBean("restTemplate"); ECourseInfo ecourseInfo = restTemplate.getForObject("http://localhost:9090/ecourse?ecourseId=" + ecourseId, ECourseInfo.class); return ecourseInfo; &#125; @Override protected ECourseInfo getFallback() &#123; return super.getFallback(); &#125; private static class GetECourseInfoFromCloudDataCmmd extends HystrixCommand&lt;ECourseInfo&gt;&#123; private Integer ecourseId; public GetECourseInfoFromCloudDataCmmd(Integer ecourseId)&#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ECourseInfoCloudDataGroup")) .andCommandKey(HystrixCommandKey.Factory.asKey("GetECourseInfoFromServiceCmmd")) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.defaultSetter() //高峰时段QPS*TP99耗时+buffer，例如30QPS*0.2S+4=10; .withCoreSize(10) .withAllowMaximumSizeToDivergeFromCoreSize(true) .withMaximumSize(30) .withKeepAliveTimeMinutes(1) .withMaxQueueSize(10) .withQueueSizeRejectionThreshold(15)) .andCommandPropertiesDefaults(HystrixCommandProperties.defaultSetter() //TP99.5延迟+buffer,例如100ms+50ms .withExecutionTimeoutInMilliseconds(150) //根据业务而定 .withCircuitBreakerErrorThresholdPercentage(30) //10秒内一个窗口处理的请求数，高峰QPS*10; .withCircuitBreakerRequestVolumeThreshold(40) .withCircuitBreakerSleepWindowInMilliseconds(60*1000))); this.ecourseId = ecourseId; &#125; @Override protected ECourseInfo run() throws Exception &#123; RestTemplate restTemplate = (RestTemplate) SpringContext.getContext().getBean("restTemplate"); ECourseInfo ecourseInfo = restTemplate.getForObject("http://localhost:9990/ecourse?ecourseId=" + ecourseId, ECourseInfo.class); return ecourseInfo; &#125; /** * 实现fallback stubbed残缺降级 * @return */ @Override protected ECourseInfo getFallback() &#123; ECourseInfo eCourseInfo = new ECourseInfo(); eCourseInfo.setPkid(this.ecourseId); eCourseInfo.setModifyTime(new Date()); eCourseInfo.setAuthorId(11111); eCourseInfo.setName("stubbed fallback ecourse name"); eCourseInfo.setPrice(123.33); return eCourseInfo; &#125; &#125;&#125; 缓存穿透问题及对策缓存穿透当一个请求从浏览器经过三级缓存查询后，没有查到数据，就会到达数据库层查询数据，如果数据库也没有查到数据就返回NULL了。如果大量的这类请求并发访问服务接口，必然会导致请求到达数据库接口层，增大数据库的压力，这个现象就是缓存穿透。 后端缓存层缓存空数据只要把数据库返回的Null数据结合请求参数放入缓存层，这样相同参数的请求再查询数据时，就能从缓存层获得空数据，而不用去查询数据库。而对应参数的数据在数据库中产生时，及时删除缓存即可。 123456789101112131415//如果从两级缓存中还是查不到，应该去数据生产服务中查询，同一个缓存服务实例接收到多个并发查询并更新redis缓存，就可能出现因为执行先后造成的新数据被旧数据覆盖的冲突问题。并发请求改为异步内存队列处理。 if (eCourseInfo == null) &#123; //从数据管理服务查询数据，存入内存队列，异步按顺序写入缓存。 GetECourseInfoFromServiceCmmd getECourseInfoFromServiceCmmd = new GetECourseInfoFromServiceCmmd(ecourseId); eCourseInfo = getECourseInfoFromServiceCmmd.execute(); //防止多级缓存被穿透，即使从课程服务接口从数据库中没有查询到课程信息，也要在此处实例化一个对象 爆炸 if (eCourseInfo == null) &#123; eCourseInfo = new ECourseInfo(ecourseId); &#125; //放入队列，等待队列处理线程把数据更新到redis //此处，线程执行保存数据到redis中时，可能有分布式并发问题，所以要使用分布式锁，解决并发冲突。 MultiThreadUpdateRedisQueue.Singleton.getInstance().put(eCourseInfo); &#125;]]></content>
      <tags>
        <tag>多级缓存</tag>
        <tag>缓存雪崩</tag>
        <tag>缓存穿透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hystrix生产环境配置实践]]></title>
    <url>%2F2018%2F07%2F25%2FHystrix%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[线程池资源优化演进在生产环境中部署一个短路器，一开始需要将一些关键配置设置的大一些，比如timeout超时时长，线程池大小，或信号量容量，然后逐渐优化这些配置，直到在一个生产系统中运作良好。 （1）一开始先不要设置timeout超时时长，默认就是1000ms，也就是1s（2）一开始也不要设置线程池大小，默认就是10（3）直接部署hystrix到生产环境，如果运行的很良好，那么就让它这样运行好了（4）让hystrix应用，24小时运行在生产环境中（5）依赖标准的监控和报警机制来捕获到系统的异常运行情况（6）在24小时之后，看一下调用延迟的占比，以及流量，来计算出让短路器生效的最小的配置数字（7）直接对hystrix配置进行热修改，然后继续在hystrix dashboard上监控（8）看看修改配置后的系统表现有没有改善 HystrixThreadPool线程数量设定计算公式每秒的高峰访问次数 * 99%的访问延时 + 缓冲数量 假设一个依赖的服务的高峰访问次数30QPS，99%的请求延时实践在200ms，那么我们可以设定的线程数量=30*0.2+4=10个线程。 HystrixCommand线程超时timeout设定结合线程池数量配置，设置能够完成执行高峰时QPS任务的timeout数值。 默认线程超时为1s。假设一个依赖服务的高峰访问时，99.5%的请求延时在250ms，假设再允许服务重试一次消耗50ms，那么这个依赖服务的延时设置为300ms就可以了，这样一个线程最多延时300ms，也就是这个线程每秒能执行三次依赖服务，线程池中10个线程，整好能处理30QPS。如果把延时设置为400ms，那么一个线程每秒最多执行2次请求，10个线程在1s内最多执行20个请求，剩余的10个请求就会被线程池阻塞住，再来新的请求也继续会被阻塞住，如果等待队列已满，那么新来的请求就会被直接Reject。所以400ms的超时设置就不能满足30QPS的请求次数要求。 生产环境中动态分配线程池资源线程池动态配置选项 //允许线程池自动从coreSize扩容到maximumSize .withAllowMaximumSizeToDivergeFromCoreSize(true) //default 10，默认线程池大小 .withCoreSize(10) //最大线程数 .withMaximumSize(30) //空闲线程存活时间1分钟，超时后线程池数量自动恢复到coreSize大小 .withKeepAliveTimeMinutes(1) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172@Slf4jpublic class GetProductInfoWithDynamicThreadPoolCmmd extends HystrixCommand&lt;ProductInfo&gt; &#123; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey("GetProductInfoCmmd"); private Long productId; public GetProductInfoWithDynamicThreadPoolCmmd(Long productId) &#123; super( Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("GetProductInfoGroup")) .andCommandKey(HystrixCommandKey.Factory.asKey("GetProductInfoCmmd")) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey("GetProductInfoThreadPool")) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.THREAD) //允许执行的最大并发数 .withExecutionIsolationSemaphoreMaxConcurrentRequests(30) //请求超时 .withExecutionTimeoutInMilliseconds(300)) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() //允许线程池自动从coreSize扩容到maximumSize .withAllowMaximumSizeToDivergeFromCoreSize(true) //default 10，默认线程池大小 .withCoreSize(10) //最大线程数 .withMaximumSize(30) //空闲线程存活时间1分钟，超时后线程池数量自动恢复到coreSize大小 .withKeepAliveTimeMinutes(1) //default 5，没有被线程池执行的请求会放到队 //列中等待线程，如果队列已满，新来的请求会被降级被拒 .withQueueSizeRejectionThreshold(10) //withQueueSizeRejectionThreshold和withMaxQueueSize比较，queueSize取较小值 .withMaxQueueSize(8)) ); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; String url = "http://127.0.0.1:8082/product/get?productId="+productId; String s = HttpClientUtils.sendGetRequestWithException(url); log.info("product-ha response &#123;&#125;",s); ProductInfo productInfo = JSONObject.parseObject(s, ProductInfo.class); return productInfo; &#125; /** * 从hystrix本地缓存中获取数据，而不是执行run方法 * @return */ @Override protected String getCacheKey() &#123; return "product_key_"+productId; &#125; /** * 清理hystrix缓存的数据 * @param productId */ public static void flushCache(Long productId)&#123; HystrixRequestCache.getInstance(KEY,HystrixConcurrencyStrategyDefault.getInstance()).clear("product_key_"+productId); &#125; /** * hystrix执行降级请求时返回的数据 * @return */ @Override protected ProductInfo getFallback() &#123; return null; &#125;&#125;]]></content>
      <tags>
        <tag>hystrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hystrix在实际应用中的多级降级和手动降级策略]]></title>
    <url>%2F2018%2F07%2F24%2FHystrix%E5%9C%A8%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%A7%E9%99%8D%E7%BA%A7%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[HystrixCommand嵌套实现多级降级当HystrixCommand在执行跨网络请求接口时，可能因为接口服务不可用而导致接口服务降级，此时回调用HystrixCommand的fallback()方法返回数据。在实际应用中，网络服务接口要实现高可用，需要对接口进行主备的部署方案，主接口宕机，可以调用备用服务接口获取数据。此时在HystrixCommand的fallback()方法中再使用HystrixCommand调用备用服务接口，实现第一次降级；如果备用服务接口仍然不可用，那么进行第二次服务降级，返回Stubbed Fallback（残缺降级）数据。 代码实现PS： GetProductInfoCmmd的GroupKey的值为GetProductInfoCmmdGroup，第一级降级fallback()中调用的GetProductInfoFallBackCmmd的GroupKey的值为GetProductInfoFallBackGroup，这两个GropKey值不能相同，因为第一级降级时GetProductInfoCmmdGroup线程池中的线程可能已经被耗尽，再使用这个GroupKey将无法从中获取空闲的线程执行降级的GetProductInfoFallBackCmmd。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788@Slf4jpublic class GetProductInfoCmmd extends HystrixCommand&lt;ProductInfo&gt; &#123; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey("GetProductInfoCmmd"); private Long productId; public GetProductInfoCmmd(Long productId) &#123; super(HystrixCommandGroupKey.Factory.asKey("GetProductInfoCmmdGroup")); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; String url = "http://127.0.0.1:8082/product/get?productId="+productId; String s = HttpClientUtils.sendGetRequestWithException(url); log.info("product-ha response &#123;&#125;",s); ProductInfo productInfo = JSONObject.parseObject(s, ProductInfo.class); return productInfo; &#125; /** * 从hystrix本地缓存中获取数据，而不是执行run方法 * @return */ @Override protected String getCacheKey() &#123; return "product_key_"+productId; &#125; /** * 清理hystrix缓存的数据 * @param productId */ public static void flushCache(Long productId)&#123; HystrixRequestCache.getInstance(KEY, HystrixConcurrencyStrategyDefault.getInstance()) .clear("product_key_"+productId); &#125; /** * 第一次降级调用备用服务的Command * hystrix执行降级请求时返回的数据 * @return */ @Override protected ProductInfo getFallback() &#123; return new GetProductInfoFallBackCmmd(productId).execute(); &#125; private class GetProductInfoFallBackCmmd extends HystrixCommand&lt;ProductInfo&gt;&#123; private Long productId; public GetProductInfoFallBackCmmd(Long productId) &#123; //特别注意：多级降级时，fallback调用的command的groupkey不能与 //调用者的groupkey相同，因为调用者 //的groupkey中的线程可能已经被耗尽，所以才走的调用者的fallback。 super(Setter .withGroupKey(HystrixCommandGroupKey.Factory .asKey("GetProductInfoFallBackGroup"))); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; //此处应该调用备用服务的接口 String url = "http://127.0.0.1:8083/product/get?productId="+productId; String s = HttpClientUtils.sendGetRequestWithException(url); log.info("product-ha response &#123;&#125;",s); ProductInfo productInfo = JSONObject.parseObject(s, ProductInfo.class); return productInfo; &#125; /** * 第二次降级，返回stubbed fallback数据 */ @Override protected ProductInfo getFallback() &#123; String product = "&#123;\"id\": 111, \"name\": \"iphone7手机\", \"price\": 5599, \"pictureList\":\"a.jpg,b.jpg\", \"specification\": \"iphone7的规格\", \"service\": \"iphone7的售后服务\", \"color\": \"红色,白色,黑色\", \"size\": \"5.5\", \"shopId\": 1, \"modifiedTime\": \"2017-01-01 12:00:00\", \"cityId\":1&#125;"; log.info("getFallback:&#123;&#125;",product); ProductInfo productInfo = JSONObject.parseObject(product, ProductInfo.class); return productInfo; &#125; &#125;&#125; HystrixCommand嵌套实现手动降级手动降级是要实现web容器运行时调用主服务接口和自定义备用接口之间手动灵活切换，可以用在主服务接口升级时，临时切换到备用接口上的业务场景，主服务接口升级完成后，再手动切换回主服务接口。实现这一功能，需要开发一个Semaphor策略的服务接口调度Command，一个调用主服务接口的ThreadPool策略的Command，一个调用备用服务接口的ThreadPool策略的Command。 代码实现12345678910111213141516171819202122232425262728293031@Slf4jpublic class GetProductInfoManualFacadeCmmd extends HystrixCommand&lt;ProductInfo&gt; &#123; private static boolean useDegrade = false; private Long productId; public GetProductInfoManualFacadeCmmd(Long productId)&#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("GetProductInfoFacade")) .andCommandPropertiesDefaults(HystrixCommandProperties.defaultSetter() .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.SEMAPHORE))); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; if (!useDegrade) &#123; return new GetProductInfoCmmd(productId).execute(); &#125;else &#123; return new GetProductInfoFromManualCmmd(productId).execute(); &#125; &#125; public static boolean isUseDegrade() &#123; return useDegrade; &#125; public static void setUseDegrade(boolean useDegrade) &#123; GetProductInfoManualFacadeCmmd.useDegrade = useDegrade; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142@Slf4jpublic class GetProductInfoCmmd extends HystrixCommand&lt;ProductInfo&gt; &#123; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey("GetProductInfoCmmd"); private Long productId; public GetProductInfoCmmd(Long productId) &#123; super(HystrixCommandGroupKey.Factory.asKey("GetProductInfoCmmdGroup")); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; String url = "http://127.0.0.1:8082/product/get?productId="+productId; String s = HttpClientUtils.sendGetRequestWithException(url); log.info("product-ha response &#123;&#125;",s); ProductInfo productInfo = JSONObject.parseObject(s, ProductInfo.class); return productInfo; &#125; /** * 从hystrix本地缓存中获取数据，而不是执行run方法 * @return */ @Override protected String getCacheKey() &#123; return "product_key_"+productId; &#125; /** * 第一次降级调用备用服务的Command * hystrix执行降级请求时返回的数据 * @return */ @Override protected ProductInfo getFallback() &#123; return null; &#125;&#125; 123456789101112131415161718@Slf4jpublic class GetProductInfoFromManualCmmd extends HystrixCommand&lt;ProductInfo&gt; &#123; private Long productId; public GetProductInfoFromManualCmmd(Long productId)&#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("GetProductInfoFromManualGroup"))); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; ProductInfo manualPro = new ProductInfo(); manualPro.setId(123); manualPro.setPrice(23); manualPro.setCityId(2L); manualPro.setCityName(LocalCache.getCityName(manualPro.getCityId())); return manualPro; &#125;&#125; 通过http请求动态修改useDegrade状态，实现GetProductInfoCmmd和GetProductInfoFromManualCmmd动态切换。 123456@RequestMapping("/product/info/usedegrade") @ResponseBody public String productInfoUseDegrade(boolean degrade)&#123; GetProductInfoManualFacadeCmmd.setUseDegrade(degrade); return "degrade="+degrade; &#125;]]></content>
      <tags>
        <tag>hystrix</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hystrix高并发请求合并处理]]></title>
    <url>%2F2018%2F07%2F24%2FHystrix%E9%AB%98%E5%B9%B6%E5%8F%91%E8%AF%B7%E6%B1%82%E5%90%88%E5%B9%B6%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[Hystrix处理单个或者数量较少的请求创建HystrixCommand12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Slf4jpublic class GetProductInfoCmmd extends HystrixCommand&lt;ProductInfo&gt; &#123; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey("GetProductInfoCmmd"); private Long productId; public GetProductInfoCmmd(Long productId) &#123; //跨网络请求通常使用HystrixCommandProperties.ExecutionIsolationStrategy.THREAD // 线程池策略（默认最多并发10个线程请求）进行隔离限流，目的是用线程池可以获取线程执行的操作状态 //GetProductInfoCmmdGroup定义了一个线程池专门执行run方法，这个线程池的大小就主动 //限制住所有请求run方法中的http请求，防止http请求阻塞后整个web容器的线程资源都被耗尽 //super(HystrixCommandGroupKey.Factory.asKey("GetProductInfoCmmdGroup")); super( Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("GetProductInfoGroup")) .andCommandKey(HystrixCommandKey.Factory.asKey("GetProductInfoCmmd")) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey("GetProductInfoThreadPool")) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.THREAD) //允许执行的最大并发数 .withExecutionIsolationSemaphoreMaxConcurrentRequests(30) //请求超时 .withExecutionTimeoutInMilliseconds(200)) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() //default 10，默认线程池大小 .withCoreSize(10) //default 5，没有被线程池执行的请求会放到队列中等待线程，如果队列已满，新来的请求会被降级被拒 .withQueueSizeRejectionThreshold(10) //withQueueSizeRejectionThreshold和withMaxQueueSize比较，queueSize取较小值 .withMaxQueueSize(8)) ); this.productId = productId; &#125; @Override protected ProductInfo run() throws Exception &#123; String url = "http://127.0.0.1:8082/product/get?productId="+productId; String s = HttpClientUtils.sendGetRequestWithException(url); log.info("product-ha response &#123;&#125;",s); ProductInfo productInfo = JSONObject.parseObject(s, ProductInfo.class); return productInfo; &#125; /** * 从hystrix本地缓存中获取数据，而不是执行run方法 * @return */ @Override protected String getCacheKey() &#123; return "product_key_"+productId; &#125; /** * 清理hystrix缓存的数据 * @param productId */ public static void flushCache(Long productId)&#123; HystrixRequestCache.getInstance(KEY,HystrixConcurrencyStrategyDefault.getInstance()).clear("product_key_"+productId); &#125; /** * hystrix执行降级请求时返回的数据 * @return */ @Override protected ProductInfo getFallback() &#123; String product = "&#123;\"id\": 111, \"name\": \"iphone7手机\", \"price\": 5599, \"pictureList\":\"a.jpg,b.jpg\", \"specification\": \"iphone7的规格\", \"service\": \"iphone7的售后服务\", \"color\": \"红色,白色,黑色\", \"size\": \"5.5\", \"shopId\": 1, \"modifiedTime\": \"2017-01-01 12:00:00\", \"cityId\":1&#125;"; log.info("getFallback:&#123;&#125;",product); ProductInfo productInfo = JSONObject.parseObject(product, ProductInfo.class); return productInfo; &#125;&#125; 开启Hystrix Request CacheCommand中配置缓存数据的KEY见Command中如下代码，Request Cache的作用域是同一个request scope内的多个command实例调用同一个请求，hystrix会根据请求的key值判断是否从Request Cache中获取请求数据，而不是执行Command的run()方法。反之，多个request之间cache不能共享。HystrixCommand抽象类的getCacheKey()方法，返回null，表示不使用Request Cache。 12345678/** * 从hystrix本地缓存中获取数据，而不是执行run方法 * @return */ @Override protected String getCacheKey() &#123; return "product_key_"+productId; &#125; 开启HystrixRequestContext上下文环境12345678910111213141516171819202122public class HystrixRequestFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; //加载HystrixRequestContext HystrixRequestContext hystrixRequestContext = HystrixRequestContext.initializeContext(); try &#123; filterChain.doFilter(servletRequest,servletResponse); &#125; finally &#123; hystrixRequestContext.shutdown(); &#125; &#125; @Override public void destroy() &#123; &#125;&#125; WebContext配置HystrixContext过滤器1234567@Bean public FilterRegistrationBean filterRegistrationBean()&#123; FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(); filterRegistrationBean.setFilter(new HystrixRequestFilter()); filterRegistrationBean.addUrlPatterns("/*"); return filterRegistrationBean; &#125; MainThread实例化并提交Command12345678@RequestMapping("/product/receiveChg") @ResponseBody public ProductInfo changeProduct(Long productId)&#123; //1.线程池隔离策略限流网络请求获取产品信息 GetProductInfoCmmd productInfoGetProductInfoCmmd = new GetProductInfoCmmd(productId); ProductInfo pro = productInfoGetProductInfoCmmd.execute(); log.info("产品信息:&#123;&#125;",JSONObject.toJSONString(pro)); &#125; 单元测试测试代码123456789101112131415161718192021222324252627private final int requestSize =20; private final int concurrentSize = 5; private final ExecutorService service = Executors.newFixedThreadPool(requestSize); private final Semaphore semaphore = new Semaphore(concurrentSize); private final CyclicBarrier cb = new CyclicBarrier(concurrentSize); private final CountDownLatch count = new CountDownLatch(requestSize); @Test public void changeProduct() throws Exception&#123; for (int i = 0; i &lt; requestSize; i++) &#123; final int flag = i; service.execute(()-&gt;&#123; try &#123; semaphore.acquire(); String s = getProductInfo("http://127.0.0.1:8081/product/receiveChg?productId=1"); log.info(" 执行结果:"+s,flag); count.countDown(); &#125; catch (Exception e) &#123; log.error("出错：",e); &#125; finally &#123; semaphore.release(); &#125; &#125;); &#125; count.await(); service.shutdown(); &#125; 测试结果执行了20次请求。 12345678910111213141516171819202018-07-24 10:51:17.012 INFO 2314 --- [nio-8082-exec-2] c.r.e.p.ha.controller.ProductController : 请求：32018-07-24 10:51:17.024 INFO 2314 --- [nio-8082-exec-3] c.r.e.p.ha.controller.ProductController : 请求：02018-07-24 10:51:17.030 INFO 2314 --- [nio-8082-exec-4] c.r.e.p.ha.controller.ProductController : 请求：12018-07-24 10:51:17.036 INFO 2314 --- [nio-8082-exec-5] c.r.e.p.ha.controller.ProductController : 请求：22018-07-24 10:51:17.038 INFO 2314 --- [nio-8082-exec-1] c.r.e.p.ha.controller.ProductController : 请求：42018-07-24 10:51:17.374 INFO 2314 --- [nio-8082-exec-6] c.r.e.p.ha.controller.ProductController : 请求：82018-07-24 10:51:17.386 INFO 2314 --- [nio-8082-exec-6] c.r.e.p.ha.controller.ProductController : 请求：72018-07-24 10:51:17.386 INFO 2314 --- [nio-8082-exec-7] c.r.e.p.ha.controller.ProductController : 请求：62018-07-24 10:51:17.386 INFO 2314 --- [nio-8082-exec-8] c.r.e.p.ha.controller.ProductController : 请求：92018-07-24 10:51:17.386 INFO 2314 --- [nio-8082-exec-9] c.r.e.p.ha.controller.ProductController : 请求：52018-07-24 10:51:17.511 INFO 2314 --- [nio-8082-exec-5] c.r.e.p.ha.controller.ProductController : 请求：132018-07-24 10:51:17.517 INFO 2314 --- [nio-8082-exec-3] c.r.e.p.ha.controller.ProductController : 请求：122018-07-24 10:51:17.521 INFO 2314 --- [nio-8082-exec-2] c.r.e.p.ha.controller.ProductController : 请求：112018-07-24 10:51:17.525 INFO 2314 --- [nio-8082-exec-1] c.r.e.p.ha.controller.ProductController : 请求：102018-07-24 10:51:17.528 INFO 2314 --- [nio-8082-exec-2] c.r.e.p.ha.controller.ProductController : 请求：142018-07-24 10:51:17.673 INFO 2314 --- [io-8082-exec-10] c.r.e.p.ha.controller.ProductController : 请求：162018-07-24 10:51:17.679 INFO 2314 --- [nio-8082-exec-6] c.r.e.p.ha.controller.ProductController : 请求：152018-07-24 10:51:17.680 INFO 2314 --- [nio-8082-exec-8] c.r.e.p.ha.controller.ProductController : 请求：192018-07-24 10:51:17.680 INFO 2314 --- [nio-8082-exec-7] c.r.e.p.ha.controller.ProductController : 请求：172018-07-24 10:51:17.684 INFO 2314 --- [nio-8082-exec-9] c.r.e.p.ha.controller.ProductController : 请求：18 Hystrix合并请求处理高并发在处理单个或者少量请求时，每个Command会占用一个线程资源执行请求，同一个服务的高并发场景下，这个服务所使用的Hystrix线程池资源会快速耗尽，并执行请求降级和断路。如果能够把对同一个服务的大量请求转化成一个请求，只消耗一个线程资源就能完成多个请求任务，虽然多个Request合并为一个Request执行后，平均到每个Request上的耗时会略有增加，但是将会大幅提升Hystrix的并发吞吐量，降低Hystrix服务降级和断路的发生概率。牺牲一点性能，提高系统的可用性是值得的。 Hystrix通过请求折叠合并，开启Hystrix Request Cache请求缓存，实现对此业务场景的支持。 创建请求折叠合并器Collapser123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869@Slf4jpublic class GetProductInfosCollapser extends HystrixCollapser&lt;List&lt;ProductInfo&gt;,ProductInfo,Long&gt; &#123; private Long productId; public GetProductInfosCollapser(Long productId)&#123; super(Setter.withCollapserKey(HystrixCollapserKey.Factory.asKey("GetProductInfosCollapser")) .andCollapserPropertiesDefaults(HystrixCollapserProperties.defaultSetter() //100ms以内的请求合并为一个请求,default 10 .withTimerDelayInMilliseconds(1000) //请求合并的最大值100,default Integer.MAX_VALUE .withMaxRequestsInBatch(100)) //配置请求合并或者全局合并，default Scope.REQUEST .andScope(Scope.GLOBAL)); this.productId = productId; &#125; @Override public Long getRequestArgument() &#123; return productId; &#125; @Override protected String getCacheKey() &#123; return "product_info_"+productId; &#125; @Override protected HystrixCommand&lt;List&lt;ProductInfo&gt;&gt; createCommand(Collection&lt;CollapsedRequest&lt;ProductInfo, Long&gt;&gt; collapsedRequests) &#123; return new GetProductInfosCmmd4Collapser(collapsedRequests); &#125; @Override protected void mapResponseToRequests(List&lt;ProductInfo&gt; batchResponse, Collection&lt;CollapsedRequest&lt;ProductInfo, Long&gt;&gt; collapsedRequests) &#123; int count = 0 ; for (CollapsedRequest&lt;ProductInfo, Long&gt; collapsedRequest : collapsedRequests) &#123; collapsedRequest.setResponse(batchResponse.get(count++)); &#125; &#125; /** * 内置Command处理多个请求 */ private static class GetProductInfosCmmd4Collapser extends HystrixCommand&lt;List&lt;ProductInfo&gt;&gt;&#123; Collection&lt;CollapsedRequest&lt;ProductInfo, Long&gt;&gt; collapsedRequests; public GetProductInfosCmmd4Collapser(Collection&lt;CollapsedRequest&lt;ProductInfo, Long&gt;&gt; collapsedRequests)&#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("GetProductInfosGroup")) .andCommandPropertiesDefaults(HystrixCommandProperties.defaultSetter() .withExecutionTimeoutInMilliseconds(20000)) .andCommandKey(HystrixCommandKey.Factory.asKey("GetProductInfosCmmd"))); this.collapsedRequests = collapsedRequests; &#125; @Override protected List&lt;ProductInfo&gt; run() throws Exception &#123; StringBuilder stringBuilder = new StringBuilder(); for (CollapsedRequest&lt;ProductInfo, Long&gt; collapsedRequest : collapsedRequests) &#123; Long productId = collapsedRequest.getArgument(); stringBuilder.append(productId+","); &#125; String productIds = stringBuilder.substring(0, stringBuilder.length() - 1); String url = "http://127.0.0.1:8082/products/get?productIds="+productIds; String s = HttpClientUtils.sendGetRequestWithException(url); log.info("product-ha response &#123;&#125;",s); List&lt;ProductInfo&gt; productInfos = JSONArray.parseArray(s, ProductInfo.class); return productInfos; &#125; &#125;&#125; 开启Hystrix Request Cache代码与处理单个请求时相同，在Collapser中重写getCacheKey方法，并在WebContex中注册HttpRequestContextFilter监听器。 PS：Collapser把多个请求合并成一个Request后，再进行Request Cache判断是否从Cache中返回结果。也就是说如果多个相同的cacheKey被合并到不同的Request中，那么Request Cache就不能再多个Request之间共享，每次请求都会执行Command的run方法获取返回数据。而被合并到同一个Request中的多个相同的cacheKey请求，会根据Request Cache的getCacheKey()方法设置的key返回Request的Cache数据，而不需要重复执行Command的run()方法获取数据。 实例化Collapser并提交请求1234567891011121314151617181920212223@RequestMapping("/product/muti3") @ResponseBody public List&lt;ProductInfo&gt; getMutiProductsWithCollapseCmmd(String productIds)&#123; List&lt;ProductInfo&gt; resList = new ArrayList&lt;&gt;(); List&lt;Future&lt;ProductInfo&gt;&gt; futures = new ArrayList&lt;&gt;(); List&lt;String&gt; productIdsList = Arrays.asList(productIds.split(",")); for (String s : productIdsList) &#123; GetProductInfosCollapser getProductInfosCollapserCmmd = new GetProductInfosCollapser(Long.valueOf(s)); Future&lt;ProductInfo&gt; queue = getProductInfosCollapserCmmd.queue(); futures.add(queue); &#125; //执行future.get()方法时才真正执行GetProductInfosCmmd4Collapse的run方法 try &#123; for(Future&lt;ProductInfo&gt; future : futures) &#123; resList.add(future.get()); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return resList; &#125; 单元测试测试代码112345@Test public void testGetProductMulti3_() throws Exception&#123; String productInfos = getProductInfo("http://127.0.0.1:8081/product/muti3?productIds=1,2,3,4,4,4,5,5,6,7"); log.info("返回结果：&#123;&#125;",productInfos); &#125; 测试结果在http://127.0.0.1:8082/products/get服务后台打印服务调用日志，发现相同的productId被Collapser缓存合并，只请求了一次。 12018-07-24 08:50:04.183 INFO 1351 --- [nio-8082-exec-2] c.r.e.p.ha.controller.ProductController : 请求：1,2,3,4,5,6,7 测试代码2模拟5个并发，总共请求20次。 123456789101112131415161718192021222324252627282930 private final int requestSize =20; private final int concurrentSize = 5; private final ExecutorService service = Executors.newFixedThreadPool(requestSize); private final Semaphore semaphore = new Semaphore(concurrentSize); private final CountDownLatch count = new CountDownLatch(requestSize);@Test public void testGetProductMulti3() throws Exception&#123; for (int i = 0; i &lt; requestSize; i++) &#123; final int flag = i; service.execute(()-&gt;&#123; try &#123; semaphore.acquire(); String productInfos = getProductInfo("http://127.0.0.1:8081/product/muti3?productIds=" + flag); log.info("返回结果：&#123;&#125;",productInfos); count.countDown(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125; &#125;); if(i&gt;0&amp;&amp;i%5==0)&#123; Thread.sleep(100); &#125; &#125; count.await(); service.shutdown(); &#125; 测试结果每5个并发请求被合并成一个请求，共20个请求执行了4次。 12342018-07-24 08:56:30.867 INFO 1351 --- [nio-8082-exec-5] c.r.e.p.ha.controller.ProductController : 请求：0,1,2,3,42018-07-24 08:56:31.640 INFO 1351 --- [nio-8082-exec-6] c.r.e.p.ha.controller.ProductController : 请求：5,6,7,8,92018-07-24 08:56:32.639 INFO 1351 --- [nio-8082-exec-7] c.r.e.p.ha.controller.ProductController : 请求：10,11,12,13,142018-07-24 08:56:33.638 INFO 1351 --- [nio-8082-exec-8] c.r.e.p.ha.controller.ProductController : 请求：16,17,18,19,15]]></content>
      <tags>
        <tag>hystrix</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解hystrix请求限流、降级条件和请求调用过程原理]]></title>
    <url>%2F2018%2F07%2F23%2F%E5%9B%BE%E8%A7%A3hystrix%E8%AF%B7%E6%B1%82%E9%99%8D%E7%BA%A7%E6%9D%A1%E4%BB%B6%E5%92%8C%E8%AF%B7%E6%B1%82%E8%B0%83%E7%94%A8%E8%BF%87%E7%A8%8B%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Hystrix原理图 Hystrix应用场景 资源隔离：你如果有很多个依赖服务，高可用性，先做资源隔离，任何一个依赖服务的故障不会导致你的服务的资源耗尽，不会崩溃 请求缓存：对于一个request context内的多个相同command，使用request cache，提升性能 熔断：基于短路器，采集各种异常事件，报错，超时，reject，短路，熔断，一定时间范围内就不允许访问了，直接降级，自动恢复的机制 降级：报错，超时，reject，熔断，降级，服务提供容错的机制 限流：在你的服务里面，通过线程池，或者信号量，限制对某个后端的服务或资源的访问量，避免从你的服务这里过去太多的流量，打死某个资源 超时：避免某个依赖服务性能过差，导致大量的线程hang住去调用那个服务，会导致你的服务本身性能也比较差 Hystrix Circuit Breaker配置（1）circuitBreaker.enabled控制短路器是否允许工作，包括跟踪依赖服务调用的健康状况，以及对异常情况过多时是否允许触发短路，默认是true 1HystrixCommandProperties.Setter().withCircuitBreakerEnabled(boolean value) （2）circuitBreaker.requestVolumeThreshold设置一个rolling window，滑动窗口中，最少要有多少个请求时，才触发开启短路 举例来说，如果设置为20（默认值），那么在一个10秒的滑动窗口内，如果只有19个请求，即使这19个请求都是异常的，也是不会触发开启短路器的 1HystrixCommandProperties.Setter().withCircuitBreakerRequestVolumeThreshold(int value) （3）circuitBreaker.sleepWindowInMilliseconds设置在短路之后，需要在多长时间内直接reject请求，然后在这段时间之后，再重新导holf-open状态，尝试允许请求通过以及自动恢复，默认值是5000毫秒 1HystrixCommandProperties.Setter().withCircuitBreakerSleepWindowInMilliseconds(int value) （4）circuitBreaker.errorThresholdPercentage设置异常请求量的百分比，当异常请求达到这个百分比时，就触发打开短路器，默认是50，也就是50% 1HystrixCommandProperties.Setter().withCircuitBreakerErrorThresholdPercentage(int value) （5）circuitBreaker.forceOpen如果设置为true的话，直接强迫打开短路器，相当于是手动短路了，手动降级，默认false 1HystrixCommandProperties.Setter().withCircuitBreakerForceOpen(boolean value) （6）circuitBreaker.forceClosed如果设置为ture的话，直接强迫关闭短路器，相当于是手动停止短路了，手动升级，默认false 1HystrixCommandProperties.Setter().withCircuitBreakerForceClosed(boolean value) Hystrix Circuit Breaker示例代码123456789101112131415161718192021222324@Slf4jpublic class GetECourseAuthorInfoFromRedisCmmd extends HystrixCommand&lt;ECourseAuthorInfo&gt; &#123; private int pkid; public GetECourseAuthorInfoFromRedisCmmd(int pkid)&#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("RedisGroup")) .andCommandPropertiesDefaults(HystrixCommandProperties.defaultSetter() //配合hystrix-dashboard收集的数据对TP99.5的请求延时如果是100ms， //那么再给一点重试时间50ms，那么就可以设置command执行超时为150ms .withExecutionTimeoutInMilliseconds(150) //circuitBreaker一个请求窗口持续10s。如果TP99的QPS为50，那么这个 //值可以设置为50*10=500 .withCircuitBreakerRequestVolumeThreshold(500) //默认值50，依赖服务的Command执行报错占比。 //可以根据业务场景修改，如果是对服务接口安全级别要求高的 //可以设置小一点,如支付接口，可以设置10，如果安全级别低的就设置大一点。 .withCircuitBreakerErrorThresholdPercentage(30) //断路器为open状态后持续多长时间直接拒绝请求，超过这个时间后状态改为half-open .withCircuitBreakerSleepWindowInMilliseconds(60*1000))); this.pkid = pkid; &#125; ... &#125;]]></content>
      <tags>
        <tag>hystrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashSet、HashMap 、ArrayList线程不安全与对策]]></title>
    <url>%2F2018%2F07%2F15%2FHashSet%E3%80%81HashMap%E3%80%81ArrayList%E7%BA%BF%E7%A8%8B%E4%B8%8D%E5%AE%89%E5%85%A8%E4%B8%8E%E5%AF%B9%E7%AD%96%2F</url>
    <content type="text"><![CDATA[测试用例设定2000个请求，分多次并发请求往容器内添加元素，使用信号量Semaphore控制并发量的大小，查看集合中元素数量是否等于2000，等于2000表示容器是并发线程安全的，小于2000表示容器是并发线程不安全的。 普通容器的线程不安全示例HashSet线程不安全示例代码123456789101112131415161718192021222324@Testpublic void testHashSet() throws InterruptedException &#123; int threadSize = 2000; HashSet set = new HashSet&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(500);//500的信号量相当于500个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); set.add(Thread.currentThread().getId()); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",set.size());&#125; 执行结果12...12:26:39.881 [main] INFO com.hledu.project1.front.concurrent.CollectionsConcurrent - 集合大小：1998 HashMap线程不安全示例代码123456789101112131415161718192021222324@Testpublic void testHashMap() throws InterruptedException &#123; int threadSize = 2000; HashMap&lt;Object, Object&gt; map = new HashMap&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(1000);//1000的信号量相当于1000个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); map.put(Thread.currentThread().getId(), Thread.currentThread().getId()); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",map.size());&#125; 执行结果12...12:35:48.188 [main] INFO com.hledu.project1.front.concurrent.CollectionsConcurrent - 集合大小：1999 ArrayList线程不安全示例代码123456789101112131415161718192021222324@Test public void testArrayList() throws InterruptedException &#123; int threadSize = 2000; ArrayList&lt;Object&gt; list = new ArrayList&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(500);//500的信号量相当于500个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); list.add(Thread.currentThread().getId()); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",list.size()); &#125; 执行结果12...12:44:06.468 [main] INFO com.hledu.project1.front.concurrent.CollectionsConcurrent - 集合大小：1999 synchronized线程安全解决方案synchronized关键字修饰要进行并发操作的代码，只允许同时只有一个线程获取到被修饰的代码执行锁，其他线程处于等待，直到已经获取到锁的线程执行完上锁的代码释放锁后，获取一把锁执行代码。可以解决集合的线程不安全问题。 代码123456789101112131415161718192021222324252627@Test public void testHashSet() throws InterruptedException &#123; int threadSize = 2000; HashSet set = new HashSet&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(500);//500的信号量相当于500个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); synchronized (this)&#123; set.add(count); &#125; semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",set.size()); &#125; 执行结果12...09:49:17.681 [main] INFO com.hledu.project1.front.concurrent.CollectionsConcurrent - 集合大小：2000 J.U.C的Lock机制的线程安全解决方案示例使用ReentrantLock解决并发线程不安全问题。与synchronized关键字比较： synchronized ReentrantLock 性能 jdk1.6以后进行了优化，增加了偏向锁和轻量级锁的优化，性能大幅提高。 性能与synchronized差异不大。 功能 简单的锁定功能，jvm自动释放锁。 有公平性、Condition获取线程状态等丰富功能，自由灵活的锁定功能，代码实现解锁。 易用性 因为功能简单，所以易用。 因为功能复杂，在必须使用其复杂功能时才能显出优势。 推荐 首选 备选 代码1234567891011121314151617181920212223242526272829303132@Test public void testLock() throws InterruptedException &#123; int threadSize = 2000; HashSet set = new HashSet&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Lock lock = new ReentrantLock(); final Semaphore semaphore = new Semaphore(500);//500的信号量相当于500个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); lock.lock(); try &#123; set.add(count); &#125; finally &#123; lock.unlock(); &#125; semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",set.size()); &#125; 执行结果12...10:31:40.058 [main] INFO com.hledu.project1.front.concurrent.CollectionsConcurrent - 集合大小：2000 同步容器的线程安全解决方案同步容器在底层执行代码增加了synchronized关键字保证线程安全。 HashSet转换为同步容器的方法代码12345678910111213141516171819202122232425@Test public void testSyncSet() throws InterruptedException &#123; int threadSize = 2000; Set set = Collections.synchronizedSet(new HashSet()); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(500);//500的信号量相当于500个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); set.add(count); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",set.size()); &#125; 执行结果12...10:30:20.787 [main] INFO com.hledu.project1.front.concurrent.CollectionsConcurrent - 集合大小：2000 HashMap转换为同步容器的方法代码112345678910111213141516171819202122232425//HashTable的key-value都不能为null@Test public void testHashTable() throws InterruptedException &#123; int threadSize = 2000; Map&lt;Object, Object&gt; map = new Hashtable&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(1000);//1000的信号量相当于1000个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); map.put(Thread.currentThread().getId(), Thread.currentThread().getId()); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",map.size()); &#125; 代码212345678910111213141516171819202122232425@Test public void testSyncMap() throws InterruptedException &#123; int threadSize = 2000; Map&lt;Object, Object&gt; map = Collections.synchronizedMap(new HashMap&lt;&gt;()); ExecutorService service = Executors.newFixedThreadPool(threadSize); Semaphore semaphore = new Semaphore(1000);//1000的信号量相当于1000个并发 CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); map.put(count, count); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",map.size()); &#125; 执行结果12...10:44:48.401 [main] INFO com.hledu.project1.front.concurrent.CollectionsConcurrent - 集合大小：2000 ArrayList转换为同步容器的方法代码1123456789101112131415161718192021222324@Test public void testVector() throws InterruptedException &#123; int threadSize = 2000; List&lt;Object&gt; list = new Vector&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); Semaphore semaphore = new Semaphore(500);//500的信号量相当于500个并发 CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); list.add(count); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 log.info("集合大小：&#123;&#125;",list.size()); &#125; 代码212345678910111213141516171819202122232425@Test public void testStack() throws InterruptedException &#123; int threadSize = 2000; List&lt;Object&gt; list = new Stack&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(500); final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); list.add(count); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",list.size()); &#125; 代码312345678910111213141516171819202122232425@Test public void testSyncList() throws InterruptedException &#123; int threadSize = 2000; List&lt;Object&gt; list = Collections.synchronizedList(new ArrayList&lt;&gt;()); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(500); final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); list.add(count); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",list.size()); &#125; 执行结果12...17:38:25.404 [main] INFO com.hledu.project1.front.concurrent.CollectionsConcurrent - 集合大小：2000 同步容器实现线程安全的缺点同步容器底层使用synchronized关键字修饰实现线程安全，但是synchronized实现线程安全会降低代码的执行性能，如果是高并发场景下，建议使用J.U.C的并发容器解决线程不安全问题。 并发容器的线程安全解决方案CopyOnWrite容器的线程安全解决方案CopyOnWrite容器在并发操作时首先会对数据进行拷贝，然后对数据进行CompareAndSet操作，当数据量比较大时，Copy和Set操作就会明显降低代码的性能。不推荐使用。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Test public void testCopyOnWriteArraySet() throws InterruptedException &#123; int threadSize = 2000; Set set = new CopyOnWriteArraySet&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(500); final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); set.add(count); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",set.size()); &#125;@Test public void testCopyOnWriteArrayList() throws InterruptedException &#123; int threadSize = 2000; List set = new CopyOnWriteArrayList&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(500);//500的信号量相当于500个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); set.add(count); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",set.size()); &#125; 执行结果12...18:09:39.398 [main] INFO com.hledu.project1.front.concurrent.CollectionsConcurrent - 集合大小：2000 Concurrent容器的线程安全解决方案线程安全，支持高并发场景的线程安全，推荐使用。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980//元素不能为null,支持元素排序 @Test public void testConcurrentSkipListSet() throws InterruptedException &#123; int threadSize = 2000; Set set = new ConcurrentSkipListSet(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(500);//500的信号量相当于500个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); set.add(count); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",set.size()); &#125; //适合高并发场景，但不支持元素排序 @Test public void testConcurrentMap() throws InterruptedException &#123; int threadSize = 2000; Map&lt;Object, Object&gt; map = new ConcurrentHashMap&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(1000);//1000的信号量相当于1000个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); map.put(count, count); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",map.size()); &#125; //性能相较于ConcurrentHashMap有所降低，支持元素排序，不能存放null，并发量越大，性能优势越明显 @Test public void testConcurrentSkipListMap() throws InterruptedException &#123; int threadSize = 2000; Map&lt;Object, Object&gt; map = new ConcurrentSkipListMap&lt;&gt;(); ExecutorService service = Executors.newFixedThreadPool(threadSize); final Semaphore semaphore = new Semaphore(1000);//1000的信号量相当于1000个并发 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; final int count = i; service.submit(()-&gt;&#123; try &#123; semaphore.acquire(); map.put(count, count); semaphore.release(); countDownLatch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.await();//主线程等待，直到countDownLatch被减到0，主线程再继续执行。 service.shutdown(); log.info("集合大小：&#123;&#125;",map.size()); &#125; 执行结果12...18:15:47.529 [main] INFO com.hledu.project1.front.concurrent.CollectionsConcurrent - 集合大小：2000]]></content>
      <tags>
        <tag>ThreadSafe</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker常用命令]]></title>
    <url>%2F2018%2F07%2F04%2FDocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[操作 命令 说明 运行 docker run –name container-name -d image-name eg:docker run –name myredis –d redis –name:自定义容器名 -d:后台运行 image-name:指定镜像模板 列表 docker ps(查看运行中的容器); 加上-a;可以查看所有容器 停止 docker stop container-name或者container-id 停止当前你运行的容器 启动 docker start container-name或者container-id 启动容器 删除 docker rm container-id 删除指定容器 端口映射 -p 6379:6379 eg:docker run -d -p 6379:6379 –name myredis docker.io/redis -p: 主机端口(映射到)容器内部的端口 容器日志 docker logs container-name或者container-id 更多命令 https://docs.docker.com/engine/reference/commandline/docker/]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nexus私服搭建]]></title>
    <url>%2F2018%2F07%2F02%2FNexus%E7%A7%81%E6%9C%8D%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[写在文前在没有配置Nexus私服的情况下，搭建maven工程后，通过pom.xml中配置的标签引入所需要的jar包，这样做会有两个很不理想的情况。 maven每次都要从网络上下载新引入的jar包，耗时耗力； jar在互联网上分布不够集中，就需要在maven的配置文件settings.xml中配置很多仓库镜像标签，对maven所依赖的jar包不方便管理。 Nexus的出现解决了上述两种问题，它作为Maven远程仓库的代理工具，可以把分散的镜像划入一个代理组内，然后再maven的配置文件中只需要配置这个代理组的私有仓库地址即可。 安装Nexus环境：CentOS 6.8 x86_64 安装包：nexus-2.14.4-03-bundle.tar.gz 安装过程： 123tar -zvxf nexus-2.14.4-03-bundle.tar.gzcd nexus-2.14.4-03/binvi nexus 如果要使用linux的root用户运行nexus，只需要修改以下代码： 运行nexus 12cd nexus-2.14.4-03/bin./nexus start 访问nexus http://ip:8081/nexus 管理nexuspublic repositories是一个公共的仓库代理组，只要把需要的第三方远程仓库以代理的形式添加到nexus中，并放入public repositories中，maven只需要依赖这一个代理仓库地址即可。 本地maven配置maven配置文件中的配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;localRepository&gt;~/.m2/repository&lt;/localRepository&gt; &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; &lt;offline&gt;false&lt;/offline&gt; &lt;pluginGroups&gt; &lt;pluginGroup&gt;org.apache.tomcat.maven&lt;/pluginGroup&gt; &lt;pluginGroup&gt;org.jenkins-ci.tools&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;public&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;url&gt;http://192.168.1.218:8081/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;public&lt;/id&gt; &lt;url&gt;http://nexus-public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;public&lt;/id&gt; &lt;url&gt;http://nexus-public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;jdk-1.8&lt;/activeProfile&gt; &lt;activeProfile&gt;nexus&lt;/activeProfile&gt; &lt;/activeProfiles&gt;&lt;/settings&gt; 项目中pom.xml配置在项目中pom.xml要使用私有仓库的代理地址和发布jar包的地址配置 12345678910111213141516171819&lt;!-- 公司Maven私服Nexus地址用于下载 --&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;public&lt;/id&gt; &lt;name&gt;Public Group&lt;/name&gt; &lt;url&gt;http://192.168.1.218:8081/nexus/content/groups/public&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!-- 公司Maven私服Nexus地址用于发布,需要maven的settings.xml中配置了release和snapshots的用户名和密码 --&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;url&gt;http://192.168.1.218:8081/nexus/content/repositories/releases&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;url&gt;http://192.168.1.218:8081/nexus/content/repositories/snapshots&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt;]]></content>
      <tags>
        <tag>Nexus</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot/Springcloud技术栈]]></title>
    <url>%2F2018%2F06%2F29%2FSpringboot-Springcloud%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[Spring Cloud预备（Preparation））技术体系 架构 面向服务架构（SOA） 微服务（MSA） 测试驱动开发（TDD) 领域驱动设计（DDD） 事件驱动架构（EDA） 技术 Java 8 基础 Spring Framework 4.x Spring Boot 1.x 环境依赖 Spring Cloud Dalston.SR3 Spring Boot 1.5.x Spring Framework 4.3.x Maven 3.5.0 Java = 8 原生云应用（Cloud Native Applications）知识储备 12-Factor 应用 链接: 12factor.net/ Codebase Dependencies Config Backing services Build, release, run Processes Port binding Concurrency Disposability Dev/prod parity Logs Admin processes Spring Framework ApplicationContext Spring Boot SpringApplication SpringApplicationBuilder Bootstrap 上下文 Bootstrap Application Context 理解 Application Context 层次性 端点（Endpoints） 上下文重启 /restart 生命周期回调 /pause /resume 分布式配置（Configuration）技术回顾 Spring Framework Environment PropertySources PropertySource @PropertySources @PropertySource @Profile Spring Boot application.properties 事件（Event） ApplicationEnvironmentPreparedEvent ApplicationPreparedEvent 监听器（Listener） ConfigFileApplicationListener Java 加密扩展（JCE） JCE 8 Spring Cloud 配置客户端 配置文件 bootstrap.yml bootstrap.properties Bootstrap 配置属性（Properties） 调整 Bootstrap 配置文件路径 覆盖远程配置属性 自定义 Bootstrap 配置 自定义 Bootstrap 配置属性源 安全 Spring Cloud 服务器 @EnableConfigServer Environment 仓储 分布式配置实现 Git 实现 认证（Authentication） 文件系统实现 自定义实现 Zookeeper 健康指标（Health Indicator） 加密和解密（Encryption and Decryption） 秘钥管理（Key Management） 创建Key @RefreshScope Bean @RefreshScope 使用场景 端点（Endpoints） Environment 端点 /env @RefreshScope 端点 /refresh 上下文重启 /restart 生命周期回调 /pause /resume 服务发现/注册（Discovery/Registry）核心理念 服务发现（Discovery） WebService UDDI REST HATEOAS Messaging JMS Java JNDI JINI Apache River 服务注册（Registry） SOA 服务提供方（Service Registry） 服务代理（Service Broker） 服务消费者（Service Consumer） 高可用（High Availability） 链接: en.wikipedia.org/wiki/High_availability 原则（Principles） 消除单点失败 可靠性交迭 故障探测 可用性比率计算 系统设计 服务发现（Discovery） 客户端 Netflix Eureka 引入 spring-cloud-starter-eureka API EurekaClient ServiceInstance Spring Cloud Commons API @EnableDiscoveryClient DiscoveryClient ServiceInstance 服务注册（Registry） 客户端 Netflix Eureka 激活 @EnableEurekaClient 区域化（zones） 链接: cloud.spring.io/spring-cloud-static/Dalston.SR3/#_zones 安全（Secure） 链接: cloud.spring.io/spring-cloud-static/Dalston.SR3/#_registering_a_secure_application 健康检查（Health Checks） 健康指标（Health Indicator） 服务端 Netflix Eureka 激活 @EnableEurekaServer 高可用 区域化 Zones Regions 负载均衡（Load Balance）核心理念 客户端（Client） Round-robin DNS 服务端（Server） 调度算法 先来先服务（First Come First Served） 最早截止时间优先（Earliest deadline first） 最短保留时间优先（Shortest remaining time first） 固定优先级（Fixed Priority） 轮训（Round-Robin） 多级别队列（Multilevel Queue） 特性（Features） 非对称负载（Asymmetric load） SSL 卸载和加速 分布式拒绝服务攻击保护 HTTP 压缩 TCP 卸载 TCP 缓冲 直接服务返回 健康检查 优先级队列 内容意识切换 客户端认证 编程式流量操作 防火墙 入侵防御系统 客户端 Netflix Ribbon 激活 @RibbonClient 负载均衡器 ILoadBalancer AbstractLoadBalancer BaseLoadBalancer DynamicServerListLoadBalancer ZoneAwareLoadBalancer 负载均衡客户端 LoadBalancerClient 自动装配 LoadBalancerAutoConfiguration 源码分析 RestTemplate 自定义器 RestTemplateCustomizer RestTemplate 请求拦截器 LoadBalancerInterceptor 标记注解 @LoadBalanced 实际请求客户端 LoadBalancerClient RibbonLoadBalancerClient 负载均衡上下文 LoadBalancerContext RibbonLoadBalancerContext 负载均衡器 ILoadBalancer BaseLoadBalancer DynamicServerListLoadBalancer ZoneAwareLoadBalancer NoOpLoadBalancer 负载均衡规则 核心规则接口 IRule 随机规则 RandomRule 最可用规则 BestAvailableRule 轮训规则 默认实现 RoundRobinRule 重试实现 RetryRule 客户端配置 ClientConfigEnabledRoundRobinRule 可用性过滤规则 AvailabilityFilteringRule RT权重规则 WeightedResponseTimeRule 规避区域规则 ZoneAvoidanceRule PING 策略 核心策略接口 IPingStrategy PING 接口 IPing 实现 NoOpPing DummyPing PingConstant PingUrl Discovery Client 实现 NIWSDiscoveryPing Spring Framework REST 客户端 模板类 RestTemplate 客户端请求拦截器 ClientHttpRequestInterceptor 服务短路（Circuit Breakers）核心理念 名词由来 Martin Fowler 短路器模式 更早由来 容错（Fault tolerance） 目的 系统整体性保护 近义词 熔断保护 容错保护 优雅降级 设计哲学 取舍之道 触发条件 错误类型 超时（Timeout） 异常（Exception） 性能类型 响应时间（RT） 处理手段 内容处理 空内容 容错内容 异常处理 程序异常 状态码 实现方式 客户端 服务端 服务端 Spring Cloud Hystrix 依赖 spring-cloud-starter-hystrix 激活 @EnableCircuitBreaker 命令 @HystrixCommand 应变方法 fallbackMethod 命令分组 groupKey 命令关联 Key commandKey 线程池关联 Key threadPoolKey 属性 HystrixProperty 通用属性 commandProperties 线程池属性 threadPoolProperties 忽略异常集合 ignoreExceptions 可观察执行模式 ObservableExecutionMode @HystrixCollapser 超时 指标 健康指标（Health Indicator） /health -&gt; hystrix 数据指标（Metrics） /hystrix.stream 生产特性（Production-Ready Features） Turbine 使用场景 指标聚合 端口 /turbine.stream 配置 turbine.aggregator.clusterConfig 激活 @EnableTurbine Turbine Stream 依赖 spring-cloud-netflix-hystrix-stream spring-cloud-starter-stream-* 激活 @EnableTurbineStream Spring Cloud Hystrix Dashboard 依赖 spring-cloud-starter-hystrix-dashboard 激活 @EnableHystrixDashboard 源码分析 Spring Cloud Hystrix TODO Netflix Hystrix TODO 服务调用（Service Call）核心理念 远程过程调用（RPC） 核心概念 接口定义语言（IDL） 根存（Stubs） 通讯协议 二进制协议 CORBA RMI EJB Hession Thrift 本文协议 XML-RPC WebService REST AJAX 客户端 Feign 灵感来源 Retrofit JAX-RS WebSocket Spring Cloud Feign 依赖 spring-cloud-starter-feign 激活 @EnableFeignClients 接口声明 @FeignClient 支持 Hystrix 配置 Ribbon 配置 源码分析 上下文 FeignContext 注解参数处理器 AnnotatedParameterProcessor 请求头 RequestHeaderParameterProcessor 请求参数 RequestParamParameterProcessor 路径变量 PathVariableParameterProcessor Hystrix 支持 命令 FallbackCommand Ribbon 支持 负载均衡 负载均衡器 FeignLoadBalancer 工厂 CachingSpringLoadBalancerFactory 客户端 LoadBalancerFeignClient 加码器（Encoder） 字符类型 SpringEncoder 解码器（Decoder） 字符类型 StringDecoder Spring MVC ResponseEntity ResponseEntityDecoder 请求拦截器 RequestInterceptor GZIP 处理 Accept-Encoding 请求头 FeignAcceptGzipEncodingInterceptor Content-Encoding 请求头 FeignContentGzipEncodingInterceptor BASIC 认证 BasicAuthRequestInterceptor 通用装配 FeignClientsConfiguration 自动装配 默认 FeignAutoConfiguration GZIP 处理 Accept-Encoding 请求头 FeignAcceptGzipEncodingAutoConfiguration Content-Encoding 请求头 FeignContentGzipEncodingAutoConfiguration Ribbon FeignRibbonClientAutoConfiguration 注册 处理对象 FeignClientsRegistrar 注解 候选注解 @FeignClient 激活 @EnableFeignClients 服务网关（Gateway）核心理念 使用场景 Web 网关 REST API 网关 WebService 网关 服务能力 监控（Monitoring） 测试（Testing） 压力测试（Stress Testing） A/B 测试（A/B Testing） 动态路由（Dynamic Routing） 路由规则 域名匹配 http://user.acme.com IP匹配 http://192.168.1.2 -&gt; user 路径匹配 http://www.acme.com/user 链接: acme.com/user 协议匹配 ftp://www.acme.com 链接: wwwacme.com 服务整合（Service Migration） 负荷减配（Load Shedding） 安全（Security） 认证（Authentication） 静态资源处理（Static Resources handling） 活跃流量管理（Active traffic management） 依赖 服务来源 服务发现 服务注册 服务通讯 协议 二进制 本文协议 方式 同步（Sync） 异步（Async） 服务接口 弱约束 XML JSON HTML 强约束 接口定义语言（IDL） RMI Hession 架构 SOA 微服务 事件驱动架构 类型 服务 ESB EJB Web 上游（Upstream） Apache Nginx 其他反向代理服务器 下游（Downstream） Java Web Server Java Servlet Tomcat Valve PHP Web Server .NET Web Server 服务端 Spring Cloud Zuul 用途 监控（Monitoring） 压力测试（Stress Testing） 动态路由（Dynamic Routing） 服务整合（Service Migration） 负荷减配（Load Shedding） 安全（Security） 认证（Authentication） 静态请求处理（Static Response handling） 活跃流量管理（Active traffic management） 依赖 spring-cloud-starter-zuul 激活 @EnableZuulProxy 合并注解 @EnableSidecar spring-cloud-netflix-sidecar @SpringCloudApplication 整合 Eureka Hystrix ZuulFallbackProvider 生命周期 初始化 ServletInitializer ZuulFilterInitializer 处理 ZuulServlet ZuulRunner ZuulFilter 销毁 移除 ZuulFilter 清楚 FilterLoader 缓存 路由 规则 配置 过程 Metrics Spectator spring-cloud-starter-spectator Servo Atlas 源码分析 通用 路由处理 代理运行器 ZuulRunner 实际执行器 ZuulFilter 执行顺序控制 filterOrder() 类型控制 filterType() 设计权限 字符类型描述类型 是否应该执行 shouldFilter() 执行 run() 类型 路由预处理 pre 路由 route 路由后处理 post 错误 error Zuul ZuulFilter 加载器 FilterLoader 加载ZuulFilter Java/Groovy 源文件 ZuulFilter 仓储 FilterRegistry Spring Cloud Zuul 路由处理 前端总控制器 Servlet ZuulServlet 装配 手动（@EnableZuulProxy） 配置 Bean（@Configuration） ZuulConfiguration 配置属性 Bean（@ConfigurationProperties） ZuulProperties 路由定位器 RouteLocator 前端总控制器 ZuulServlet ZuulFilter 内置实现 Bean DispatcherServlet 探测 ServletDetectionFilter Form 表单数据 FormBodyWrapperFilter 设计模式 J2EE 设计模式 前端控制器模式 GoF 23 责任链模式 代理模式 策略模式 消息驱动整合（Message-Driven Integration）核心理念消息总线（Message Bus）核心理念分布式跟踪（Distributed Tracing）核心理念]]></content>
      <tags>
        <tag>Springboot</tag>
        <tag>Springcloud</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7安装Docker-CE]]></title>
    <url>%2F2018%2F06%2F27%2FCentOS7%E5%AE%89%E8%A3%85Docker-CE%2F</url>
    <content type="text"><![CDATA[转【CentOS安装Docker CE】 环境要求To install Docker CE, you need the 64-bit version of CentOS 7.The centos-extras repository must be enabled. This repository is enabled by default, but if you have disabled it, you need to re-enable it. 卸载安装的所有Docker组件123[root@spark32 lib]# systemctl stop docker.service[root@spark32 lib]# yum remove docker docker-common docker-selinux docker-engine container-selinux[root@spark32 lib]# rm -rf docker/ 通过仓库安装Docker CE1.安装依赖包 1[root@spark32 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 2.下载docker yum源 1[root@spark32 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 3.更新软件缓存 1[root@spark32 ~]# yum makecache fast 4.安装Docker CE 1[root@spark32 ~]# yum install docker-ce -y 5.启动docker 1[root@spark32 ~]# systemctl start docker.service 6.查看docker版本信息 123456789101112[root@spark32 ~]# docker infoContainers: 0 Running: 0 Paused: 0 Stopped: 0Images: 0Server Version: 17.06.1-ceStorage Driver: overlay Backing Filesystem: extfs Supports d_type: trueLogging Driver: json-fileCgroup Driver: cgroupfs 【注意】:在生产环境中，可能需要使用某一个特定版本的Docker，而不是使用最新的版本，我们可以先列出版本，然后安装指定版本： 12[root@spark32 ~]# yum list docker-ce.x86_64 --showduplicates | sort -r[root@spark32 ~]# yum install docker-ce-&lt;VERSION&gt; 配置镜像加速国内访问 Docker Hub 有时会遇到困难，此时可以配置镜像加速器。国内很多云服务商都提供了加速器服务，例如： 阿里云加速器 DaoCloud 加速器 灵雀云加速器 以阿里云加速器为例1.首先进入阿里云docker库首页注册用户并登录，点击右上角“管理中心”。如果第一次，会提示“您还没有开通服务”，点击“确定”，然后会出现弹窗“初始化设置”，设置docker登录时使用的密码。 2.点击左侧菜单“Docker Hub镜像站点”，可以看到“您的专属加速器地址：https://6101v8g5.mirror.aliyuncs.com”，我们需要将其配置到Docker 引擎。 3.修改daemon配置文件/etc/docker/daemon.json来使用加速器： 123456[root@spark32 ~]# vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://6101v8g5.mirror.aliyuncs.com&quot;]&#125;[root@spark32 ~]# systemctl daemon-reload[root@spark32 ~]# systemctl restart docker 也可以配置docker中国的镜像源 123&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125; 卸载Docker CE12[root@spark32 ~]# yum remove docker-ce[root@spark32 ~]# rm -rf /var/lib/docker]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Nginx双层架构和数据缓存服务层组成的数据三级缓存策略]]></title>
    <url>%2F2018%2F06%2F20%2FNginx%E8%AF%B7%E6%B1%82%E5%88%86%E5%8F%91%E5%B1%82%E5%92%8C%E5%BA%94%E7%94%A8%E5%B1%82%E5%8F%8C%E5%B1%82%E6%9E%B6%E6%9E%84%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84%E5%BA%94%E7%94%A8%E5%B1%82%E6%95%B0%E6%8D%AE%E9%9D%99%E6%80%81%E5%8C%96%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[接续《基于OpenResty安装配置Nginx+LUA并实现请求分发的双层Nginx架构策略》 应用架构设计Nginx请求分发层+Nginx应用层（nginx cache）+业务数据两级缓存服务接口（ehcache+redis） Step1:应用服务器Nginx安装resty静态化模板应用服务器要从缓存数据中获取数据并渲染到静态化模板中，所以在应用层Nginx安装模板包。 12345cd /usr/hello/lualib/resty/wget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template.luamkdir /usr/hello/lualib/resty/htmlcd /usr/hello/lualib/resty/htmlwget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template/html.lua Step2：模板配置123456789101112131415161718192021cd /usr/hello/mkdir templatesvi hello.conf##在server配置中增加以下两行配置set $template_location &quot;/templates&quot;; set $template_root &quot;/usr/hello/templates&quot;;cd /usr/hello/templatesvi ecourse.html##放入以下内容ecourse id: &#123;* ecourseId *&#125;&lt;/br&gt; ecourse name: &#123;* ecourseName *&#125;&lt;/br&gt;ecourse price: &#123;* ecoursePrice *&#125;&lt;/br&gt;ecourse author name: &#123;* authorName *&#125;&lt;/br&gt;ecourse author type: &#123;* authorType *&#125;&lt;/br&gt;ecourse author addr: &#123;* authorAddr *&#125;&lt;/br&gt; ecourse author sex: &#123;* authorSex *&#125;&lt;/br&gt;ecourse author level: &#123;* authorLevel *&#125;&lt;/br&gt; 配置Nginx本地缓存的大小。Nginx会在缓存数据达到设置的大小后，进行LRU算法清理掉无效数据。还可以设置缓存数据过期时间，到期或者达到缓存设置的大小都会进行LRU清理缓存。 1234vi /usr/servers/nginx/conf/nginx.conf##在http中增加以下配置，eccache是自定义的cache namelua_shared_dict eccache 128m; Step3:应用层Nginx数据静态化过程LUA脚本12345678cd /usr/hello/vi hello.conf##增加一个新的location配置 location /ecourse/info &#123; default_type &apos;text/html&apos;; content_by_lua_file /usr/local/common/lua_scripts/hello/lua/ecourse.lua; &#125; 录入ecourse.lua脚本内容 1234567891011121314151617181920212223242526272829303132333435363738local uri_args = ngx.req.get_uri_args()local ecourseId = uri_args[&quot;ecourseId&quot;]### eccache就是在hello.conf中定义的cache namelocal cache_ngx = ngx.shared.eccachelocal ecourseCacheKey = &quot;ecourse_info_&quot;..ecourseIdlocal ecourseCache = cache_ngx:get(ecourseCacheKey)if ecourseCache == &quot;&quot; or ecourseCache == nil then local http = require(&quot;resty.http&quot;) local httpc = http.new() local resp, err = httpc:request_uri(&quot;http://192.168.1.103:8080&quot;,&#123; method = &quot;GET&quot;, path = &quot;/getEcourseInfo?ecourseId=&quot;..ecourseId &#125;) ecourseCache = resp.body cache_ngx:set(ecourseCacheKey, ecourseCache, 10 * 60)endlocal cjson = require(&quot;cjson&quot;)local ecourseCacheJSON = cjson.decode(ecourseCache)local context = &#123; ecourseId = ecourseCacheJSON.ecourseId, ecourseName = ecourseCacheJSON.ecourseName, ecoursePrice = ecourseCacheJSON.ecoursePrice, authorName = ecourseCacheJSON.authorName, authorType = ecourseCacheJSON.authorType, authorAddr = ecourseCacheJSON.authorAddr, authorSex = ecourseCacheJSON.authorSex, authorLevel = ecourseCacheJSON.authorLevel,&#125;local template = require(&quot;resty.template&quot;)template.render(&quot;ecourse.html&quot;, context) cache_ngx:set(ecourseCacheKey, ecourseCache, 10 * 60)设置了课程信息在nginx缓存中保留的时间为600s也就是10分钟。也就是说Nginx缓存保存的数据对时效性要求不能太高，对数据实时性要求很高的数据需要使用高并发请求转化成串型队列请求执行的逻辑，更新数据生产服务端的Redis缓存的方式实现，直接从Redis中读取数据，更好的实现数据状态的实时性 Step4:Nginx配置重新加载1/usr/servers/nginx/sbin/nginx -s reload Step5:数据缓存层实现查询数据因为ehcache是与数据缓存服务web实例绑定，多个数据缓存服务实例之间不能共享，所以查询接口要先从分布式缓存redis中获取数据，如果redis中数据为空，再从数据缓存服务实例的本地缓存ehcache中获取。 123456789101112131415161718192021222324252627282930313233343536@RequestMapping(&quot;/getEcourseInfo&quot;) @ResponseBody public JSONObject getEcourseInfo(Integer ecourseId)&#123; ECourseInfo eCoureseInfo = ecCacheService.getECoureseInfoFromRedis(ecourseId); log.info(&quot;=======从redis中查询课程信息=========&quot;); if (eCoureseInfo == null) &#123; log.info(&quot;=======从ehcache中查询课程信息=========&quot;); eCoureseInfo = ecCacheService.getECoureseInfoFromEhcache(ecourseId); &#125; //如果从两级缓存中还是查不到，应该去数据生产服务中查询，注意并发查询更新缓存的冲突问题。 if (eCoureseInfo == null) &#123; //some code &#125; Integer authorId = eCoureseInfo.getAuthorId(); ECourseAuthorInfo eCourseAuthorInfo = ecCacheService.getECourseAuthorInfoFromRedis(authorId); log.info(&quot;=======从redis中查询作者信息=========&quot;); if (eCourseAuthorInfo == null) &#123; log.info(&quot;=======从ehcache中查询作者信息=========&quot;); eCourseAuthorInfo = ecCacheService.getECourseAuthorInfoFromEhcache(authorId); &#125; //如果从两级缓存中还是查不到，应该去数据生产服务中查询，注意并发查询更新缓存的冲突问题。 if (eCourseAuthorInfo == null) &#123; //some code &#125; JSONObject ecourseInfoJson = new JSONObject(); ecourseInfoJson.put(&quot;ecourseId&quot;,eCoureseInfo.getPkid()); ecourseInfoJson.put(&quot;ecourseName&quot;,eCoureseInfo.getName()); ecourseInfoJson.put(&quot;ecoursePrice&quot;,eCoureseInfo.getPrice()); ecourseInfoJson.put(&quot;authorName&quot;,eCourseAuthorInfo.getName()); ecourseInfoJson.put(&quot;authorType&quot;,eCourseAuthorInfo.getType()); ecourseInfoJson.put(&quot;authorAddr&quot;,eCourseAuthorInfo.getAddr()); ecourseInfoJson.put(&quot;authorSex&quot;,eCourseAuthorInfo.getSex()); ecourseInfoJson.put(&quot;authorLevel&quot;,eCourseAuthorInfo.getLevel()); return ecourseInfoJson; &#125; Step6:测试use case1：启动数据缓存层web服务192.168.1.103:8080,然后在浏览器中请求分发层Nginx代理服务http://192.168.1.212/course/info?ecourseId=1,分别查看Nginx分发层和应用层请求的路由过程。查看数据缓存服务后台日志： 12342018-06-20 12:08:07.081 INFO 3963 --- [nio-8080-exec-1] c.k.eccache.controller.CacheController : =======从ehcache中查询课程信息=========2018-06-20 12:08:07.081 INFO 3963 --- [nio-8080-exec-1] c.k.eccache.controller.CacheController : =======从redis中查询课程信息=========2018-06-20 12:08:07.167 INFO 3963 --- [nio-8080-exec-1] c.k.eccache.controller.CacheController : =======从ehcache中查询作者信息=========2018-06-20 12:08:07.167 INFO 3963 --- [nio-8080-exec-1] c.k.eccache.controller.CacheController : =======从redis中查询作者信息========= 浏览器显示： 12345678ecourse id: 1ecourse name: 你要弄懂的学习力ecourse price: 29.99ecourse author name: 你要弄懂的学习力ecourse author type: 2ecourse author addr: 北京海淀ecourse author sex: 1ecourse author level: 8 use case2：数据缓存服务的本地缓存ehcache，在未达到maxElementsInMemory限制时，不会执行LRU数据清理。与此同时，应用层Nginx cache设置的数据过期时间为10分钟（为了测试方便可以缩短这个过期时间），在Nginx cache未过期时，在浏览器中再次请求Nginx分发层代理服务http://192.168.1.212/course/info?ecourseId=1，此时查看数据缓存服务端后台日志。没有日志输出 浏览器显示（乱码问题忽略）： 12345678ecourse id: 1ecourse name: 你要弄懂的学习力ecourse price: 29.99ecourse author name: 你要弄懂的学习力ecourse author type: 2ecourse author addr: 北京海淀ecourse author sex: 1ecourse author level: 8 use case3：等待应用层Nginx cache过期后，再次请求分发层Nginx代理服务http://192.168.1.212/course/info?ecourseId=1，此时再查看数据缓存服务后台日志，此时数据缓存服务只需要从本地缓存ehcache中读取数据。 122018-06-20 12:08:37.505 INFO 3963 --- [nio-8080-exec-2] c.k.eccache.controller.CacheController : =======从ehcache中查询课程信息=========2018-06-20 12:08:37.505 INFO 3963 --- [nio-8080-exec-2] c.k.eccache.controller.CacheController : =======从ehcache中查询作者信息========= 浏览器显示： 12345678ecourse id: 1ecourse name: 你要弄懂的学习力ecourse price: 29.99ecourse author name: 你要弄懂的学习力ecourse author type: 2ecourse author addr: 北京海淀ecourse author sex: 1ecourse author level: 8 use case4：给数据缓存服务从发送消息，更改课程价格： 1ThreadPool.Singleton.getInstance().submit(new KafkaProducer(&quot;kafkaConsume&quot;,&quot;abc&quot;,&quot;&#123;serviceId:&apos;ecourseInfoServiceUpdate&apos;,price:19.99&#125;&quot;)); 1234log.info(&quot;=======保存课程信息ehcache缓存数据：【&quot;+ JSONObject.toJSONString(eCourseInfo)+&quot;】=========&quot;); ecCacheService.saveECourseInfo2Ehcache(eCourseInfo);log.info(&quot;=======保存课程信息redis缓存数据：【&quot;+ JSONObject.toJSONString(eCourseInfo)+&quot;】=========&quot;);ecCacheService.saveECourseInfo2Redis(eCourseInfo); 数据缓存服务后台日志输出： 1232018-06-20 12:41:47.032 INFO 4010 --- [pool-1-thread-2] c.k.eccache.kafka.KafkaMessageProcessor : =======收到消息[&#123;serviceId:&apos;ecourseInfoServiceUpdate&apos;,price:19.99&#125;]========2018-06-20 12:41:47.160 INFO 4010 --- [pool-1-thread-2] c.k.eccache.kafka.KafkaMessageProcessor : =======保存课程信息ehcache缓存数据：【&#123;&quot;authorId&quot;:1,&quot;name&quot;:&quot;初二物理&quot;,&quot;pkid&quot;:1,&quot;price&quot;:19.99&#125;】=========2018-06-20 12:41:47.201 INFO 4010 --- [pool-1-thread-2] c.k.eccache.kafka.KafkaMessageProcessor : =======保存课程信息redis缓存数据：【&#123;&quot;authorId&quot;:1,&quot;name&quot;:&quot;初二物理&quot;,&quot;pkid&quot;:1,&quot;price&quot;:19.99&#125;】========= 此时再此请求分发层Nginx代理服务http://192.168.1.212/course/info?ecourseId=1，查看数据缓存层后台日志： 122018-06-20 12:44:56.516 INFO 4010 --- [nio-8080-exec-3] c.k.eccache.controller.CacheController : =======从ehcache中查询课程信息=========2018-06-20 12:44:56.516 INFO 4010 --- [nio-8080-exec-3] c.k.eccache.controller.CacheController : =======从ehcache中查询作者信息========= 此时如果应用层Nginx cache未过期，浏览器仍然显示课程价格29.99。 12345678ecourse id: 1ecourse name: 你要弄懂的学习力ecourse price: 29.99ecourse author name: 你要弄懂的学习力ecourse author type: 2ecourse author addr: 北京海淀ecourse author sex: 1ecourse author level: 8 如果应用层Nginx cache已经过期，浏览器显示课程价格为19.99。 12345678ecourse id: 1ecourse name: 你要弄懂的学习力ecourse price: 19.99ecourse author name: 你要弄懂的学习力ecourse author type: 2ecourse author addr: 北京海淀ecourse author sex: 1ecourse author level: 8 use case5：浏览显示查询到的数据后，如果在Nginx cache过期时间内，停掉数据缓存服务，那么在浏览器刷新请求http://192.168.1.212/course/info?ecourseId=1，应用层Nginx从本地缓存中查询到数据，就不会再请求数据缓存服务，即使停掉数据缓存服务，浏览器也能够正常显示数据。]]></content>
      <tags>
        <tag>OpenResty</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于OpenResty安装配置Nginx+LUA并实现请求分发的双层Nginx架构策略]]></title>
    <url>%2F2018%2F06%2F19%2F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AENginx-LUA%E5%8F%8C%E5%B1%82Ngnix%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[环境准备台CentOS6.x192.168.1.210192.168.1.211192.168.1.212网络拓扑210和211作为应用层web服务器212作为网络请求分发代理服务器 Step1:安装Linux依赖1yum install -y readline-devel pcre-devel openssl-devel gcc Step2:安装Nginx Openresty12345678910111213141516171819wget http://openresty.org/download/ngx_openresty-1.7.7.2.tar.gz tar -xzvf ngx_openresty-1.7.7.2.tar.gz cd /usr/servers/ngx_openresty-1.7.7.2/cd bundle/LuaJIT-2.1-20150120/ make clean &amp;&amp; make &amp;&amp; make install ln -sf luajit-2.1.0-alpha /usr/local/bin/luajitcd bundle wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz tar -xvf 2.3.tar.gz cd bundle wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz tar -xvf v0.3.0.tar.gz cd /usr/servers/ngx_openresty-1.7.7.2 ./configure --prefix=/usr/servers --with-http_realip_module --with-pcre --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2 make &amp;&amp; make install Step3:检查安装并启动安装完成后有以下目录： 123456cd /usr/servers/ ll/usr/servers/luajit/usr/servers/lualib/usr/servers/nginx 检查nginx版本 1/usr/servers/nginx/sbin/nginx -V 启动nginx: 1/usr/servers/nginx/sbin/nginx Step4:Nginx添加LUA配置使LUA配置按照工程化目录结构进行配置。 123mkdir /usr/hellocp -r /usr/servers/lualib /usr/hello/ 打开nginx.conf配置文件 1vi /usr/servers/nginx/conf/nginx.conf 在http部分添加： 123lua_package_path &quot;/usr/hello/lualib/?.lua;;&quot;; lua_package_cpath &quot;/usr/hello/lualib/?.so;;&quot;; include /usr/hello/hello.conf; 创建hello的lua配置和脚本 1234567891011vi /usr/hello/hello.confserver &#123; listen 80; server_name _; location /hello/test &#123; default_type &apos;text/html&apos;; content_by_lua_file /usr/hello/lua/test.lua; &#125; &#125; 编辑lua脚本: 12345mkdir /usr/hello/luavi /usr/hello/lua/test.luangx.say(&quot;hello world&quot;) Nginx检查配置是否正确 123[root@eshop-cache01 hello]# /usr/servers/nginx/sbin/nginx -t nginx: the configuration file /usr/servers/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/servers/nginx/conf/nginx.conf test is successful Nginx重新加载配置生效 1/usr/servers/nginx/sbin/nginx -s reload Step5:访问LUA配置的location路径1curl http://192.168.1.210/hello/test 返回test.lua中脚本输出的“hello world”。 Step6:分发层Nginx安装LUA脚本实现请求分发我们作为一个流量分发的nginx，会发送http请求到后端的应用nginx上面去，所以要先引入lua http lib包。 123cd /usr/hello/lualib/resty/ wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http_headers.lua wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua 在分发层Nginx写LUA脚本： 1234567891011121314151617181920212223242526272829303132333435nano /usr/hello/lua/test.lualocal uri_args = ngx.req.get_uri_args()local ecourseId = uri_args[&quot;ecourseId&quot;]local host = &#123;&quot;192.168.1.210&quot;, &quot;192.168.1.211&quot;&#125;local hash = ngx.crc32_long(ecourseId)hash = (hash % 2) + 1backend = &quot;http://&quot;..host[hash]local paras = &quot;&quot;;local request_args_tab = ngx.req.get_uri_args()for k, v in pairs(request_args_tab) do paras=paras..k..&quot;=&quot;..v..&quot;&amp;&quot;endlocal requestPath = ngx.var.urirequestPath = requestPath..&quot;?&quot;..paraslocal http = require(&quot;resty.http&quot;)local httpc = http.new()local resp, err = httpc:request_uri(backend, &#123; method = &quot;GET&quot;, path = requestPath&#125;)if not resp then ngx.say(&quot;request error :&quot;, err) returnendngx.say(resp.body)httpc:close() Nginx重新加载配置生效 1/usr/servers/nginx/sbin/nginx -s reload 测试请求分发,在浏览器地址栏输入 http://192.168.1.212/hello/test?ecourseId=1http://192.168.1.212/hello/test?ecourseId=2http://192.168.1.212/hello/test?ecourseId=3http://192.168.1.212/hello/test?ecourseId=4查看返回的结果，请求被分发到应用层web服务器节点了。根据ecourseId与应用层web服务节点数取模，找到对应的应用层服务器节点。 Step7:按照相同的方法部署另外两台机器的Nginx安装过程，略…应用层Nginx添加http请求功能包即可。 123cd /usr/hello/lualib/resty/ wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http_headers.lua wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua 补充：LUA脚本获取Nginx Http请求的相关参数说明1.获取当前请求的url相关信息 12345678910111213141516171819202122232425262728function test()-- 这个变量等于包含一些客户端请求参数的原始URI，它无法修改，请查看$uri更改或重写URI。local request_uri = ngx.var.request_uri log(tools.gbk_to_u8(&quot;获取当前请求的url==&quot;) .. tools.u8_to_gbk(cjson.encode(request_uri)) ) -- HTTP方法（如http，https）。按需使用，例： local scheme = ngx.var.scheme server_addr log(tools.gbk_to_u8(&quot;获取当前请求的url scheme==&quot;) .. tools.u8_to_gbk(cjson.encode(scheme)) ) -- 服务器地址，在完成一次系统调用后可以确定这个值，如果要绕开系统调用，则必须在listen中指定地址并且使用bind参数。 local server_addr = ngx.var.server_addruri log(tools.gbk_to_u8(&quot;获取当前请求的url server_addr==&quot;) .. tools.u8_to_gbk(cjson.encode(server_addr)) )-- 请求中的当前URI(不带请求参数，参数位于$args)，可以不同于浏览器传递的$request_uri的值，它可以通过内部重定向，或者使用index指令进行修改。 local uri = ngx.var.uri log(tools.gbk_to_u8(&quot;获取当前请求的url uri==&quot;) .. tools.u8_to_gbk(cjson.encode(uri)) ) -- 服务器名称 local server_name = ngx.var.server_name log(tools.gbk_to_u8(&quot;获取当前请求的url server_name ==&quot;) .. tools.u8_to_gbk(cjson.encode(server_name )) -- 请求到达服务器的端口号。local server_port = ngx.var.server_name log(tools.gbk_to_u8(&quot;获取当前请求的url server_port ==&quot;) .. tools.u8_to_gbk(cjson.encode(server_port )) endtest() 2.获取发送请求端过来的url相关信息 1234567-- 获取远程的IP地址。local remote_addr = ngx.var.remote_addr log(m_uuid,tools.gbk_to_u8(&quot;获取发送请求过来的远程请求remote_addr ==&quot;) .. tools.u8_to_gbk(cjson.encode(remote_addr )) ) -- 获取远程的端口号 local remote_port = ngx.var.remote_port log(m_uuid,tools.gbk_to_u8(&quot;获取发送请求过来的远程请求remote_port ==&quot;) .. tools.u8_to_gbk(cjson.encode(remote_port )) )]]></content>
      <tags>
        <tag>Nginx</tag>
        <tag>LUA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis LRU算法详情]]></title>
    <url>%2F2018%2F06%2F14%2FRedis-LRU%E7%AE%97%E6%B3%95%E8%AF%A6%E6%83%85%2F</url>
    <content type="text"><![CDATA[什么是LRU算法LRU-Leaste Recently Use，最近使用最少。算法原理略。 Redis LRU算法redis在存储数据时达到最大内存设置maxmemory后，自身会清理数据，以减少内存使用量，保证Reids服务的正常状态。Redis 内存使用策略maxmemory-policy可配置项： 123456noeviction 如果内存使用达到了maxmemory，client继续写入数据，那么Redis就直接报错给客户端allkeys-lru 移除掉最近最少使用的那些key对应的数据volatile-lru 仅对设置了过期时间的key进行LRU清理allkeys-random 随机删除一些key的数据volatile-random 随机删除一些设置了过期时间的key对应的数据volatile-ttl 删除哪些过期时间比较短的key对应的数据 配置项多个，但实际应用场景中大多使用allkeys-lru策略 maxmemory-sample配置Redis在进行LRU算法清理数据时，需要对多个key进行采样，选出采样key中最近使用最少的key进行数据清理，maxmemory-sample选项就是用来配置采样大小的。Redis给出了3\5\10三个可配置的值，配置为3执行LRU速度更快，但是精确率不高，5是一个速度和精确率适中的配置，10使得LRU算法精确率更高，但是会消耗更多的CPU 在实际应用中，会选择配置10，消耗更多CPU以获得更精确的LRU数据清理]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis-trib.rb工具的使用]]></title>
    <url>%2F2018%2F06%2F14%2Fredis-trib-rb%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[【转】https://www.jianshu.com/p/21f67bd739cc【官方】http://www.redis.cn/topics/cluster-tutorial.html 参数配置12345678910111、create：创建集群2、check：检查集群3、info：查看集群信息4、fix：修复集群5、reshard：在线迁移slot6、rebalance：平衡集群节点slot数量7、add-node：将新节点加入集群8、del-node：从集群中删除节点9、set-timeout：设置集群节点间心跳连接的超时时间10、call：在集群全部节点上执行命令11、import：将外部redis数据导入集群 创建集群用户无需指定哪台节点为master，哪台节点为slave，因为redis内部算法帮我们实现了 create–replicas # 可选参数，replicas表示每个master需要有几个slave。 只有master节点的创建方式 1./redis-trib.rb create 192.168.66.2:7000 192.168.66.2:7001 192.168.66.2:7002 192.168.66.3:7003 192.168.66.3:7004 192.168.66.3:7005 使用 –replicas 1 创建 每个master带一个 slave 指令 1./redis-trib.rb create --replicas 1 192.168.66.2:7000 192.168.66.2:7001 192.168.66.2:7002 192.168.66.3:7003 192.168.66.3:7004 192.168.66.3:7005 检查集群情况check1./redis-trib.rb check 192.168.66.2:7000 #后面的IP与端口,随便指定集群中的一个就行了。 查看集群信息info1./redis-trib.rb info 192.168.66.2:7000 输出： 12[OK] 1 keys in 4 masters.0.00 keys per slot on average. 使用reshard在线迁移slot1234567host:port：这个是必传参数，用来从一个节点获取整个集群信息，相当于获取集群信息的入口。--from &lt;arg&gt;：需要从哪些源节点上迁移slot，可从多个源节点完成迁移，以逗号隔开，传递的是节点的node id，还可以直接传递--from all，这样源节点就是集群的所有节点，不传递该参数的话，则会在迁移过程中提示用户输入。--to &lt;arg&gt;：slot需要迁移的目的节点的node id，目的节点只能填写一个，不传递该参数的话，则会在迁移过程中提示用户输入。--slots &lt;arg&gt;：需要迁移的slot数量，不传递该参数的话，则会在迁移过程中提示用户输入。--yes：设置该参数，可以在打印执行reshard计划的时候，提示用户输入yes确认后再执行reshard。--timeout &lt;arg&gt;：设置migrate命令的超时时间。--pipeline &lt;arg&gt;：定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10。 1./redis-trib.rb reshard --from all --to 80b661ecca260c89e3d8ea9b98f77edaeef43dcd --slots 11 平衡集群节点slot数量rebalance12345678host:port：这个是必传参数，用来从一个节点获取整个集群信息，相当于获取集群信息的入口。--weight &lt;arg&gt;：节点的权重，格式为node_id=weight，如果需要为多个节点分配权重的话，需要添加多个--weight &lt;arg&gt;参数，即--weight b31e3a2e=5 --weight 60b8e3a1=5，node_id可为节点名称的前缀，只要保证前缀位数能唯一区分该节点即可。没有传递–weight的节点的权重默认为1。--auto-weights：这个参数在rebalance流程中并未用到。--threshold &lt;arg&gt;：只有节点需要迁移的slot阈值超过threshold，才会执行rebalance操作。具体计算方法可以参考下面的rebalance命令流程的第四步。--use-empty-masters：rebalance是否考虑没有节点的master，默认没有分配slot节点的master是不参与rebalance的，设置--use-empty-masters可以让没有分配slot的节点参与rebalance。--timeout &lt;arg&gt;：设置migrate命令的超时时间。--simulate：设置该参数，可以模拟rebalance操作，提示用户会迁移哪些slots，而不会真正执行迁移操作。--pipeline &lt;arg&gt;：与reshar的pipeline参数一样，定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10。 增加一个主节点1./redis-trib.rb add-node 192.168.66.3:7006 192.168.66.2:7000 添加成功，但是并没有指定 slot ,所以必须 迁移slot节点 1./redis-trib.rb reshard 192.168.66.2:7000 提示一 ：How many slots do you want to move (from 1 to 16384)?为了平衡每个master管理的slot的个数，所以输入 16384/master 的数量。如这里为4 那么就是 16384/4 = 4096个。 输入 4096 提示二：What is the receiving node ID?(接受的node ID是多少) 890d2c8d989cce50e5fa48e37cd35738887f3f7d 输入7006的ID 提示三： Please enter all the source node IDs. Type ‘all’ to use all the nodes as source nodes for the hash slots. Type ‘done’ once you entered all the source nodes IDs. （要从哪个节点中获取lost ？）不打算从特定的节点上取出指定数量的哈希槽， 那么可以输入 all，否则输入某个节点的 node ID 检查是否成功 1./redis-trib.rb check 192.168.66.2:7000 增加一个从节点这样创建从节点会自动匹配主节点 1./redis-trib.rb add-node --slave 127.0.0.1:7007 127.0.0.1:7000 增加从节点的时候指定主节点。 1./redis-trib.rb add-node --slave --master-id 890d2c8d989cce50e5fa48e37cd35738887f3f7d 192.168.66.3:7008 192.168.66.2:7000 从集群中删除节点del-node12host:port：从该节点获取集群信息。node_id：需要删除的节点id。 例如 1./redis-trib.rb del-node 192.168.66.2:7000 d5f6d1d17426bd564a6e309f32d0f5b96962fe53 宕机情况当某个从节点挂掉之后，对于redis集群来说几乎没有什么影响，相当于这个从节点对应的 主节点少了一个备份而已。当某一个主节点挂掉之后，redis 会从这个 主节点 的 多个从节点 中推选一个出来，担当master的工作，并且把之前依附在主节点的从节点调整依附到新的master上。如果新任的master也挂掉并且他没有从节点了，那么这个集群也真正的挂掉了。集群创建时 replicas 参数 指定情况。使用 –replicas 1 参数时，如果节点数量少于六个。报错 1234*** ERROR: Invalid configuration for cluster creation.*** Redis Cluster requires at least 3 master nodes.*** This is not possible with 5 nodes and 1 replicas per node.*** At least 6 nodes are required. 使用 –replicas 1 参数时，如果节点数量 大于六个，且为单数时。这样会造成某个master拥有两个salve]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>Redis集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库和缓存双写数据不一致问题分析和解决方案]]></title>
    <url>%2F2018%2F06%2F07%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8C%E7%BC%93%E5%AD%98%E5%8F%8C%E5%86%99%E5%AF%BC%E8%87%B4%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[低并发读写—更新数据和删除缓存的双写导致数据不一致step1：更新数据库中的数据step2：删除缓存如果step1执行成功，step2执行失败，那么客户端获取缓存中的数据就是旧数据，而数据库中是新数据，造成数据不一致。 解决方案：先删除缓存，在执行数据库更新，保证缓存和数据库中数据一致。 高并发读写—更新数据和删除缓存双写导致数据不一致低并发的读写策略在高并发场景下也会出现数据不一致的问题，图片中展示的操作步骤就会出现此类问题。step1：删除缓存step2：更新数据库 如果step1执行完，step2正在执行的过程中还未flush到数据库，此时，客户端来一个请求获取正在更新的数据，缓存中已经删除了该数据，那么服务端会转到后端服务从数据库中加载旧数据并存到缓存一份，恰巧缓存完成后，step2执行完成了，该数据在数据库被更新了，那么就导致数据库和缓存中的数据是不一致的。 解决方案：把并发读写的的指令在服务层内存做成串行队列按顺序执行，在内存中执行效率高，不会明显影响数据服务的响应速度。把被执行更新操作的对象id做hash，然后根据队列数取模，对应该对象的操作指令队列。（可采取nginx的hash路由功能，保证在读取每个对象数据时，每次都从同一个内存队列中执行） 读指令堆积的内存队列： 进一步优化，因为服务接口每次查询都会把数据放入缓存，那么有一个更新后堆积了大量的查询操作，就会造成数据读取性能。可以在查询操作在内存队列存储之前，先查找队列上一个指令是不是update操作，如果是update操作，那么就直接存储当前select操作，如果是select操作，那么就把当前的查询服务等待200ms后，等待上一个select完成把数据存入缓存，当前select转向查询缓存数据。 优化后，出现update指令堆积的内存队列： PS：需要做数据演练，监视内存队列中update指令和最后一个select指令的个数，判断最后一个select需要等待前面n个update操作完成后的时间间隔t（单个update请求执行时间t/n应该小于10ms），t不能超过系统预期要求（通常要求一个读请求在200ms内完成）。如果内存队列堆积的update操作指令数据量太大，那么就需要考虑增加更多机器处理内存队列中的update操作，减少队列堆积量，提高服务接口的响应速度。 主机异常导致内存队列数据丢失的解决方案采用异步内存阻塞队列，按顺序执行数据读写指令，当队列中存在写指令时，主机发生宕机，就会导致在内存队列中未写入数据库的数据丢失。 解决方案：为解决这个问题，可以在执行写指令时，先把数据写入一个独立的数据库缓冲集群中，等到内存队列中的数据写入到数据库后，再删除数据库缓冲表中的数据。如果内存队列所在主机发生宕机，机器在重启后，服务启动时启动监听器Listener，从数据库缓冲表中的数据写入指令加载到内存阻塞队列。]]></content>
      <tags>
        <tag>数据不一致</tag>
        <tag>数据丢失</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis生产环境常见问题及优化]]></title>
    <url>%2F2018%2F06%2F06%2FRedis%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[fork导致的Redis延时增大Redis在进行RDB和AOF持久化时，为了使主进程继续提供读写服务，Redis就会fork一个子进程进行RDB和AOF持久化，子进程会拷贝Redis主进程的全量数据，此时Redis会阻塞执行读写服务，如果Redis中存储的数据量过大，那么在拷贝时耗时会增大，因为阻塞导致Redis的响应速度从几毫秒延迟到几百或者达到1秒。 解决方案：单个Redis实例分配的内存空间不要大于10G，降低fork给Redis带来的延迟影响 一主多从导致的复制风暴问题如果一个Master带有n多个Slava，可能会出现Slave在某一个时间节点，同时对Master进行复制，那么就会导致网络带宽被复制数据占满，导致Redis服务不可用。 解决方案：把围绕一个Master创建n个Slave的网络拓扑结构进行优化，减少1个Master的Slave节点。Redis集群只支持一层Slave，如果要使用更多的Slave节点，那么就不能使用集群，而使用Redis的主从复制能力进行纵向扩展，给Slave节点创建Slave节点，使Redis节点成树形结构。 AOF持久化导致的Redis阻塞问题Redis的AOF持久化方式，把数据以Redis写指令日志的形式先被写入os cache，然后进每秒进行fsync操作，把数据从os cache落入磁盘。Redis自身会对fsync的状态进行检查，如果两次fsync操作的时间间隔超过2s，那么Redis的写数据服务就会被阻塞，整个Redis的写服务就会被严重拖慢。 解决方案： 造成两次fsync操作的间隔超过两秒，是因为磁盘的I/O性能不足所导致数据不能高效地从os cache落入磁盘，可以通过更换SSD固态硬盘，提高磁盘的I/O来提高fsync的数据写入速度，减少延迟。 主从复制的数据同步状态延迟较大Redis在主从复制过程中，可能因为网络等例外因素导致主从复制延迟过大问题，影响Redis的数据一致性。 解决方案：人为介入，写定时任务脚本检查主从复制的offset值之间的差值，当差值达到预设阈值时，进行报警通知，排查估值，使主从复制的数据延迟缩小到有效范围内。 Redis申请内存的linux内核参数优化-vm.overcommit_memory看一下redis启动日志的警告提醒： 1225483:M 27 May 13:41:39.856 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &apos;vm.overcommit_memory = 1&apos; to /etc/sysctl.conf and then reboot or run the command &apos;sysctl vm.overcommit_memory=1&apos; for this to take effect. 0：检查有没有足够的内存，没有的话申请内存失败。例如fork子进程时申请内存，可能因为申请不到内存导致fork操作失败。1：允许使用内存，直到用完为止。常用配置2：内存地址使用率不能超过swap+50% 按照redis的警告提醒进行修改： 123cat /proc/sys/vm/overcommit_memoryecho &quot;vm.overcommit_memory=1&quot; &gt;&gt; /etc/sysctl.confsysctl vm.overcommit_memory=1 Redis申请内存的linux内核参数优化-swappiness设置linux在内存不足时，是否杀掉进程以获得更多空间。查看linux内核版本 1cat /proc/version linux内核版本&lt;3.5，swappiness设置为0，linux内核版本&gt;=3.5，swappiness设置为1，这样在内存不足时lunix进行swap操作，也不会杀掉进程，保证Redis进程不会被linux杀掉。 修改方法： 12echo 0 &gt; /proc/sys/vm/swappinessecho vm.swappiness=0 &gt;&gt; /etc/sysctl.conf linux打开的文件句柄数修改125525:M 27 May 15:48:29.008 * Increased maximum number of open files to 10032 (it was originally set to 1024). linux系统默认的打开句柄数最大为1024，修改为10032。 1ulimit -n 10032 10032 linux内核进行TCP Socket连接通信参数优化-tcp backlogbacklog在Redis主从复制时，用来缓存每次进行socket连接复制的数据和offset。 125483:M 27 May 13:41:39.856 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 修改 1echo 511 &gt;&gt; /proc/sys/net/core/somaxconn PS:TCP socket服务开发的4个步骤 socket-&gt;bind-&gt;listen-&gt;accept调用listen函数时,有一个backlog参数，此时TCP请求的连接状态是ESTABLISHED，TCP backlog值表示的就是ESTABLISHED状态的请求连接所在的队列大小，在linux内核中/proc/sys/net/core/somaxconn进行设置。]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>Redis集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis Cluster维护]]></title>
    <url>%2F2018%2F06%2F01%2FRedis-Cluster%E7%BB%B4%E6%8A%A4%2F</url>
    <content type="text"><![CDATA[增加Redis Cluster Master节点在安装了Redis Cluster集群环境机器上执行增加节点命令。redis-trib.rb add-node NEW_NODE ANY_NODE_OF_CLUSTER 1234567891011121314151617181920212223242526272829redis-trib.rb add-node 192.168.1.212:7007 192.168.1.212:7005[root@eshop-cache01 init.d]# redis-trib.rb add-node 192.168.1.212:7007 192.168.1.211:7003&gt;&gt;&gt; Adding node 192.168.1.212:7007 to cluster 192.168.1.211:7003&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.211:7003)M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 279addebc5ef0f0d7beef81d86201e3f8874fbd7 192.168.1.210:7001 slots: (0 slots) slave replicates 389568b7d660edfb016521d2c06aa898733a28c0S: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768S: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 slots: (0 slots) slave replicates b843b0530233e861dc674f1d78856a0de1f5c438M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)M: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 slots:0-5460 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 192.168.1.212:7007 to make it join the cluster.[OK] New node added correctly. 再次查看集群状态 1234567891011121314151617181920212223242526272829redis-trib.rb check 192.168.1.211:7003[root@eshop-cache01 init.d]# redis-trib.rb check 192.168.1.212:7007&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.212:7007)M: db864e7965225405f65f36c1cd0a0bb32f2df6ce 192.168.1.212:7007 slots: (0 slots) master 0 additional replica(s)M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 slots:0-5460 (5461 slots) master 1 additional replica(s)S: 279addebc5ef0f0d7beef81d86201e3f8874fbd7 192.168.1.210:7001 slots: (0 slots) slave replicates 389568b7d660edfb016521d2c06aa898733a28c0M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 slots: (0 slots) slave replicates b843b0530233e861dc674f1d78856a0de1f5c438S: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 但是新增Master节点的slots=0，那么需要重新规划Redis Cluster Hash Slots. 12345redis-trib.rb reshard 192.168.1.211:7004...How many slots do you want to move (from 1 to 16384)? 迁移多少个Slot，这里是指新的Master节点分配多少个Slot，可以用16384除以4（4个Master节点）得到的整数结,当然也可以根据实际情况配置合适的Slot个数。 后面还需要输入从哪几个Master源获取Slot，输入Master Node ID，最后一个需要结束时输入done，Redis Cluster重新规划Hash Slot。 查看重新规划后的Slot，每个Master都是4096个。 1234567891011121314151617181920212223242526272829redis-trib.rb check 192.168.1.212:7005[root@eshop-cache01 init.d]# redis-trib.rb check 192.168.1.212:7005&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.212:7005)M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:12288-16383 (4096 slots) master 1 additional replica(s)M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:6827-10922 (4096 slots) master 1 additional replica(s)S: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 slots: (0 slots) slave replicates b843b0530233e861dc674f1d78856a0de1f5c438M: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 slots:1365-5460 (4096 slots) master 1 additional replica(s)S: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768S: 279addebc5ef0f0d7beef81d86201e3f8874fbd7 192.168.1.210:7001 slots: (0 slots) slave replicates 389568b7d660edfb016521d2c06aa898733a28c0M: db864e7965225405f65f36c1cd0a0bb32f2df6ce 192.168.1.212:7007 slots:0-1364,5461-6826,10923-12287 (4096 slots) master 0 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 给Master节点增加Slaveredis-trib.rb add-node –slave –master-id MASTER_ID NEW_NODE ANY_NODE_OF_CLUSTER 123456789101112131415161718192021222324252627282930313233[root@eshop-cache01 init.d]# redis-trib.rb add-node --slave --master-id 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7008 192.168.1.211:7003&gt;&gt;&gt; Adding node 192.168.1.211:7008 to cluster 192.168.1.211:7003&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.211:7003)M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:6827-10922 (4096 slots) master 1 additional replica(s)M: db864e7965225405f65f36c1cd0a0bb32f2df6ce 192.168.1.212:7007 slots:0-1364,5461-6826,10923-12287 (4096 slots) master 0 additional replica(s)S: 279addebc5ef0f0d7beef81d86201e3f8874fbd7 192.168.1.210:7001 slots: (0 slots) slave replicates 389568b7d660edfb016521d2c06aa898733a28c0S: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768S: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 slots: (0 slots) slave replicates b843b0530233e861dc674f1d78856a0de1f5c438M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:12288-16383 (4096 slots) master 1 additional replica(s)M: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 slots:1365-5460 (4096 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 192.168.1.211:7008 to make it join the cluster.Waiting for the cluster to join.&gt;&gt;&gt; Configure node as replica of 192.168.1.211:7004.[OK] New node added correctly. 检查新增的slave节点 123456789101112131415161718192021222324252627282930[root@eshop-cache01 init.d]# redis-trib.rb check 192.168.1.211:7003&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.211:7003)M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:6827-10922 (4096 slots) master 1 additional replica(s)M: db864e7965225405f65f36c1cd0a0bb32f2df6ce 192.168.1.212:7007 slots:0-1364,5461-6826,10923-12287 (4096 slots) master 1 additional replica(s)S: 279addebc5ef0f0d7beef81d86201e3f8874fbd7 192.168.1.210:7001 slots: (0 slots) slave replicates db864e7965225405f65f36c1cd0a0bb32f2df6ceS: 7fe0f90658458209f39578981b32d24a223e26b1 192.168.1.211:7008 slots: (0 slots) slave replicates 389568b7d660edfb016521d2c06aa898733a28c0S: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768S: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 slots: (0 slots) slave replicates b843b0530233e861dc674f1d78856a0de1f5c438M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:12288-16383 (4096 slots) master 1 additional replica(s)M: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 slots:1365-5460 (4096 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 删除集群节点step1:迁移要删除节点Hash Slotredis-trib.rb reshard ANY_NODE_OF_CLUSTER 1redis-trib.rb reshard 192.168.1.211:7004 把要删除的Master节点Slots分配给其他Master step2：删除节点redis-trib.rb del-node ANY_NODE_OF_CLUSTER DEL_NODE_ID 删除新增的7007节点（Master节点） 12345[root@eshop-cache01 init.d]# redis-trib.rb del-node 192.168.1.210:7001 db864e7965225405f65f36c1cd0a0bb32f2df6ce&gt;&gt;&gt; Removing node db864e7965225405f65f36c1cd0a0bb32f2df6ce from cluster 192.168.1.210:7001&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node. step3：查看集群 123456789101112131415161718192021222324252627[root@eshop-cache01 init.d]# redis-trib.rb check 192.168.1.210:7001&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.210:7001)S: 279addebc5ef0f0d7beef81d86201e3f8874fbd7 192.168.1.210:7001 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768S: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:5461-6825,12288-16383 (5461 slots) master 1 additional replica(s)M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:6826-12287 (5462 slots) master 2 additional replica(s)M: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 slots:0-5460 (5461 slots) master 1 additional replica(s)S: 7fe0f90658458209f39578981b32d24a223e26b1 192.168.1.211:7008 slots: (0 slots) slave replicates 389568b7d660edfb016521d2c06aa898733a28c0S: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 slots: (0 slots) slave replicates b843b0530233e861dc674f1d78856a0de1f5c438[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 被删除的Master节点的Slave节点会自动被Redis Cluster挂在到其他Maser节点上去，本例中看到Redis Cluster 7003节点多了一个从节点。 123M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:6826-12287 (5462 slots) master 2 additional replica(s) 在删除的7007节点机器上查看redis进程发现，7007的Redis已经被Redis Cluster自动关闭了。 1234[root@eshop-cache03 init.d]# ps -ef | grep redisroot 25143 1 0 May26 ? 00:00:47 /usr/local/bin/redis-server 192.168.1.212:7005 [cluster]root 25149 1 0 May26 ? 00:00:30 /usr/local/bin/redis-server 192.168.1.212:7006 [cluster]root 25272 25091 0 03:57 pts/1 00:00:00 grep redis]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>Redis集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RedisCluster配置和部署--创建多master多slave高可用集群环境]]></title>
    <url>%2F2018%2F06%2F01%2FRedisCluster%E9%85%8D%E7%BD%AE%E5%92%8C%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Redis的主从配置的主要作用是实现读写分离，并且可以通过配置一主多从来实现Redis横向扩展，保证Redis读取数据的QPS能够达到10W+。如果应用的Redis写数据量QPS只需要几万，而读数据要几十万或上百万QPS，那么Redis Sentinel集群就可以满足业务需求，一主多从，而且可以实现自动Master主备切换，实现高可用。当写数据的QPS需要几十万或者上百万QPS，或者写数据量达到1T以上的场景，Sentinel的一主多从就不能满足业务需求，此时需要更多的Master来支撑写操作。 Redis Cluster实现多主多从的集群架构，并且能够进行自动故障节点转移和恢复，并支持多Master节点写入，能够支撑更大数据量的写操作。虽然Redis Cluster实现了读写分离的主从结构，但是Redis的客户端Jedis操作对读写分离支持不够完善，需要对Jedis进行扩展才能支持读写分离。而且Redis Cluster本身slave节点不能直接读取数据，需要先执行readonly命令，再读取数据。所以Redis Cluster的重点在实现Master节点的横向扩展支撑更大数据量，能够通过Master节点实现读写操作，Slave节点的作用主要用是数据热备和在Master节点故障时的主备自动切换，保证Redis Cluster的高可用。 创建配置文件Redis Cluster的用处可以实现多主多从，实现读写分离，实现Redis高可用。 官方推荐至少3个master，3个slave，这样可以让数据冗余1份。 需要n个节点的redis集群，就创建n份redis配置文件。For example：7001.conf 7002.conf 7003.conf 7004.conf 7005.conf 7006.conf 创建备用目录 12345mkdir -pv /etc/redis-clutermkdir -pv /var/log/redismkdir -pv /var/redis/7001mv /usr/local/common/redis-3.28/redis.conf /etc/redis/7001.confvi /etc/redis/7001.conf 修改7001.conf以下配置项 12345678910port 7001cluster-enabled yescluster-config-file /etc/redis-cluster/node-7001.confcluster-node-timeout 15000daemonize yespidfile /var/run/redis_7001.piddir /var/redis/7001logfile /var/log/redis/7001.logbind 192.168.1.210appendonly yes 创建启动脚本123cp /etc/init.d/redis_6379 /etc/init.d/redis_7001vi redis_7001 修改启动脚本中使用的端口号即可。 12345...chkconfig redis_7001 onREDISPORT=7001... 启动 1./redis_7001 start 创建集群step1：安装ruby环境 12345678910111213yum install gcc-c++ patch readline readline-devel zlib zlib-devel \ libyaml-devel libffi-devel openssl-devel make \ bzip2 autoconf automake libtool bison iconv-devel sqlite-devel wget https://cache.ruby-lang.org/pub/ruby/2.3/ruby-2.3.4.tar.gztar zxf ruby-2.3.4.tar.gzcd ruby-2.3.4./configure --prefix=/usr/local/rubymakemake installcd /usr/local/rubycp bin/ruby /usr/local/bin/cp bin/gem /usr/local/bin/ step2：使用ruby安装redis集群redis-trib 12wget http://rubygems.org/downloads/redis-3.3.0.gemgem install -l redis-3.3.0.gem 此时在redis-3.2.8/src/下出现一个redis-trib.rb文件 1cp /usr/local/common/redis-3.2.8/src/redis-trib.rb /usr/local/bin/ 使用redis-trib.rb创建集群，–replicas 1表示每个master有1个slave，并且会让master和slave尽量不在一台机器上。 1redis-trib.rb create --replicas 1 192.168.1.210:7001 192.168.1.210:7002 192.168.1.211:7003 192.168.1.211:7004 192.168.1.212:7005 192.168.1.212:7006 输出以下集群信息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:192.168.1.210:7001192.168.1.211:7003192.168.1.212:7005Adding replica 192.168.1.211:7004 to 192.168.1.210:7001Adding replica 192.168.1.210:7002 to 192.168.1.211:7003Adding replica 192.168.1.212:7006 to 192.168.1.212:7005M: 279addebc5ef0f0d7beef81d86201e3f8874fbd7 192.168.1.210:7001 slots:0-5460 (5461 slots) masterS: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 replicates 434f96e83bea38adeb97a5df293da3b008f76768M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:5461-10922 (5462 slots) masterS: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 replicates 279addebc5ef0f0d7beef81d86201e3f8874fbd7M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:10923-16383 (5461 slots) masterS: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 replicates b843b0530233e861dc674f1d78856a0de1f5c438Can I set the above configuration? (type &apos;yes&apos; to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join....&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.210:7001)M: 279addebc5ef0f0d7beef81d86201e3f8874fbd7 192.168.1.210:7001 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 slots: (0 slots) slave replicates 279addebc5ef0f0d7beef81d86201e3f8874fbd7S: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 slots: (0 slots) slave replicates b843b0530233e861dc674f1d78856a0de1f5c438[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 集群创建成功集群环境支持master+slave的读写分离，支持master宕机的，slave自动切换成master，支持Redis Hash Slot分布式算法，支持Redis服务高可用。 查看集群状态 12345678910111213141516171819202122232425[root@eshop-cache01 src]# redis-trib.rb check 192.168.1.210:7001&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.210:7001)M: 279addebc5ef0f0d7beef81d86201e3f8874fbd7 192.168.1.210:7001 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 slots: (0 slots) slave replicates 279addebc5ef0f0d7beef81d86201e3f8874fbd7S: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 slots: (0 slots) slave replicates b843b0530233e861dc674f1d78856a0de1f5c438[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 干掉其中一个master，在看集群状态 123456789101112131415161718192021222324252627[root@eshop-cache01 src]# ps -ef | grep redisroot 2300 1 0 12:16 ? 00:00:05 /usr/local/bin/redis-server 192.168.1.210:7001 [cluster]root 2309 1 0 12:16 ? 00:00:06 /usr/local/bin/redis-server 192.168.1.210:7002 [cluster]root 16856 2229 0 13:54 pts/2 00:00:00 grep redis[root@eshop-cache01 src]# kill -9 2300[root@eshop-cache01 src]# rm -rf /var/run/redis_7001.pid [root@eshop-cache01 src]# redis-trib.rb check 192.168.1.210:7002&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.210:7002)S: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 slots: (0 slots) slave replicates b843b0530233e861dc674f1d78856a0de1f5c438M: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 slots:0-5460 (5461 slots) master 0 additional replica(s)M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 恢复启动7001的redis，再看集群状态： 123456789101112131415161718192021222324252627[root@eshop-cache01 src]# cd /etc/init.d/[root@eshop-cache01 init.d]# ./redis_7001 startStarting Redis server...[root@eshop-cache01 init.d]# redis-trib.rb check 192.168.1.210:7001&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.210:7001)S: 279addebc5ef0f0d7beef81d86201e3f8874fbd7 192.168.1.210:7001 slots: (0 slots) slave replicates 389568b7d660edfb016521d2c06aa898733a28c0S: d2e15ab71bbe2cca6cce19a5bacd92944368d230 192.168.1.210:7002 slots: (0 slots) slave replicates 434f96e83bea38adeb97a5df293da3b008f76768M: b843b0530233e861dc674f1d78856a0de1f5c438 192.168.1.212:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s)M: 434f96e83bea38adeb97a5df293da3b008f76768 192.168.1.211:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: 389568b7d660edfb016521d2c06aa898733a28c0 192.168.1.211:7004 slots:0-5460 (5461 slots) master 1 additional replica(s)S: 70ae536a26cd28807a41b2009755f613d9691322 192.168.1.212:7006 slots: (0 slots) slave replicates b843b0530233e861dc674f1d78856a0de1f5c438[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. Redis集群数据操作数据插入会根据key的hash值与16384取摩，分配到固定的slot值所在的master节点，例如加入k1 v1可以存在7001节点，k2 v2只能存在7005节点，那么通过7001节点的客户端操作set k1 v1可以存入，但是set k2 v2就会返回数据要存入哪个master节点的路由信息。 123[root@eshop-cache01 init.d]# redis-cli -h 192.168.1.211 -p 7003192.168.1.211:7003&gt; set k2 v2(error) MOVED 449 192.168.1.211:7004 这样操作数据会很麻烦，不过redis cluster为了能让redis操作起来更简单，可以进行一下操作。在使用redis-cli连接redis cluster时，加上-c的这个参数就可以了。操作会返回key的hash slot值，并带有存储到哪个master节点的路由信息。 1234[root@eshop-cache01 init.d]# redis-cli -h 192.168.1.211 -p 7003 -c192.168.1.211:7003&gt; set k2 v2-&gt; Redirected to slot [449] located at 192.168.1.211:7004OK 查看master下的slave节点，并在slave节点读取数据。 12345678910192.168.1.211:7003&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=192.168.1.210,port=7002,state=online,offset=16769,lag=1master_repl_offset:16769repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:2repl_backlog_histlen:16768 连接slave节点 123[root@eshop-cache01 init.d]# redis-cli -h 192.168.1.210 -p 7002192.168.1.210:7002&gt; get k4(error) MOVED 8455 192.168.1.211:7003 直接获取k4的值，会提示需要从master节点7003获取的错误提示。此时需要先发送readonly指令，再执行get操作即可。 1234192.168.1.210:7002&gt; readonlyOK192.168.1.210:7002&gt; get k4&quot;v4&quot;]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>Redis集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentinels哨兵管理]]></title>
    <url>%2F2018%2F05%2F31%2FSentinels%E5%93%A8%E5%85%B5%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[删除哨兵节点step1：停掉哨兵进程ps -ef | grep redis-sentinelkill -9 PROCESSIDstep2：过一会，在其他哨兵节点执行哨兵重置，清理旧哨兵的状态。 123[root@eshop-cache01 ~]# redis-cli -h 192.168.1.210 -p 5000192.168.1.210:5000&gt; sentinel reset * (integer) 1 step3：在所有哨兵节点上执行以下命令，查看所有的哨兵数量是否一致。 1sentinel master mymaster Redis的slave节点下线停掉或删除Redis服务后，停掉哨兵节点，在其他所有哨兵节点上执行sentinel reset mymaster。]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>Sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Sentinel（哨兵）实现Redis主从切换的机制原理]]></title>
    <url>%2F2018%2F05%2F31%2F%E4%BD%BF%E7%94%A8Sentinel%EF%BC%88%E5%93%A8%E5%85%B5%EF%BC%89%E5%AE%9E%E7%8E%B0Redis%E4%B8%BB%E4%BB%8E%E5%88%87%E6%8D%A2%E7%9A%84%E6%9C%BA%E5%88%B6%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[哨兵-Sentinel，用于监控和管理Redis集群的高可用中间件。 哨兵配置和启动创建需要的目录 12345mkdir -pv /etc/sentinelmkdir -pv /var/sentinel/5000mkdir -pv /var/log/sentinel/5000/cp /usr/local/common/redis-3.2.8/sentinel.conf /etc/sentinel/5000.conf 5000.conf哨兵配置,可以配置多份，监控多个redis。 123456789101112131415161718#守护进程启动daemonize yes#哨兵日志logfile /var/log/sentinel/5000#哨兵绑定本机IPbind 192.168.1.212 #哨兵服务端口号port 5002#sentinel工作空间dir /var/sentinel/5000/#哨兵监控的master配置信息 master_redis_name master_redis_ip master_redis_port quorumsentinel monitor mymaster 192.168.1.210 6379 2#哨兵给master发送ping指令的超时时间，用来确定master的主观宕机sdownsentinel down-after-milliseconds mymaster 5000 #哨兵执行slave切换成master的失败超时，新的哨兵接替执行sentinel failover-timeout mymaster 60000#slave切换成master后，设置n个slave节点同时连接新master进行数据复制sentinel parallel-syncs mymaster 1 哨兵启动 1234567891011121314151617181920212223242526272829redis-sentinel /etc/sentinel/5000.conf2083:X 27 May 07:29:50.702 * Increased maximum number of open files to 10032 (it was originally set to 1024). _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 3.2.8 (00000000/0) 32 bit .-`` .-```. ```\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in sentinel mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 5000 | `-._ `._ / _.-&apos; | PID: 2083 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos; 2083:X 27 May 07:29:50.703 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.2083:X 27 May 07:29:50.703 # Sentinel ID is 79c3f195844be0b84babec19b2fa8a84cc07190d2083:X 27 May 07:29:50.703 # +monitor master mymaster 192.168.1.210 6379 quorum 22083:X 27 May 07:29:50.704 * +slave slave 192.168.1.211:6379 192.168.1.211 6379 @ mymaster 192.168.1.210 63792083:X 27 May 07:29:50.714 * +slave slave 192.168.1.212:6379 192.168.1.212 6379 @ mymaster 192.168.1.210 63792083:X 27 May 07:30:22.643 * +sentinel sentinel 6e8c9dbc97a385acbbb2cd640e63b0370d1a5c31 192.168.1.211 5000 @ mymaster 192.168.1.210 63792083:X 27 May 07:30:43.491 * +sentinel sentinel b273c5b89b90f01ca5b90bd167a8092dedfae69a 192.168.1.212 5000 @ mymaster 192.168.1.210 63792083:X 27 May 07:30:51.041 * +fix-slave-config slave 192.168.1.211:6379 192.168.1.211 6379 @ mymaster 192.168.1.210 6379 连接并查看哨兵信息 123456789101112131415161718192021222324252627282930313233343536373839404142redis-cli -h 192.168.1.210 -p 5000192.168.1.210:5000&gt; sentinel master mymaster 1) &quot;name&quot; 2) &quot;mymaster&quot; 3) &quot;ip&quot; 4) &quot;192.168.1.210&quot; 5) &quot;port&quot; 6) &quot;6379&quot; 7) &quot;runid&quot; 8) &quot;8e990a05ba6ab1e915870dffc9184b689e960c7d&quot; 9) &quot;flags&quot;10) &quot;master&quot;11) &quot;link-pending-commands&quot;12) &quot;0&quot;13) &quot;link-refcount&quot;14) &quot;1&quot;15) &quot;last-ping-sent&quot;16) &quot;0&quot;17) &quot;last-ok-ping-reply&quot;18) &quot;394&quot;19) &quot;last-ping-reply&quot;20) &quot;394&quot;21) &quot;down-after-milliseconds&quot;22) &quot;5000&quot;23) &quot;info-refresh&quot;24) &quot;159&quot;25) &quot;role-reported&quot;26) &quot;master&quot;27) &quot;role-reported-time&quot;28) &quot;381542&quot;29) &quot;config-epoch&quot;30) &quot;0&quot;31) &quot;num-slaves&quot;32) &quot;2&quot;33) &quot;num-other-sentinels&quot;34) &quot;2&quot;35) &quot;quorum&quot;36) &quot;2&quot;37) &quot;failover-timeout&quot;38) &quot;60000&quot;39) &quot;parallel-syncs&quot;40) &quot;1&quot; 其他命令 12345678#查看master下的slave信息sentinel slaves mymaster#mymaster下所有的哨兵sentinel sentinels mymaster#当前哨兵所指向的master地址和端口sentinel get-master-addr-by-name mymaster 哨兵转换机制n个哨兵分别对Redis的master节点发送ping命令，如果在is-master-down-after-milliseconds指定的连接超时后，哨兵会“主观”认为master节点宕机-主观宕机（subjective down=sdown)。在哨兵机制中配置一个quorum参数m，用来判断m个哨兵都认为master主观宕机sdown，此时master就被哨兵认为是“客观”上宕机了（objective down=odown)，也就完成了哨兵从主观宕机到客观宕机的转换。 多个哨兵节点之间的相互发现机制Redis具有Pub/Sub的发布订阅消息队列的能力，每个哨兵都会往[sentinel:hello]这个channel中发送状态消息，并且其他的哨兵节点都监听这个channel的消息，感知到其他哨兵的存在。每隔2秒钟，每隔哨兵都会往[sentinel:hello]的channel发送自己的host、ip、runid、对master的监控配置信息。 哨兵对Redis的slave节点自动纠正哨兵发现master宕机后，会把slave节点通过选举出新的master提供服务，并且其他slave节点都要连接到新的master上。 Redis的slave被转换为master的选举算法如果一个master被quorum个哨兵认为主观宕机sdown了，master也就是达到了客观宕机odown的条件，而且配置的大多数（majority配置大多数的个数）哨兵同意进行主备切换，那么就通过选举算法，选出一个slave切换成master。（1）与master断开时长过滤，如果一个slave跟master断开连接已经超过了down-after-milliseconds的10倍+master宕机的时长，那么slave就被认为不适合选举为master，这样的slave节点就会被过滤掉。(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state；（2）slave的优先级slave-priority,在redis配置文件中默认是100，越小优先级越高；（3）slave复制数据的offset，offset越大，优先级越高；（每个slave在与master同步后offset自动增加）（4）slave节点run_id越小，优先级越高。（每个redis实例，都会有一个runid） 哨兵接替进行主备切换的机制哨兵进行主备切换时，会从当选master的slave节点获取一个进行切换的版本号configuration epoch信息，如果进行操作主备切换的哨兵，执行切换的等待时间超过了failover-timeout配置，那么就会有新的哨兵接替进行主备切换，此时哨兵要重新获取configuration epoch版本号的值，保证每次主备切换版本号是唯一的。 master配置信息的传播哨兵把当选的slave节点转换成master，并且把新的master配置信息通过[sentinel:hello]这个channel发布出去，订阅这个channel的其他slave节点收到消息后，自动更改配置连接到新的master节点上。]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>Sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis主从复制原理]]></title>
    <url>%2F2018%2F05%2F29%2FRedis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[主从复制原理 PS: full resynchronization:新的slave节点连接master同步复制数据，是一个完整的数据同步过程，叫做full resynchronization操作。 backlog：master在进行主从复制时，先在本机内存生成backlog，并且把要每次复制给slave的数据保存到backlog一份，目的是要记录数据同步的位置标识offset，以便于全量复制失败后，能够再次进行断点续传的增量复制。backlog的大小为1M。 master run id：salve用来区分master节点是新数据同步还是已有数据同步，如果slave发现mater的run id发生了变化，那么slava就会进行全量数据复制。在slave执行命令，查看master的run id： 1234567891011121314151617181920127.0.0.1:6379&gt; info server# Serverredis_version:3.2.8redis_git_sha1:00000000redis_git_dirty:0redis_build_id:5da7c5438a3ad5deredis_mode:standaloneos:Linux 2.6.32-431.el6.i686 i686arch_bits:32multiplexing_api:epollgcc_version:4.4.7process_id:2119run_id:5b80bdc81f1624e2ee403c5ccfa0a542e99fbb3etcp_port:6379uptime_in_seconds:62uptime_in_days:0hz:10lru_clock:562478executable:/usr/local/bin/redis-serverconfig_file:/etc/redis/6379.conf 全量复制（1）master执行bgsave命令，在本地生成一份rdb数据快照。（2）master把rdb快照文件发送给slave，发送过程有超时配置。 master节点主从复制的超时设置： 1repl-timeout 60 默认配置，如果主从复制时间超过60s，那么slave就会复制失败，所以根据rdb文件大小、网卡I/O能力、带宽大小调节该参数。例如rdb文件6G，千兆网卡的I/O在每秒中100M，那么就需要传输60s才能复制完毕，因此就可能导致复制失败。 （3）master在拷贝并同步原有rdb数据时，会将新增数据写入内存缓冲区，等到原有数据同步完成后，再从内存缓冲区把新增数据同步给slave。 master节点主从复制的内存缓冲区设置： 123456789101112131415161718192021client-output-buffer-limit slave 256M 64m 60``` 在配置的复制持续事件60s内，master内存缓冲区消耗64M，或者一次性超过256M，那么就会停止复制，复制失败。所以可以根据master节点的硬件条件，优化该配置。内存大的机器，就配置的大一点，内存小的机器就配置的小一点。（4）slave接收rdb数据快照文件后，清空自己的旧数据，然后重新加载rdb到内存中，在数据没有完全加载完之前，slave仍然使用旧数据提供读写服务。（5）如果slave开启了AOF配置，那么就会立即执行BGREWRITEAOF，重新生成AOF文件。# 增量复制（1） 如果全量复制过程中，master和slave断开连接，那么slave重新连接master后，会触发增量复制。（2）master从backlog中可以得到数据已经同步到offset位置，那么直接发送未同步的数据给slave。（3）从节点执行psync命令获取master的runid和全局offset，如果master返回PSYNC_FULLRESYNC，则进行全量复制；如果返回PSYNC_CONTINUE，则进行增量复制。# 断点续传 Redis在主从数据同步过程中，master会在内存中创建backlog日志，把同步的数据位置标识offset记录在backlog中，master和slave都会记录masterid和offset，如果slave在同步数据过程中断开了与master之间的连接，等到再次连接时，slave会从上次记录的offset开始同步数据，而不是全部重新同步。 # 无磁盘操作设置Redis在默认配置下进行主从同步复制数据，只在内存中就能完成操作，不需要数据在磁盘落地。master在内存中拷贝一份完整的数据快照，通过slave socket发送给slave，slave在内存中加载数据。当然Redis也支持数据在磁盘落地的方式进行主从复制。需要修改以下配置： 无磁盘操作 根据英语语境肯定回答要使用无磁盘操作，就配置为no；如果使用磁盘操作，就配置为yes。repl-diskless-sync no 延迟多少秒后开始数据同步，目的是等待更多的slave节点连接上master一起同步数据。如果master已经完成完整数据快照的拷贝，那么新的slave节点就无法再连接master，直到这次同步完成为止。repl-diskless-sync-delay 5 ` 过期key数据同步master中的key在一定时间有效后会做过期处理，对于这样的过期key，master会向slave发送一条del操作指令，在slave节点删除过期key的数据。]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis数据灾备和恢复过程中踩到的雷]]></title>
    <url>%2F2018%2F05%2F28%2FRedis%E6%95%B0%E6%8D%AE%E7%81%BE%E5%A4%87%E5%92%8C%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B%E4%B8%AD%E8%B8%A9%E5%88%B0%E7%9A%84%E9%9B%B7%2F</url>
    <content type="text"><![CDATA[生产环境配置策略RDB 生成快照的频率策略要在每分钟生成一次，但是有多少key发生变化可根据自身业务量配置。save 60 1000 AOF 在生产环境一定要打开，与RDB同时运行，fsync策略为everysec。 生产环境备份策略RDB主要做冷备，用craontab执行定时脚本，做数据备份。备份数据保留48小时，脚本清理48小时以前的数据。每天备份当日的数据，每天的数据保存一个月。把所有数据再备份到远程云服务器上。 每小时备份脚本 12345678#!/bin/bashcur_date=`date +%Y%m%d%H`rm -rf /var/redis/6379/bak/hour/$cur_datecp /var/redis/6379/dump.rdb /var/redis/6379/bak/hour/$cur_datepre_date=`date -d -48hour +%Y%m%d%H`rm -rf /var/redis/6379/bak/hour/$pre_date 每天备份 12345678#!/bin/bashcur_date=`date +%Y%m%d`rm -rf /var/redis/6379/bak/hour/$cur_datecp /var/redis/6379/dump.rdb /var/redis/6379/bak/dayli/$cur_datepre_date=`date -d -1month +%Y%m%d`rm -rf /var/redis/6379/bak/dayli/$pre_date 创建定时脚本crontab -e 123[root@eshop-cache01 dayli]# crontab -e0 * * * * sh /usr/local/redis/redis_rdb_data_bak_hour.sh0 0 * * * sh /usr/local/redis/redis_rdb_data_bak_dayli.sh 踩过雷后的数据恢复流程NO.1雷：我们理解的redis优先使用aof文件恢复数据，如果aof文件没有数据，应该从rdb恢复数据。【错】 原因：在redis配置文件中appendonly为yes，把备份数据拷贝到redis数据目录下，启动redis服务，并没有加载备份数据，而是优先加载了aof空数据，恢复失败。 NO.2雷：既然优先使用aof恢复数据，那么我们把aof文件删除掉，再启动redis服务，恢复rdb数据。【错】 原因：即使删除了aof文件，redis在启动时也会优先检查是否存在aof文件，如果没有则创建一个空的aof文件并加载空数据。恢复失败。 NO.3雷：把redis配置的appendonly设置为no，再恢复rdb数据，启动redis加载rdb数据后，再停掉redis，修改配置appendonly为yes，再启动redis，恢复数据。【错】 原因：配置appendonly为no，那么redis的appendonly.aof文件将失效，其中的数据也将不是正确的数据，恢复rdb数据后，再把appendonly修改为yes，此时再重启redis服务，redis将加载appendonly.aof的错误数据。恢复失败。 正确的数据恢复流程停止redis服务 1redis-cli SHUDOWN 修改redis配置vi /etc/redis/6379.conf 123...appendonly no... 删除aof文件 1rm -rf /var/redis/6379/appendonly.aof 拷贝最新一个小时的数据到redis目录下命名为dump.rdb 1cp /var/redis/6379/bak/2019052600 /var/redis/6379/dump.rdb 启动redis服务 12cd /etc/init.d/./redis_6379 start 热配置redis的appendonly为yes，这是最重要的一步。 123456[root@eshop-cache01 src]# redis-cli 127.0.0.1:6379&gt; config get appendonly1) &quot;appendonly&quot;2) &quot;no&quot;127.0.0.1:6379&gt; config set appendonly yesOK 停止redis服务 1redis-cli SHUDOWN 修改redis配置vi /etc/redis/6379.conf 123...appendonly yes... 启动redis服务 12cd /etc/init.d/./redis_6379 start]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis持久化方式RDB和AOF比较应用]]></title>
    <url>%2F2018%2F05%2F25%2FRedis%E6%8C%81%E4%B9%85%E5%8C%96%E6%96%B9%E5%BC%8FRDB%E5%92%8CAOF%E6%AF%94%E8%BE%83%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[RDB持久化方式的工作原理设定时间间隔t内有n次key的操作检查，就进行持久化。例如save 60 1000，每个60秒有1000个key发生变化，则进行持久化。RDB在做持久化时，保存完整的一份数据快照，旧的快照文件被覆盖。 RDB持久化方案的优点（1）持久化的时间间隔可以由redis控制，所以可以按照一定的时间间隔把数据快照文件拷贝出来，更好的支撑数据冷备份。（2）RDB持久化文件是Redis数据文件，在做redis故障恢复时，数据加载效率更高。（3）Redis数据操作都在内存中进行，效率高。（4）Redis会创建一个副本进程（fork）进行数据持久化，持久化过程中，对redis本身的数据操作影响较小。 RDB持久化方式的缺点（1）两次持久化操作的间隔时间内，Redis发生故障，那么将丢失这个时间内的所有数据。–重点（2）当两次持久化操作间隔时间较长，Redis内可能产生大量新的数据，生成数据快照文件时，可能导致Redis数据服务暂停数秒。 AOF持久化方式的工作原理设定一个AOF持久化文件大小，Redis每隔1秒进行一次append only的完整数据指令日志的写文件持久化操作，当AOF文件大小达到指定大小后，Redis会使用LRU算法淘汰无效数据，缩小数据大小，重新创建一个AOF文件，旧的AOF文件就会删除。AOF在做持久化时，保存的是完整数据的写指令日志数据。 PS:现代操作系统的写文件操作过程是，先将文件数据写入到系统os cache缓存层，当os cache内的数据达到一定量后，再写入磁盘。 AOF持久化方式的优点（1）数据持久化每秒钟进行一次，即使Redis发生故障，丢失的数据只是1秒钟的数据。–重点（2）数据持久化时，数据从os cache中写入磁盘时，文件不易破损，即使破损，redis有可用的工具进行修复。 AOF持久化方式的缺点（1）Redis数据操作每次都些人os cache中，要比直接写入内存效率有所降低，导致Redis的吞吐量QPS略有下降。（2）AOF文件的rewrite时机不可控，所以不容易实现数据备份。（3）持久化保存的是全量数据的写指令日志，并非Redis数据文件，在Redis启动加载数据时，效率低下。–重点 RDB和AOF持久化方式选择两种方式，各有优缺点。RDB效率更高，但是丢失数据量会比较大；AOF的数据要进行写文件操作，效率略低，但是保存的持久化数据更完整。在实际应用中，通常会同时选择两种方案。可用方案：实现Redis主从集群，主节点用AOF，保证数据更好的完整性，即使出现故障，也能从故障中恢复数据，虽然AOF方式的Redis数据QPS会下降，但影响不大；从节点用短时间间隔（秒级）的RDB，从节点对外不提供服务，这样从节点出现故障的几率就会大大降低，RDB方式能够很好的实现数据文件的备份。]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 6.8 + MariaDB 10.0 with Galera Cluster+Keepalived高可用解决方案]]></title>
    <url>%2F2018%2F05%2F24%2FCentOS-6-8-MariaDB-10-0-with-Galera-Cluster-Keepalived%E9%AB%98%E5%8F%AF%E7%94%A8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[环境准备使用三台最小化安装的CentOS 6.8 x86_64新环境。 节点 节点名称 节点IP donor db1 192.168.1.150 node1 db2 192.168.1.151 node2 db3 192.168.1.152 Step 1- 设置MariaDB的yum安装源创建/etc/yum.repos.d/mariadb.repo CentOS 6 – 64bit系统mariadb.repo内容如下: 12345[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.0/centos6-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 For CentOS 6 – 32bit系统mariadb.repo内容如下: 12345[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.0/centos6-x86gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 Step 2 – 设置SELinux状态在开始安装之前，先把三台机器的SELinux状态设置为permissive，临时设置如下，重启失效。 1sudo setenforce 0 永久设置: 1vi /etc/selinux/config 12345678910# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced. default# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=permissive# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted Step 3 – 安装 MariaDB Galera Cluster 10.0使用yum安装MariaDB Galera Cluster，需要先安装socat工具包，用来确保yum可以找到安装源。CentOS 6用以下方式安装： 1sudo yum install -y http://dl.fedoraproject.org/pub/epel/6/x86_64/Packages/s/socat-1.7.2.3-1.el6.x86_64.rpm CentOS 7用以下方式安装： 1sudo yum install socat 开始安装数据库集群环境 1sudo yum install MariaDB-Galera-server MariaDB-client rsync galera Step 4: MariaDB安全性设置MariaDB是Mysql的一个分支，所以使用Mysql的命令启动数据库。 1sudo service mysql start 执行以下脚本，并按提示进行相应操作，提升数据库安全。设置root初始密码为。 1sudo /usr/bin/mysql_secure_installation 我把root用户密码设置为root。 Step 5 – 创建MariaDB Galera Cluster集群数据通信用户创建一个集群节点之间进行数据状态快照转移（State Transfer Snapshot – SST）所使用的用户账号。 1234567mysql -u root -pmysql&gt; DELETE FROM mysql.user WHERE user=&apos;&apos;;mysql&gt; GRANT ALL ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;dbpass&apos;;mysql&gt; GRANT USAGE ON *.* to galera@&apos;%&apos; IDENTIFIED BY &apos;galera&apos;;mysql&gt; GRANT ALL PRIVILEGES on *.* to galera@&apos;%&apos;;mysql&gt; FLUSH PRIVILEGES;mysql&gt; quit 在开发或测试环境中，使用%表示任意主机，也就是说允许root用户和galera用户可以从任意主机登访问数据库。出于安全考虑，你可以把%替换成你所允许的主机名称或者主机IP地址。 Step 6 – MariaDB Galera Cluster 集群配置停止Mysql服务 1sudo service mysql stop 先创建Donor节点的配置信息。Donor节点只用来同步数据，不能有外部链接操作Donor节点数据，保证Donor节点的健康状态，维护正常的集群运行状态。其他节点只需要修改wsrep_node_address=’192.168.1.150’和wsrep_node_name=’db1’两个配置即可，其他配置相同。 向/etc/my.cnf.d/server.cnf文件中添加以下配置信息: 1sudo cat &gt;&gt; /etc/my.cnf.d/servebinlog_format=ROW 123456789101112131415161718default-storage-engine=innodbinnodb_autoinc_lock_mode=2innodb_locks_unsafe_for_binlog=1query_cache_size=0query_cache_type=0bind-address=0.0.0.0datadir=/var/lib/mysqlinnodb_log_file_size=100Minnodb_file_per_tableinnodb_flush_log_at_trx_commit=2wsrep_provider=/usr/lib64/galera/libgalera_smm.sowsrep_cluster_address=&quot;gcomm://192.168.1.150,192.168.1.151,192.168.1.152&quot;wsrep_cluster_name=&apos;galera_cluster&apos;wsrep_node_address=&apos;192.168.1.150&apos;wsrep_node_name=&apos;db1&apos;wsrep_sst_method=rsyncwsrep_sst_auth=sst_user:dbpassEOF wsrep_cluster_address配置省略了默认端口4567。 其他两个节点所需要修改的配置： node1节点 : 12wsrep_node_address=&apos;192.168.1.151&apos;wsrep_node_name=&apos;db2&apos; node2节点 : 12wsrep_node_address=&apos;192.168.1.152&apos;wsrep_node_name=&apos;db3&apos; Step 7– 初始化Donor节点Donor节点是整个集群的优先初始化节点，只有Donor节点先启动，其他子节点才能正常加入到集群当中。启动Donor节点，需要配置‐‐wsrep-new-cluster参数。 1sudo /etc/init.d/mysql start --wsrep-new-cluster 运行以下命令，查看集群的运行状态。 1mysql -uroot -p -e&quot;show status like &apos;wsrep%&apos;&quot; 输出以下信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263+------------------------------+-----------------------------------------------+| Variable_name | Value |+------------------------------+-----------------------------------------------+| wsrep_local_state_uuid | 47b3fdaa-5f43-11e8-8cbd-62ce6e10f564 || wsrep_protocol_version | 8 || wsrep_last_committed | 2 || wsrep_replicated | 0 || wsrep_replicated_bytes | 0 || wsrep_repl_keys | 0 || wsrep_repl_keys_bytes | 0 || wsrep_repl_data_bytes | 0 || wsrep_repl_other_bytes | 0 || wsrep_received | 10 || wsrep_received_bytes | 2304 || wsrep_local_commits | 0 || wsrep_local_cert_failures | 0 || wsrep_local_replays | 0 || wsrep_local_send_queue | 0 || wsrep_local_send_queue_max | 1 || wsrep_local_send_queue_min | 0 || wsrep_local_send_queue_avg | 0.000000 || wsrep_local_recv_queue | 0 || wsrep_local_recv_queue_max | 2 || wsrep_local_recv_queue_min | 0 || wsrep_local_recv_queue_avg | 0.100000 || wsrep_local_cached_downto | 1 || wsrep_flow_control_paused_ns | 0 || wsrep_flow_control_paused | 0.000000 || wsrep_flow_control_sent | 0 || wsrep_flow_control_recv | 0 || wsrep_cert_deps_distance | 1.000000 || wsrep_apply_oooe | 0.000000 || wsrep_apply_oool | 0.000000 || wsrep_apply_window | 1.000000 || wsrep_commit_oooe | 0.000000 || wsrep_commit_oool | 0.000000 || wsrep_commit_window | 1.000000 || wsrep_local_state | 4 || wsrep_local_state_comment | Synced || wsrep_cert_index_size | 1 || wsrep_causal_reads | 0 || wsrep_cert_interval | 0.000000 || wsrep_incoming_addresses | 192.168.1.150:3306 || wsrep_desync_count | 0 || wsrep_evs_delayed | || wsrep_evs_evict_list | || wsrep_evs_repl_latency | 4.67e-06/9.14775e-06/1.1497e-05/2.63955e-06/4 || wsrep_evs_state | OPERATIONAL || wsrep_gcomm_uuid | 559ef6a3-5f45-11e8-92a5-52514b1a7147 || wsrep_cluster_conf_id | 7 || wsrep_cluster_size | 1 || wsrep_cluster_state_uuid | 47b3fdaa-5f43-11e8-8cbd-62ce6e10f564 || wsrep_cluster_status | Primary || wsrep_connected | ON || wsrep_local_bf_aborts | 0 || wsrep_local_index | 0 || wsrep_provider_name | Galera || wsrep_provider_vendor | Codership Oy &lt;info@codership.com&gt; || wsrep_provider_version | 25.3.23(r3789) || wsrep_ready | ON || wsrep_thread_count | 2 |+------------------------------+-----------------------------------------------+58 rows in set (0.00 sec) 重点查看这些信息是否正确：wsrep_cluster_size 当前集群中节点个数wsrep_local_state_comment 集群数据同步状态wsrep_incoming_addresses 数据提供方节点地址和端口wsrep_ready ON集群状态正常wsrep_connected ON集群连接状态正常 Step 8– 动态添加集群节点按照Step6的操作，配置node1和node2两个节点的配置文件，用以下命令启动集群子节点。注意与Donor节点的启动方式不同。 1sudo service mysql start 检查node1和node2两个节点的状态，是否正常。 1mysql -u root -p -e &quot;show status like &apos;wsrep%&apos;&quot; node1和node2两个子节点启动后，状态查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263+------------------------------+----------------------------------------------------------+| Variable_name | Value |+------------------------------+----------------------------------------------------------+| wsrep_local_state_uuid | 47b3fdaa-5f43-11e8-8cbd-62ce6e10f564 || wsrep_protocol_version | 8 || wsrep_last_committed | 2 || wsrep_replicated | 0 || wsrep_replicated_bytes | 0 || wsrep_repl_keys | 0 || wsrep_repl_keys_bytes | 0 || wsrep_repl_data_bytes | 0 || wsrep_repl_other_bytes | 0 || wsrep_received | 3 || wsrep_received_bytes | 476 || wsrep_local_commits | 0 || wsrep_local_cert_failures | 0 || wsrep_local_replays | 0 || wsrep_local_send_queue | 0 || wsrep_local_send_queue_max | 1 || wsrep_local_send_queue_min | 0 || wsrep_local_send_queue_avg | 0.000000 || wsrep_local_recv_queue | 0 || wsrep_local_recv_queue_max | 1 || wsrep_local_recv_queue_min | 0 || wsrep_local_recv_queue_avg | 0.000000 || wsrep_local_cached_downto | 18446744073709551615 || wsrep_flow_control_paused_ns | 0 || wsrep_flow_control_paused | 0.000000 || wsrep_flow_control_sent | 0 || wsrep_flow_control_recv | 0 || wsrep_cert_deps_distance | 0.000000 || wsrep_apply_oooe | 0.000000 || wsrep_apply_oool | 0.000000 || wsrep_apply_window | 0.000000 || wsrep_commit_oooe | 0.000000 || wsrep_commit_oool | 0.000000 || wsrep_commit_window | 0.000000 || wsrep_local_state | 4 || wsrep_local_state_comment | Synced || wsrep_cert_index_size | 0 || wsrep_causal_reads | 0 || wsrep_cert_interval | 0.000000 || wsrep_incoming_addresses | 192.168.1.151:3306,192.168.1.152:3306,192.168.1.150:3306 || wsrep_desync_count | 0 || wsrep_evs_delayed | || wsrep_evs_evict_list | || wsrep_evs_repl_latency | 0.00115351/0.00145445/0.0018473/0.000297523/4 || wsrep_evs_state | OPERATIONAL || wsrep_gcomm_uuid | 273a3916-5f4f-11e8-88db-8ab9682bb4ca || wsrep_cluster_conf_id | 9 || wsrep_cluster_size | 3 || wsrep_cluster_state_uuid | 47b3fdaa-5f43-11e8-8cbd-62ce6e10f564 || wsrep_cluster_status | Primary || wsrep_connected | ON || wsrep_local_bf_aborts | 0 || wsrep_local_index | 0 || wsrep_provider_name | Galera || wsrep_provider_vendor | Codership Oy &lt;info@codership.com&gt; || wsrep_provider_version | 25.3.23(r3789) || wsrep_ready | ON || wsrep_thread_count | 2 |+------------------------------+----------------------------------------------------------+58 rows in set (0.00 sec) Step 9 – 验证数据同步复制模拟场景1：集群中三个节点全部正常启动。在node1和node2两个节点分别创建库和表，查看donor节点是否正常的同步更新数据。 模拟场景2：node1和node2中的一个节点宕机。在没有宕机的节点（除donor节点）中进行数据CUD操作，先查看donor节点是否正常的同步更新数据，如果正常，则重新启动宕机节点，启动成功后，查看该节点能否正常的把不一致的数据同步过来。 PS:Galera在加入数据不一致的节点时，外部不能访问该节点，直到数据同步后才能访问该节点。 Keepalived实现MariaDB集群的高可用配请参考这篇文章《Keepalived HA +LVS+ Galera Cluster环境》]]></content>
      <tags>
        <tag>MariaDB</tag>
        <tag>Keepalived</tag>
        <tag>Galera</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Galera Cluster 3 + Mysql wresp 5.6安装配置]]></title>
    <url>%2F2018%2F05%2F24%2FGalera-Cluster-3-Mysql-wresp-5-6%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装yum-builddep工具yum install yum-utils –enablerepo=extras]]></content>
      <tags>
        <tag>Galera</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keepalived HA +LVS+ Galera Cluster环境]]></title>
    <url>%2F2018%2F05%2F23%2FKeepalived-HA-LVS-Galera-Cluster%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[Galera Cluster实现Mysql集群，再配合Keepalived的VRRP和LVS实现Mysql集群的高可用和负载均衡。 开发环境CentOS 6.8 + Mysql Galera 5.5 + Keepalived 1.4.4 1.安装Keepalived 123yum -y install kernel-develyum -y install ipvsadm 命令号执行ipvsadm是否安装成功，如果可以执行，则说明已经安装。 123wget http://www.keepalived.org/software/keepalived-1.4.4.tar.gztar -zvxf keepalived-1.4.4.tar.gz 找到Linux的内核源码路径，不同版本的操作系统版本号不同。 /usr/src/kernels/2.6.32-696.30.1.el6.x86_64/ 安装到/usr/local/keepalived目录，安装LVS功能需要指定内容源码路径。 1./configure --prefix=/usr/local/keepalived --with-kernel-dir=/usr/src/kernels/2.6.32-696.30.1.el6.x86_64 执行结果 12345678910111213141516171819202122232425262728293031323334353637Keepalived configuration------------------------Keepalived version : 1.4.4Compiler : gccPreprocessor flags : -I/usr/src/kernels/2.6.32-696.30.1.el6.x86_64/includeCompiler flags : -Wall -Wunused -Wstrict-prototypes -Wextra -g -O2 -D_GNU_SOURCE -fPIELinker flags : -pieExtra Lib : -lcrypto -lssl Use IPVS Framework : YesIPVS use libnl : NoIPVS syncd attributes : NoIPVS 64 bit stats : Nofwmark socket support : YesUse VRRP Framework : YesUse VRRP VMAC : YesUse VRRP authentication : YesWith ip rules/routes : YesSNMP vrrp support : NoSNMP checker support : NoSNMP RFCv2 support : NoSNMP RFCv3 support : NoDBUS support : NoSHA1 support : NoUse Debug flags : Nosmtp-alert debugging : NoUse Json output : NoStacktrace support : NoMemory alloc check : Nolibnl version : NoneUse IPv4 devconf : NoUse libiptc : NoUse libipset : Noinit type : upstartBuild genhash : YesBuild documentation : No*** WARNING - this build will not support IPVS with IPv6. Please install libnl/libnl-3 dev libraries to support IPv6 with IPVS. 编译安装 1make &amp; make install 拷贝keepalived配置文件到/etc/keepalived/keepalived.conf 12345mkdir /etc/keepalivedcp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/cd /usr/local/keepalived/sbin/./keepalived 查看keepalived运行日志 1tail -f /var/log/messages 停止keepalived 1pkill keepalived 修改配置文件 1vi /etc/keepalived/keepalived.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859! Configuration File for keepalived## 重点配置router_id LVS_150global_defs &#123; notification_email &#123;# admin@example.com &#125; # notification_email_from Alexandre.Cassen@firewall.loc # smtp_server 192.168.200.1 #smtp_connect_timeout 30 #router_id LVS_DEVEL #vrrp_skip_check_adv_addr #vrrp_strict #vrrp_garp_interval 0 #vrrp_gna_interval 0 router_id LVS_150&#125;##重点配置 虚拟路由节点virtual_router_id 51，主从配置：state MASTER/SLAVE，虚拟ip可以多个用多行表示：virtual_ipaddressvrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.200.16 &#125;&#125;##重点配置lb_algo wrr ，lb_kind DR，路由协议protocol TCP，真实IP和端口号real_server 192.168.1.150 3306，connect_port 3306virtual_server 192.168.200.16 3306 &#123; delay_loop 6 lb_algo wrr lb_kind DR persistence_timeout 50 protocol TCP real_server 192.168.1.150 3306 &#123; weight 5 TCP_CHECK &#123; connect_timeout 10 nb_get_retry 3 delay_before_retry 3 connect_port 3306 &#125; &#125; real_server 192.168.1.152 3306 &#123; weight 5 TCP_CHECK &#123; connect_timeout 10 nb_get_retry 3 delay_before_retry 3 connect_port 3306 &#125; &#125;&#125; 启动keepalived 1./keepalived 在虚拟IP对应的其他节点，按照上述同样的操作安装keepalived，环境准备完成后，可以通过虚拟IP访问mysql数据库。 1mysql -h 192.168.200.16 -ugalera -pgalera 后续问题：在Mysql Galera Cluster单点故障的恢复和重启。]]></content>
      <tags>
        <tag>Keepalived</tag>
        <tag>Galera</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 6.8+MySQL 5.5 with Galera Cluster环境搭建]]></title>
    <url>%2F2018%2F05%2F23%2FCentOS-6-8-MySQL-5-5-with-Galera-Cluster%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718##先安装依赖库yum install libaio gcc gcc-c++ boost-devel scons check-devel openssl-develln -sf /usr/lib64/libssl.so.10 /usr/lib64/libssl.so.6ln -sf /usr/lib64/libcrypto.so.10 /usr/lib64/libcrypto.so.6##下载MySQL with wsrep源码包，目前最新版本是5.5.34-25.9wget https://launchpad.net/codership-mysql/5.5/5.5.34-25.9/+download/mysql-5.5.34_wsrep_25.9-linux-x86_64.tar.gztar zxvf mysql-5.5*mv mysql-5.5.34_wsrep_25.9-linux-x86_64 /usr/local/mysqlcd /usr/local/mysql/groupadd mysqluseradd -r -g mysql mysqlchown -R mysql:mysql ../scripts/mysql_install_db --no-defaults --datadir=/opt/mysqldb/ --user=mysqlchown -R root .chown -R mysql /opt/mysqldb/echo &quot;export PATH=$PATH:/usr/local/mysql/bin&quot; &gt;&gt; /etc/profilesource /etc/profile[root@localhost mysql]# vi /etc/my.cnf [mysqld]datadir=/opt/mysqldbsocket = /tmp/mysql.sockuser=mysql symbolic-links=0 12345678910111213141516171819202122##启动Mysqlmysqld_safe --wsrep_cluster_address=gcomm:// &gt;/dev/null &amp;##安装Xtrabackupyum install perl-DBD-MySQL perl-Time-HiRes ncwget https://www.percona.com/downloads/XtraBackup/XtraBackup-2.1.9/RPM/rhel6/x86_64/percona-xtrabackup-2.1.9-744.rhel6.x86_64.rpmrpm -ivh percona-xtrabackup-2.1.9-744.rhel6.x86_64.rpm##安装Galera复制插件wget https://launchpad.net/galera/3.x/25.3.5/+download/galera-25.3.5-src.tar.gztar zxvf galera-25.3.5-src.tar.gzcd galera-25.3.5-srcsconscp garb/garbd /usr/local/mysql/bin/cp libgalera_smm.so /usr/local/mysql/lib/plugin/ mysql初始化配置 12345678910111213141516171819mkdir -p /var/lib/mysqlmkdir -p /etc/mysql/conf.d/chown mysql:mysql /var/lib/mysqlcp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql/usr/local/mysql/bin/mysqld_safe --wsrep_cluster_address=gcomm:// &gt;/dev/null &amp;##创建用于同步的帐号，注意替换掉示例值。mysql -e &quot;SET wsrep_on=OFF; GRANT ALL ON *.* TO &apos;galera&apos;@&apos;%&apos; IDENTIFIED BY &apos;galera&apos;&quot;;##我使用的mysql版本再执行上述授权语句后，%代表任意主机，却不包含localhost，所以要给防伪localhost单独授权。mysql -e &quot;SET wsrep_on=OFF; GRANT ALL ON *.* TO &apos;galera&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;galera&apos;&quot;;##修改root帐号密码，注意替换掉示例值。mysql -e &quot;SET wsrep_on=OFF;GRANT ALL PRIVILEGES ON * . * TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION MAX_QUERIES_PER_HOUR 0 MAX_CONNECTIONS_PER_HOUR 0 MAX_UPDATES_PER_HOUR 0 MAX_USER_CONNECTIONS 0 ; &quot;##查看授权mysql -uroot -proot;show grants for galera@&quot;%&quot;;show grants for galera@&quot;localhost&quot;;##关闭MySQLpkill mysql Galera集群配置 修改my.cnf 1vi /etc/my.cnf my.cnf配置： 12345678910111213141516171819202122232425262728293031[mysqld]datadir=/opt/mysqldbsocket = /tmp/mysql.sockuser=mysqlsymbolic-links=0######galera conf start########server-id=101wsrep_node_name = mysql1wsrep_provider = /usr/local/mysql/lib/plugin/libgalera_smm.sowsrep_sst_method = rsync#使用sst的用户和密码，这里如果开启，需要在mysql上创建该用户，并授予其足够的权限wsrep_sst_auth=galera:galera# 配置集群的所有节点wsrep_cluster_address=gcomm://192.168.1.150:4567,192.168.1.151:4567# 配置自己的ip:port，每个配置各不相同wsrep_node_address=192.168.1.150:4567default_storage_engine=InnoDBinnodb_autoinc_lock_mode=2innodb_locks_unsafe_for_binlog=1innodb_flush_log_at_trx_commit=1innodb_file_per_table=1binlog_format=ROWlog-bin=mysql-binrelay-log=mysql-relay-binlog-slave-updates=1#[mysqld_safe]#log-error=/var/lib/mysql/mysqld.log#pid-file=/var/lib/mysql/mysqld.pid 节点mysql1启动 1/usr/local/mysql/bin/mysqld_safe --wsrep_cluster_address=gcomm:// &gt;/dev/null &amp; 检查启动的端口，应该有3306和4567两个端口 12netstat -tunlp |grep 4567netstat -tunlp |grep 3306 在集群中添加新的节点，并执行启动脚本。 1/etc/init.d/mysql start PS：复制虚拟机，设置新IP 在新的虚拟机中修改ifconfig-eth0，设置IP为192.168.1.151，去掉UUID、MAC两个参数。 1vi /etc/sysconfig/network-scripts/ifcfg-eth0 重建路由 1rm -rf /etc/udev/rules.d/70-persistent-net.rules 重启虚拟机 1shutown -r now 测试网络 1ping 192.168.1.150]]></content>
      <tags>
        <tag>Galera</tag>
        <tag>CentOS</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Galera Cluster环境快速搭建]]></title>
    <url>%2F2018%2F05%2F22%2FDocker-Galera-Cluster%E7%8E%AF%E5%A2%83%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[使用Docker快速创建MariaDB（与MySql同源） Galera集群环境 Docker开发环境：MacOS High Sieera 10.13.2 (17C88)MariaDB Galera Cluster容器环境：CentOS 7.3 + MariaDB 10.1.23 1.执行Docker拉取镜像 docker pull mjstealey/mariadb-galera:10.1 2.下载测试脚本 123git clone https://github.com/mjstealey/mariadb-galera.gitcd mariadb-galera/ 下载好的代码中包括创建docker容器的脚本和测试脚本。 3.创建集群节点并执行测试语句 ./three-node-test.sh 命令执行过程中会创建三个节点，并且执行测试sql查看集群节点数据同步是否正确。测试通过后，可以使用后续命令分别操作三个节点的数据库。 4.查看已经运行的节点进程 docker container ls 输出结果 1234CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0a06b6789c1c mjstealey/mariadb-galera:10.1 &quot;/docker-entrypoint.…&quot; 58 seconds ago Up 59 seconds 0.0.0.0:32791-&gt;3306/tcp, 0.0.0.0:32790-&gt;4444/tcp, 0.0.0.0:32789-&gt;4567/tcp, 0.0.0.0:32788-&gt;4568/tcp galera-node-3c4e5eb6d2ba1 mjstealey/mariadb-galera:10.1 &quot;/docker-entrypoint.…&quot; About a minute ago Up About a minute 0.0.0.0:32787-&gt;3306/tcp, 0.0.0.0:32786-&gt;4444/tcp, 0.0.0.0:32785-&gt;4567/tcp, 0.0.0.0:32784-&gt;4568/tcp galera-node-2bf53b99d540c mjstealey/mariadb-galera:10.1 &quot;/docker-entrypoint.…&quot; About a minute ago Up About a minute 0.0.0.0:32783-&gt;3306/tcp, 0.0.0.0:32782-&gt;4444/tcp, 0.0.0.0:32781-&gt;4567/tcp, 0.0.0.0:32780-&gt;4568/tcp galera-node-1 5.单独进入节点操作数据库docker exec -it bf53b99d540c mysql -uroot -ptemppassword MariaDB数据库的root用户密码是Docker在创建容器的时候配置的，可以从Dockerfile中找到配置，直接从镜像中拉取的容器镜像，是已经配置好的。如果本地执行Docker容器创建，那么可以修改Dockerfile中的相关配置。 命令中bf53b99d540c是指容器进程的CONTAINER ID的值。 参数解释-i 可交互-t 分配终端 命令执行完会进入MariaDB的命令行交互窗口。 6.在任一一个节点操作数据库，数据都会在其他节点同步（接近同步）更新。==Galera规则要求创建的表必须要有主键，没有主键也要指定自增列。== 12345create table tbl_user( pkid int auto_increment primary key, username varchar(255) not null);]]></content>
      <tags>
        <tag>MariaDB</tag>
        <tag>Galera</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql分区操作（三）]]></title>
    <url>%2F2018%2F05%2F17%2FMySql%E5%88%86%E5%8C%BA%E6%93%8D%E4%BD%9C%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[子分区操作常用RANGE和HASH复合分区实现子分区 1234567891011121314151617CREATE TABLE myblog.tbl_users_2 ( `pkid` INT NOT NULL , `username` VARCHAR(255) NOT NULL, `password` VARCHAR(255) NOT NULL, `email` VARCHAR(255) NULL, `sex` CHAR(1) NULL, `create_time` DATE NULL)ENGINE = InnoDBDEFAULT CHARACTER SET = utf8 PARTITION BY RANGE(YEAR(create_time))SUBPARTITION BY HASH(TO_DAYS(create_time))SUBPARTITIONS 2(PARTITION p0 VALUES LESS THAN (2010),PARTITION p1 VALUES LESS THAN (2017),PARTITION p2 VALUES LESS THAN (MAXVALUE)); 123INSERT INTO TBL_USERS_2 VALUES (1,&apos;1&apos;,&apos;1&apos;,&apos;1&apos;,1,&apos;2009-08-08&apos;);INSERT INTO TBL_USERS_2 VALUES (2,&apos;1&apos;,&apos;1&apos;,&apos;1&apos;,1,&apos;2016-08-07&apos;);INSERT INTO TBL_USERS_2 VALUES (3,&apos;1&apos;,&apos;1&apos;,&apos;1&apos;,1,&apos;2018-08-08&apos;); 查看产生的分区 123456789101112131415161718EXPLAIN PARTITIONS SELECT * FROM tbl_users_2 \G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tbl_users_2 partitions: p0_p0sp0,p0_p0sp1,p1_p1sp0,p1_p1sp1,p2_p2sp0,p2_p2sp1 type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 3 filtered: 100.00 Extra: NULL1 row in set, 2 warnings (0.00 sec)ERROR: No query specified 查看其中一条数据所在分区 123456789101112131415EXPLAIN PARTITIONS SELECT * FROM tbl_users_2 where create_time = &apos;2017-08-07&apos;\G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tbl_users_2 partitions: p2_p2sp1 type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 1 filtered: 100.00 Extra: Using where1 row in set, 2 warnings (0.00 sec) 分区字段特殊性 按字段分区的字段不能为NULL，所以在建表时需要指定分区字段为NOT NULL。 分区管理 删除分区和分区上的数据 1alter table tbl_user drop partiton p0; 增加分区对于RANGE分区，只能添加比已经存在的分区范围更大的值。1alter table tbl_user add partition ( partition p0 values less than (100)); 对于LIST分区，只能添加不存在与分区列表中的值。 1alter table tbl_user add partition ( partition p0 values in (100,101,102)); 也就是说，100，101，102不在已定义的LIST分区内。 不丢失数据修改分区 如果原来的分区是这样的： 123456789create table tbl_user( pkid int not null, name varchar(255) ) partition by range(pkid) partitions 3( partition p0 values less than(100), partition p1 values less than(200), partition p2 values less than(300) ); ==修改(拆分)分区：== 1234alter table tbl_user reorganize partition p0 into ( partition s0 values less than(50), partition s1 values less than(100) ); 相当于原来的分区是这样创建的： 12345678910create table tbl_user( pkid int not null, name varchar(255) ) partition by range(pkid) partitions 4( partition s0 values less than(50), partition s1 values less than(100), partition p1 values less than(200), partition p2 values less than(300) ); LIST分区修改（拆分）同理。==修改(合并)分区：== 123alter table tbl_user reorganize partition s0,s1 into ( partition p0 values less than(100) ); 删除分区，不删除数据 1alter table tbl_user remove partitioning HASH和KEY分区管理 在没有数据的时候进行修改分区操作是可以的，如果在数据存在后再减少和增加分区，数据分布就不均匀了。 减少n个分区 1alter table tbl_user coalesce partition 2; 增加n个分区 1alter table tbl_user add partition 2; 其他分区管理语句 重建分区 1alter table tbl_user rebuild partition p0,p1,p2,p3; 优化分区(包括分析、检查、修补分区） 1alter table tbl_user optimize partition p0,p1,p2,p3; 分析分区 1alter table tbl_user analyze partiton p0,p1,p2,p3; 检查分区 1alter table tbl_user check partition p0,p1,p2,p3; 修补分区 1alter table tbl_user repair partition p0,p1,p2,p3; 其他细节 分区数最大不超过1024个，实际应用中分区数不超过150个；如果有唯一索引或者主键，分区列必须包含所有的唯一索引或者主键；不支持外键；不支持全区索引；常用日期进行分区；临时表不能被分区；单条数据查询分区管理意义不大，除非指定数据所在分区；计算分区成本，因为每次插入数据都会进行分区计算，分区函数不能过于复杂；分区字段不能为NULL；]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql分区操作（二）]]></title>
    <url>%2F2018%2F05%2F17%2FMySql%E5%88%86%E5%8C%BA%E6%93%8D%E4%BD%9C%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[【转自】https://blog.csdn.net/tjcyjd/article/details/11194489HASH分区和线性LINEAR HASH分区 123456789CREATE TABLE `myblog`.`tbl_user_friends_linear_hash` ( `pkid` INT NOT NULL AUTO_INCREMENT, `user_id` INT NOT NULL, `friend_id` INT NOT NULL, `create_time` DATETIME NOT NULL, PRIMARY KEY (`pkid`))ENGINE = InnoDBDEFAULT CHARACTER SET = utf8COMMENT = &apos;用户好友&apos; PARTITION BY LINEAR HASH(pkid) PARTITIONS 3 ; MySQL还支持线性哈希功能，它与常规哈希的区别在于，线性哈希功能使用的一个线性的2的幂（powers-of-two）运算法则，而常规 哈希使用的是求哈希函数值的模数。 线性哈希分区和常规哈希分区在语法上的唯一区别在于，在“PARTITION BY” 子句中添加“LINEAR”关键字，如下面所示： 1234567891011CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT &apos;1970-01-01&apos;, separated DATE NOT NULL DEFAULT &apos;9999-12-31&apos;, job_code INT, store_id INT)PARTITION BY LINEAR HASH(YEAR(hired))PARTITIONS 4； 假设一个表达式expr, 当使用线性哈希功能时，记录将要保存到的分区是num 个分区中的分区N，其中N是根据下面的算法得到： 找到下一个大于num.的、2的幂，我们把这个值称为V ，它可以通过下面的公式得到： V = POWER(2, CEILING(LOG(2, num)))（例如，假定num是13。那么LOG(2,13)就是3.7004397181411。 CEILING(3.7004397181411)就是4，则V = POWER(2,4), 即等于16）。 设置 N = F(column_list) &amp; (V - 1). 当 N &gt;= num: · 设置 V = CEIL(V / 2) · 设置 N = N &amp; (V - 1) 例如，假设表t1，使用线性哈希分区且有4个分区，是通过下面的语句创建的： 123CREATE TABLE t1 (col1 INT, col2 CHAR(5), col3 DATE) PARTITION BY LINEAR HASH( YEAR(col3) ) PARTITIONS 6; 现在假设要插入两行记录到表t1中，其中一条记录col3列值为’2003-04-14’，另一条记录col3列值为’1998-10-19’。第一条记录将要保存到的分区确定如下： V = POWER(2, CEILING(LOG(2,7))) = 8N = YEAR(‘2003-04-14’) &amp; (8 - 1) = 2003 &amp; 7 = 3 (3 &gt;= 6 为假（FALSE）: 记录将被保存到#3号分区中)第二条记录将要保存到的分区序号计算如下： V = 8N = YEAR(‘1998-10-19’) &amp; (8-1) = 1998 &amp; 7 = 6 (6 &gt;= 4 为真（TRUE）: 还需要附加的步骤) N = 6 &amp; CEILING(5 / 2) = 6 &amp; 3 = 2 (2 &gt;= 4 为假（FALSE）: 记录将被保存到#2分区中)按照线性哈希分区的优点在于增加、删除、合并和拆分分区将变得更加快捷，有利于处理含有极其大量（1000吉）数据的表。它的缺点在于，与使用常规HASH分区得到的数据分布相比，各个分区间数据的分布不大可能均衡。 KEY分区和LINEAR KEY分区 123456789CREATE TABLE `myblog`.`tbl_user_friends_linear_key` ( `pkid` INT NOT NULL AUTO_INCREMENT, `user_id` INT NOT NULL, `friend_id` INT NOT NULL, `create_time` DATETIME NOT NULL, primary key(pkid))ENGINE = InnoDBDEFAULT CHARACTER SET = utf8COMMENT = &apos;用户好友&apos; PARTITION BY LINEAR KEY(pkid) PARTITIONS 3 ; 按照KEY进行分区类似于按照HASH分区，除了HASH分区使用的用户定义的表达式，而KEY分区的 哈希函数是由MySQL 服务器提供。MySQL 簇（Cluster）使用函数MD5()来实现KEY分区；对于使用其他存储引擎的表，服务器使用其自己内部的 哈希函数，这些函数是基于与PASSWORD()一样的运算法则。 “CREATE TABLE … PARTITION BY KEY”的语法规则类似于创建一个通过HASH分区的表的规则。它们唯一的区别在于使用的关键字是KEY而不是HASH，并且KEY分区只采用一个或多个列名的一个列表。 通过线性KEY分割一个表也是可能的。下面是一个简单的例子： 1234567CREATE TABLE tk ( col1 INT NOT NULL, col2 CHAR(5), col3 DATE) PARTITION BY LINEAR KEY (col1)PARTITIONS 3; 在KEY分区中使用关键字LINEAR和在HASH分区中使用具有同样的作用，分区的编号是通过2的幂（powers-of-two）算法得到，而不是通过模数算法。]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为未来而学]]></title>
    <url>%2F2018%2F05%2F16%2F%E4%B8%BA%E6%9C%AA%E6%9D%A5%E8%80%8C%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[123如果一个人只满足于完成别人所要求的事情，那么，他只能是个奴隶，只有当他超越了这个限度，才会成为一个自由人。——哈佛学习格言 有关学习的两个困境 一、学习是一场竞赛，我们凭什么才可以胜出？ 大城市里面人们为了能够上一所“好的”小学，要选择购置学区房； 大城市里面为了能够将来上好的中学，还要报各种课外班，什么奥数，英语，绘画，书法，钢琴，足球，篮球······； 大城市里面孩子们每天不是在上学就是在上课外班，家长不是在送孩子去上学的路上就是在送孩子去课外班的路上。 这一切正常吗，我们是选择了随大流还是经过了深思熟虑的思考？这样的学习方式和节奏是最有效的吗？这场学习的竞赛普通的家庭有竞争优势吗？哪些能力在我们未来会让我们更有竞争优势呢？哪些方面的学习对我们一生更加重要呢？ 不知道这些问题，朋友们有没有问过自己，有没有过深入的思考，有没有给自己一个答案。 与此相反的是，小城市、农村、郊区的孩子们多数情况是玩玩玩，大好的青春真的就在玩耍中渡过啦，想必长大后会后悔吧。毕竟，人生中有许多事情是需要时间的积累的，而时间是不可以购买的，时间是一去不复返的，错过真的很难弥补上。阅读，就属于这样的事情。 现在，城市与郊区的教育，重点学校与普通学校之间教育的差距越来越大，这种差距一方面体现在对教育的认知上，另一方面体现在优质的教育资源和用钱堆出来的课外兴趣班、优培班。 好在，我们还有一种方法，能够缩小这种无法改变的学习资源和环境差距。是什么样的方法呢？ 俗话说，要用正确的方法做正确的事情。这一点最关键。我们要从学习的最根本出发，去探寻学习的本质，去掌握学习的方法，去洞察未来，这就是我们需要做的最正确的事情，它保证我们走在一条正确的道路上。而做这些事情，好的学校和富裕的家庭条件都不是必须的条件，需要的是我们的态度和行动。 二、学习是为了生活，生活是一辈子的事情，我们是否有终身学习的准备？ 先不说，终身学习，就是学校里的学习，我们都无法很好地完成。这是为什么呢？ 说不爱学习的人，实际是没有发现好的学习方式、引人入胜的学习资源、让人体会到收获和成就感的学习成果。甚至，从更根本上来说，是没有找到或发现学习的意义。 其实，人天生就是学习的好手，不仅具有好奇心，而且也爱学习。 学习中往往会： 因为不知道自己不知道，而没有思考； 因为不知道学习目标是什么，不知道为什么而学，而没有动力； 因为不知道学习方法，而无法采取行动； 因为不知道学习需要经历的过程，而无法坚持。 在这里，你将看清楚“学习”的样子，看清楚“学习的过程”，掌握“学习”的方法，知道并体会到“学习”带来的各种机会和可能性。 从此，你会亲手为自己打开一扇通往未来的希望之门，你会走在通往未来的道路上，而且还站在巨人的肩膀上。慢慢地，你会发现你已经不是原来的你了，你能看的更远，也能看的更加清晰。终身学习也因此成为了你生活中形影不离的朋友！]]></content>
      <tags>
        <tag>学习力</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql分区操作（一）]]></title>
    <url>%2F2018%2F05%2F16%2FMySql%E5%88%86%E5%8C%BA%E6%93%8D%E4%BD%9C%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[##分区类型RANGE 连续的列值区间分区，有主键和唯一键必须使用，如果没有则可以指定任何一列；LIST 类似RANGE分区，区别在于指定一系列的列值作为分区条件；HASH 由函数表达式返回值决定所在分区，函数返回值必须为非负整数；KEY 由mysql提供的HASH函数进行服务，==使用较少==。 ##创建表分区 1234567891011121314CREATE TABLE tbl_users ( `pkid` INT NOT NULL AUTO_INCREMENT, `username` VARCHAR(255) NOT NULL, `password` VARCHAR(255) NOT NULL, `email` VARCHAR(255) NULL, `sex` CHAR(1) NULL, PRIMARY KEY (`pkid`))ENGINE = InnoDBDEFAULT CHARACTER SET = utf8 PARTITION BY RANGE(pkid) PARTITIONS 3( PARTITION part0 VALUES LESS THAN (10000), PARTITION part1 VALUES LESS THAN (20000), PARTITION part2 VALUES LESS THAN (MAXVALUE)) ; ##查看分区 1select * from information_schema.partitions where table_schema=&apos;myblog&apos; and table_name=&apos;tbl_users&apos; \G; 得到如下结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879*************************** 1. row *************************** TABLE_CATALOG: def TABLE_SCHEMA: myblog TABLE_NAME: tbl_users PARTITION_NAME: part0 SUBPARTITION_NAME: NULL PARTITION_ORDINAL_POSITION: 1SUBPARTITION_ORDINAL_POSITION: NULL PARTITION_METHOD: RANGE SUBPARTITION_METHOD: NULL PARTITION_EXPRESSION: pkid SUBPARTITION_EXPRESSION: NULL PARTITION_DESCRIPTION: 10000 TABLE_ROWS: 0 AVG_ROW_LENGTH: 0 DATA_LENGTH: 16384 MAX_DATA_LENGTH: NULL INDEX_LENGTH: 0 DATA_FREE: 0 CREATE_TIME: 2018-05-16 18:21:08 UPDATE_TIME: NULL CHECK_TIME: NULL CHECKSUM: NULL PARTITION_COMMENT: NODEGROUP: default TABLESPACE_NAME: NULL*************************** 2. row *************************** TABLE_CATALOG: def TABLE_SCHEMA: myblog TABLE_NAME: tbl_users PARTITION_NAME: part1 SUBPARTITION_NAME: NULL PARTITION_ORDINAL_POSITION: 2SUBPARTITION_ORDINAL_POSITION: NULL PARTITION_METHOD: RANGE SUBPARTITION_METHOD: NULL PARTITION_EXPRESSION: pkid SUBPARTITION_EXPRESSION: NULL PARTITION_DESCRIPTION: 20000 TABLE_ROWS: 0 AVG_ROW_LENGTH: 0 DATA_LENGTH: 16384 MAX_DATA_LENGTH: NULL INDEX_LENGTH: 0 DATA_FREE: 0 CREATE_TIME: 2018-05-16 18:21:08 UPDATE_TIME: NULL CHECK_TIME: NULL CHECKSUM: NULL PARTITION_COMMENT: NODEGROUP: default TABLESPACE_NAME: NULL*************************** 3. row *************************** TABLE_CATALOG: def TABLE_SCHEMA: myblog TABLE_NAME: tbl_users PARTITION_NAME: part2 SUBPARTITION_NAME: NULL PARTITION_ORDINAL_POSITION: 3SUBPARTITION_ORDINAL_POSITION: NULL PARTITION_METHOD: RANGE SUBPARTITION_METHOD: NULL PARTITION_EXPRESSION: pkid SUBPARTITION_EXPRESSION: NULL PARTITION_DESCRIPTION: MAXVALUE TABLE_ROWS: 0 AVG_ROW_LENGTH: 0 DATA_LENGTH: 16384 MAX_DATA_LENGTH: NULL INDEX_LENGTH: 0 DATA_FREE: 0 CREATE_TIME: 2018-05-16 18:21:08 UPDATE_TIME: NULL CHECK_TIME: NULL CHECKSUM: NULL PARTITION_COMMENT: NODEGROUP: default TABLESPACE_NAME: NULL3 rows in set (0.01 sec) ##查看分区上的数据 1select * from tbl_users partition(p0) ##查看分区数据查询性能 1explain partitions select * from tbl_users where pkid =2; ##其他分区类型LIST类型 12345678910111213CREATE TABLE `myblog`.`tbl_blog` ( `pkid` INT NOT NULL AUTO_INCREMENT, `title` VARCHAR(255) NOT NULL, `content` VARCHAR(5000) NULL, `create_time` DATETIME NULL, PRIMARY KEY (`pkid`))ENGINE = InnoDBDEFAULT CHARACTER SET = utf8COMMENT = &apos;博客文章&apos; PARTITION BY LIST(pkid) PARTITIONS 3( PARTITION part0 VALUES IN (1,2,3), PARTITION part1 VALUES IN (4,5,6), PARTITION part2 VALUES IN (7,8,9)) ; HASH类型利用分区字段除分区数量取余数，就把数据放到第余数个分区上。 123456789CREATE TABLE `myblog`.`tbl_user_friends` ( `pkid` INT NOT NULL AUTO_INCREMENT, `user_id` INT NOT NULL, `friend_id` INT NOT NULL, `create_time` DATETIME NOT NULL, PRIMARY KEY (`pki`))ENGINE = InnoDBDEFAULT CHARACTER SET = utf8COMMENT = &apos;用户好友&apos; PARTITION BY HASH(pkid) PARTITIONS 3 ; 插入数据 12insert into tbl_user_friends values(1,2,3,NULL);insert into tbl_user_friends values(5,2,3,NULL); 得到结论 1234567891011121314select * from tbl_user_friends partition(p1);+------+---------+-----------+-------------+| pkid | user_id | friend_id | create_time |+------+---------+-----------+-------------+| 1 | 2 | 3 | NULL |+------+---------+-----------+-------------+1 row in set (0.00 sec)mysql&gt; select * from tbl_user_friends partition(p2);+------+---------+-----------+-------------+| pkid | user_id | friend_id | create_time |+------+---------+-----------+-------------+| 5 | 2 | 3 | NULL |+------+---------+-----------+-------------+1 row in set (0.00 sec)]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis单机QPS压力测试]]></title>
    <url>%2F2018%2F01%2F18%2FRedis%E5%8D%95%E6%9C%BAQPS%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[#使用redis-benchmark命令进行QPS压力测试 12cd /usr/local/common/redis/src./redis-benchmark -h 192.168.1.113 -c 1000 -n 100000 -d 3 &gt;&gt; ~/redis_qps.log redis-benchmark 常用参数-h redis主机ip地址-c 客户端连接数-n request请求数量-d redis进行SET/GET操作的数据大小，单位byte 压力测试结果示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044104510461047104810491050105110521053105410551056105710581059106010611062106310641065106610671068106910701071107210731074107510761077107810791080108110821083108410851086108710881089109010911092109310941095109610971098109911001101110211031104110511061107110811091110111111121113111411151116111711181119112011211122112311241125112611271128112911301131113211331134113511361137113811391140114111421143114411451146114711481149115011511152115311541155115611571158115911601161116211631164116511661167116811691170117111721173117411751176117711781179118011811182118311841185118611871188118911901191119211931194119511961197119811991200120112021203120412051206120712081209121012111212121312141215121612171218121912201221122212231224122512261227122812291230123112321233123412351236123712381239124012411242124312441245124612471248124912501251125212531254125512561257125812591260126112621263126412651266126712681269127012711272127312741275127612771278127912801281128212831284128512861287128812891290129112921293129412951296129712981299130013011302130313041305130613071308130913101311131213131314131513161317131813191320132113221323132413251326132713281329133013311332133313341335133613371338133913401341134213431344134513461347134813491350135113521353135413551356135713581359136013611362136313641365136613671368136913701371137213731374137513761377137813791380138113821383138413851386138713881389139013911392139313941395139613971398139914001401140214031404140514061407140814091410141114121413141414151416141714181419142014211422142314241425142614271428142914301431143214331434143514361437143814391440144114421443144414451446144714481449145014511452145314541455145614571458145914601461146214631464146514661467146814691470147114721473147414751476147714781479148014811482148314841485148614871488148914901491149214931494149514961497149814991500150115021503150415051506150715081509151015111512151315141515151615171518151915201521152215231524152515261527152815291530153115321533153415351536153715381539154015411542154315441545154615471548154915501551155215531554155515561557155815591560156115621563156415651566156715681569157015711572157315741575157615771578157915801581158215831584158515861587158815891590159115921593159415951596159715981599160016011602160316041605160616071608160916101611161216131614161516161617161816191620162116221623162416251626162716281629163016311632163316341635163616371638163916401641164216431644164516461647164816491650165116521653165416551656165716581659166016611662166316641665166616671668166916701671167216731674167516761677167816791680168116821683168416851686168716881689169016911692169316941695169616971698169917001701170217031704170517061707170817091710171117121713171417151716171717181719172017211722172317241725172617271728172917301731173217331734173517361737173817391740174117421743174417451746174717481749175017511752175317541755175617571758175917601761176217631764176517661767176817691770177117721773177417751776177717781779178017811782178317841785178617871788178917901791179217931794179517961797179817991800180118021803180418051806180718081809181018111812181318141815181618171818181918201821182218231824182518261827182818291830183118321833183418351836183718381839184018411842184318441845PING_INLINE: 0.00PING_INLINE: 46245.35PING_INLINE: 46635.84PING_INLINE: 47530.48PING_INLINE: 48039.77PING_INLINE: 48494.55PING_INLINE: 47967.45PING_INLINE: 47430.65PING_INLINE: 46487.76====== PING_INLINE ====== 100000 requests completed in 2.17 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 3 milliseconds0.18% &lt;= 4 milliseconds0.92% &lt;= 5 milliseconds2.32% &lt;= 6 milliseconds4.32% &lt;= 7 milliseconds9.11% &lt;= 8 milliseconds16.99% &lt;= 9 milliseconds27.26% &lt;= 10 milliseconds39.69% &lt;= 11 milliseconds53.45% &lt;= 12 milliseconds65.56% &lt;= 13 milliseconds75.19% &lt;= 14 milliseconds82.30% &lt;= 15 milliseconds87.56% &lt;= 16 milliseconds92.08% &lt;= 17 milliseconds94.87% &lt;= 18 milliseconds96.59% &lt;= 19 milliseconds97.49% &lt;= 20 milliseconds98.54% &lt;= 21 milliseconds99.14% &lt;= 22 milliseconds99.62% &lt;= 23 milliseconds99.80% &lt;= 24 milliseconds99.97% &lt;= 25 milliseconds100.00% &lt;= 25 milliseconds46189.38 requests per secondPING_BULK: 0.00PING_BULK: 50348.31PING_BULK: 46936.17PING_BULK: 46209.09PING_BULK: 45281.28PING_BULK: 45749.22PING_BULK: 46259.40PING_BULK: 46627.02PING_BULK: 46880.55====== PING_BULK ====== 100000 requests completed in 2.14 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 2 milliseconds0.01% &lt;= 3 milliseconds0.48% &lt;= 4 milliseconds1.45% &lt;= 5 milliseconds3.42% &lt;= 6 milliseconds6.71% &lt;= 7 milliseconds12.13% &lt;= 8 milliseconds20.67% &lt;= 9 milliseconds29.84% &lt;= 10 milliseconds41.66% &lt;= 11 milliseconds55.40% &lt;= 12 milliseconds68.79% &lt;= 13 milliseconds80.71% &lt;= 14 milliseconds86.64% &lt;= 15 milliseconds90.72% &lt;= 16 milliseconds93.48% &lt;= 17 milliseconds95.29% &lt;= 18 milliseconds97.57% &lt;= 19 milliseconds98.43% &lt;= 20 milliseconds98.96% &lt;= 21 milliseconds99.08% &lt;= 22 milliseconds99.25% &lt;= 23 milliseconds99.47% &lt;= 24 milliseconds99.53% &lt;= 25 milliseconds99.62% &lt;= 26 milliseconds99.65% &lt;= 27 milliseconds99.80% &lt;= 28 milliseconds99.95% &lt;= 29 milliseconds100.00% &lt;= 29 milliseconds46816.48 requests per secondSET: 0.00SET: 42943.82SET: 41888.89SET: 40346.60SET: 38730.81SET: 38268.33SET: 38060.63SET: 38324.93SET: 38287.60SET: 38164.56SET: 38803.06====== SET ====== 100000 requests completed in 2.58 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 4 milliseconds0.13% &lt;= 5 milliseconds0.44% &lt;= 6 milliseconds1.18% &lt;= 7 milliseconds2.08% &lt;= 8 milliseconds3.17% &lt;= 9 milliseconds3.94% &lt;= 10 milliseconds5.60% &lt;= 11 milliseconds8.40% &lt;= 12 milliseconds12.39% &lt;= 13 milliseconds16.85% &lt;= 14 milliseconds21.76% &lt;= 15 milliseconds25.25% &lt;= 16 milliseconds31.24% &lt;= 17 milliseconds38.57% &lt;= 18 milliseconds47.07% &lt;= 19 milliseconds54.54% &lt;= 20 milliseconds60.23% &lt;= 21 milliseconds64.63% &lt;= 22 milliseconds70.25% &lt;= 23 milliseconds73.82% &lt;= 24 milliseconds77.21% &lt;= 25 milliseconds80.82% &lt;= 26 milliseconds84.45% &lt;= 27 milliseconds87.18% &lt;= 28 milliseconds89.37% &lt;= 29 milliseconds91.31% &lt;= 30 milliseconds93.97% &lt;= 31 milliseconds95.67% &lt;= 32 milliseconds97.05% &lt;= 33 milliseconds97.64% &lt;= 34 milliseconds98.13% &lt;= 35 milliseconds98.61% &lt;= 36 milliseconds98.92% &lt;= 37 milliseconds99.21% &lt;= 38 milliseconds99.34% &lt;= 39 milliseconds99.43% &lt;= 40 milliseconds99.57% &lt;= 41 milliseconds99.78% &lt;= 42 milliseconds99.92% &lt;= 43 milliseconds99.95% &lt;= 44 milliseconds99.97% &lt;= 46 milliseconds99.99% &lt;= 47 milliseconds100.00% &lt;= 47 milliseconds38744.67 requests per secondGET: 0.00GET: 44460.38GET: 48447.67GET: 47419.82GET: 46048.04GET: 46374.51GET: 45363.58GET: 44230.17GET: 43709.27GET: 43370.23====== GET ====== 100000 requests completed in 2.31 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 3 milliseconds0.17% &lt;= 4 milliseconds0.43% &lt;= 5 milliseconds1.08% &lt;= 6 milliseconds3.49% &lt;= 7 milliseconds7.04% &lt;= 8 milliseconds11.62% &lt;= 9 milliseconds18.45% &lt;= 10 milliseconds26.59% &lt;= 11 milliseconds37.03% &lt;= 12 milliseconds48.12% &lt;= 13 milliseconds60.62% &lt;= 14 milliseconds71.23% &lt;= 15 milliseconds79.02% &lt;= 16 milliseconds84.98% &lt;= 17 milliseconds88.97% &lt;= 18 milliseconds91.90% &lt;= 19 milliseconds94.02% &lt;= 20 milliseconds95.97% &lt;= 21 milliseconds96.87% &lt;= 22 milliseconds97.78% &lt;= 23 milliseconds98.51% &lt;= 24 milliseconds99.01% &lt;= 25 milliseconds99.26% &lt;= 26 milliseconds99.42% &lt;= 27 milliseconds99.44% &lt;= 28 milliseconds99.52% &lt;= 29 milliseconds99.67% &lt;= 30 milliseconds99.79% &lt;= 31 milliseconds99.86% &lt;= 32 milliseconds99.87% &lt;= 33 milliseconds100.00% &lt;= 33 milliseconds43271.31 requests per secondINCR: 0.00INCR: 43350.55INCR: 37198.91INCR: 36066.42INCR: 35544.85INCR: 37008.44INCR: 37827.19INCR: 38857.61INCR: 39449.47INCR: 40050.60====== INCR ====== 100000 requests completed in 2.53 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 3 milliseconds0.02% &lt;= 4 milliseconds0.14% &lt;= 5 milliseconds0.43% &lt;= 6 milliseconds1.48% &lt;= 7 milliseconds3.04% &lt;= 8 milliseconds4.97% &lt;= 9 milliseconds7.16% &lt;= 10 milliseconds9.89% &lt;= 11 milliseconds13.07% &lt;= 12 milliseconds17.86% &lt;= 13 milliseconds23.34% &lt;= 14 milliseconds28.59% &lt;= 15 milliseconds36.97% &lt;= 16 milliseconds45.31% &lt;= 17 milliseconds53.59% &lt;= 18 milliseconds61.76% &lt;= 19 milliseconds69.99% &lt;= 20 milliseconds75.75% &lt;= 21 milliseconds79.88% &lt;= 22 milliseconds83.41% &lt;= 23 milliseconds85.81% &lt;= 24 milliseconds87.66% &lt;= 25 milliseconds89.64% &lt;= 26 milliseconds91.63% &lt;= 27 milliseconds92.98% &lt;= 28 milliseconds94.47% &lt;= 29 milliseconds95.47% &lt;= 30 milliseconds96.38% &lt;= 31 milliseconds96.75% &lt;= 32 milliseconds97.23% &lt;= 33 milliseconds97.64% &lt;= 34 milliseconds97.87% &lt;= 35 milliseconds97.98% &lt;= 36 milliseconds98.01% &lt;= 37 milliseconds98.11% &lt;= 38 milliseconds98.16% &lt;= 39 milliseconds98.17% &lt;= 40 milliseconds98.23% &lt;= 41 milliseconds98.24% &lt;= 42 milliseconds98.29% &lt;= 43 milliseconds98.39% &lt;= 44 milliseconds98.44% &lt;= 46 milliseconds98.47% &lt;= 47 milliseconds98.52% &lt;= 48 milliseconds98.53% &lt;= 51 milliseconds98.53% &lt;= 52 milliseconds98.57% &lt;= 53 milliseconds98.63% &lt;= 54 milliseconds98.67% &lt;= 55 milliseconds98.75% &lt;= 61 milliseconds98.75% &lt;= 62 milliseconds98.85% &lt;= 63 milliseconds98.86% &lt;= 65 milliseconds98.88% &lt;= 66 milliseconds98.92% &lt;= 67 milliseconds98.93% &lt;= 68 milliseconds98.96% &lt;= 71 milliseconds98.97% &lt;= 72 milliseconds98.98% &lt;= 73 milliseconds98.99% &lt;= 74 milliseconds99.00% &lt;= 77 milliseconds99.04% &lt;= 78 milliseconds99.05% &lt;= 81 milliseconds99.08% &lt;= 82 milliseconds99.09% &lt;= 87 milliseconds99.13% &lt;= 88 milliseconds99.14% &lt;= 89 milliseconds99.19% &lt;= 90 milliseconds99.23% &lt;= 98 milliseconds99.28% &lt;= 101 milliseconds99.29% &lt;= 103 milliseconds99.43% &lt;= 104 milliseconds99.43% &lt;= 105 milliseconds99.47% &lt;= 109 milliseconds99.47% &lt;= 110 milliseconds99.50% &lt;= 111 milliseconds99.50% &lt;= 112 milliseconds99.58% &lt;= 119 milliseconds99.58% &lt;= 126 milliseconds99.61% &lt;= 127 milliseconds99.62% &lt;= 131 milliseconds99.62% &lt;= 132 milliseconds99.69% &lt;= 133 milliseconds99.72% &lt;= 138 milliseconds99.75% &lt;= 139 milliseconds99.78% &lt;= 140 milliseconds99.80% &lt;= 144 milliseconds99.82% &lt;= 145 milliseconds99.85% &lt;= 146 milliseconds99.89% &lt;= 147 milliseconds99.93% &lt;= 148 milliseconds99.94% &lt;= 150 milliseconds99.95% &lt;= 152 milliseconds99.96% &lt;= 153 milliseconds100.00% &lt;= 153 milliseconds39541.32 requests per secondLPUSH: 0.00LPUSH: 36614.23LPUSH: 38839.77LPUSH: 39792.75LPUSH: 41082.19LPUSH: 40486.64LPUSH: 40505.58LPUSH: 40440.00LPUSH: 40923.00LPUSH: 41500.44====== LPUSH ====== 100000 requests completed in 2.43 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 3 milliseconds0.01% &lt;= 4 milliseconds0.02% &lt;= 5 milliseconds0.02% &lt;= 6 milliseconds0.40% &lt;= 7 milliseconds0.89% &lt;= 8 milliseconds1.33% &lt;= 9 milliseconds2.36% &lt;= 10 milliseconds3.60% &lt;= 11 milliseconds4.92% &lt;= 12 milliseconds7.13% &lt;= 13 milliseconds9.98% &lt;= 14 milliseconds12.54% &lt;= 15 milliseconds17.18% &lt;= 16 milliseconds23.76% &lt;= 17 milliseconds33.21% &lt;= 18 milliseconds43.72% &lt;= 19 milliseconds53.84% &lt;= 20 milliseconds62.24% &lt;= 21 milliseconds68.77% &lt;= 22 milliseconds74.40% &lt;= 23 milliseconds78.45% &lt;= 24 milliseconds82.74% &lt;= 25 milliseconds86.25% &lt;= 26 milliseconds89.99% &lt;= 27 milliseconds91.90% &lt;= 28 milliseconds93.56% &lt;= 29 milliseconds95.13% &lt;= 30 milliseconds96.70% &lt;= 31 milliseconds98.21% &lt;= 32 milliseconds98.88% &lt;= 33 milliseconds99.36% &lt;= 34 milliseconds99.52% &lt;= 35 milliseconds99.67% &lt;= 36 milliseconds99.70% &lt;= 38 milliseconds99.73% &lt;= 39 milliseconds99.82% &lt;= 40 milliseconds99.89% &lt;= 41 milliseconds99.93% &lt;= 42 milliseconds100.00% &lt;= 47 milliseconds41084.63 requests per secondRPUSH: 0.00RPUSH: 38049.50RPUSH: 40777.58RPUSH: 42499.38RPUSH: 43915.01RPUSH: 44144.16RPUSH: 44390.20RPUSH: 45003.84RPUSH: 44804.44====== RPUSH ====== 100000 requests completed in 2.23 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 4 milliseconds0.00% &lt;= 5 milliseconds0.28% &lt;= 6 milliseconds1.06% &lt;= 7 milliseconds2.16% &lt;= 8 milliseconds3.68% &lt;= 9 milliseconds5.77% &lt;= 10 milliseconds7.78% &lt;= 11 milliseconds10.56% &lt;= 12 milliseconds14.10% &lt;= 13 milliseconds20.39% &lt;= 14 milliseconds27.68% &lt;= 15 milliseconds37.40% &lt;= 16 milliseconds49.89% &lt;= 17 milliseconds58.89% &lt;= 18 milliseconds64.72% &lt;= 19 milliseconds71.89% &lt;= 20 milliseconds79.99% &lt;= 21 milliseconds85.12% &lt;= 22 milliseconds88.84% &lt;= 23 milliseconds91.48% &lt;= 24 milliseconds93.80% &lt;= 25 milliseconds95.13% &lt;= 26 milliseconds96.33% &lt;= 27 milliseconds97.43% &lt;= 28 milliseconds97.90% &lt;= 29 milliseconds98.64% &lt;= 30 milliseconds98.76% &lt;= 31 milliseconds98.95% &lt;= 32 milliseconds99.24% &lt;= 33 milliseconds99.37% &lt;= 34 milliseconds99.42% &lt;= 35 milliseconds99.44% &lt;= 36 milliseconds99.46% &lt;= 37 milliseconds99.55% &lt;= 38 milliseconds99.61% &lt;= 41 milliseconds99.65% &lt;= 42 milliseconds99.67% &lt;= 43 milliseconds99.73% &lt;= 44 milliseconds99.89% &lt;= 45 milliseconds99.90% &lt;= 46 milliseconds99.90% &lt;= 47 milliseconds99.92% &lt;= 48 milliseconds99.95% &lt;= 49 milliseconds100.00% &lt;= 50 milliseconds100.00% &lt;= 51 milliseconds44883.30 requests per secondLPOP: 0.00LPOP: 41616.54LPOP: 41331.40LPOP: 41964.80LPOP: 42209.23LPOP: 42445.58LPOP: 42723.98LPOP: 42962.69LPOP: 42578.01LPOP: 42283.45====== LPOP ====== 100000 requests completed in 2.38 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 4 milliseconds0.04% &lt;= 5 milliseconds0.26% &lt;= 6 milliseconds1.06% &lt;= 7 milliseconds2.21% &lt;= 8 milliseconds3.59% &lt;= 9 milliseconds5.04% &lt;= 10 milliseconds6.92% &lt;= 11 milliseconds9.99% &lt;= 12 milliseconds13.85% &lt;= 13 milliseconds18.83% &lt;= 14 milliseconds24.43% &lt;= 15 milliseconds30.09% &lt;= 16 milliseconds38.13% &lt;= 17 milliseconds48.56% &lt;= 18 milliseconds58.66% &lt;= 19 milliseconds67.06% &lt;= 20 milliseconds73.94% &lt;= 21 milliseconds80.23% &lt;= 22 milliseconds84.82% &lt;= 23 milliseconds87.86% &lt;= 24 milliseconds90.00% &lt;= 25 milliseconds92.32% &lt;= 26 milliseconds93.93% &lt;= 27 milliseconds95.66% &lt;= 28 milliseconds96.98% &lt;= 29 milliseconds97.64% &lt;= 30 milliseconds98.20% &lt;= 31 milliseconds99.04% &lt;= 32 milliseconds99.46% &lt;= 33 milliseconds99.69% &lt;= 34 milliseconds99.73% &lt;= 35 milliseconds99.75% &lt;= 36 milliseconds99.83% &lt;= 37 milliseconds99.83% &lt;= 39 milliseconds99.90% &lt;= 40 milliseconds99.95% &lt;= 41 milliseconds99.98% &lt;= 45 milliseconds100.00% &lt;= 45 milliseconds42087.54 requests per secondRPOP: 0.00RPOP: 43228.26RPOP: 45958.25RPOP: 46097.81RPOP: 45476.19RPOP: 44500.39RPOP: 43137.75RPOP: 42008.93RPOP: 41954.97RPOP: 41620.23====== RPOP ====== 100000 requests completed in 2.40 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 3 milliseconds0.01% &lt;= 4 milliseconds0.11% &lt;= 5 milliseconds0.46% &lt;= 6 milliseconds1.10% &lt;= 7 milliseconds1.93% &lt;= 8 milliseconds3.23% &lt;= 9 milliseconds4.46% &lt;= 10 milliseconds6.30% &lt;= 11 milliseconds9.22% &lt;= 12 milliseconds13.87% &lt;= 13 milliseconds17.58% &lt;= 14 milliseconds22.66% &lt;= 15 milliseconds28.07% &lt;= 16 milliseconds38.93% &lt;= 17 milliseconds49.37% &lt;= 18 milliseconds56.45% &lt;= 19 milliseconds62.92% &lt;= 20 milliseconds69.25% &lt;= 21 milliseconds74.77% &lt;= 22 milliseconds79.90% &lt;= 23 milliseconds83.63% &lt;= 24 milliseconds86.96% &lt;= 25 milliseconds89.32% &lt;= 26 milliseconds91.51% &lt;= 27 milliseconds93.45% &lt;= 28 milliseconds95.22% &lt;= 29 milliseconds96.43% &lt;= 30 milliseconds97.09% &lt;= 31 milliseconds97.66% &lt;= 32 milliseconds98.19% &lt;= 33 milliseconds98.72% &lt;= 34 milliseconds99.23% &lt;= 35 milliseconds99.37% &lt;= 36 milliseconds99.42% &lt;= 37 milliseconds99.62% &lt;= 38 milliseconds99.88% &lt;= 39 milliseconds99.94% &lt;= 40 milliseconds99.95% &lt;= 41 milliseconds100.00% &lt;= 41 milliseconds41649.31 requests per secondSADD: 0.00SADD: 51855.02SADD: 52179.73SADD: 52210.32SADD: 51780.37SADD: 51128.91SADD: 51004.55SADD: 50487.45====== SADD ====== 100000 requests completed in 2.01 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 2 milliseconds0.02% &lt;= 3 milliseconds0.69% &lt;= 4 milliseconds1.65% &lt;= 5 milliseconds3.53% &lt;= 6 milliseconds7.50% &lt;= 7 milliseconds13.13% &lt;= 8 milliseconds20.58% &lt;= 9 milliseconds29.14% &lt;= 10 milliseconds39.49% &lt;= 11 milliseconds52.15% &lt;= 12 milliseconds63.90% &lt;= 13 milliseconds72.46% &lt;= 14 milliseconds77.78% &lt;= 15 milliseconds83.04% &lt;= 16 milliseconds87.36% &lt;= 17 milliseconds91.32% &lt;= 18 milliseconds94.04% &lt;= 19 milliseconds95.76% &lt;= 20 milliseconds97.11% &lt;= 21 milliseconds97.93% &lt;= 22 milliseconds98.69% &lt;= 23 milliseconds99.18% &lt;= 24 milliseconds99.45% &lt;= 25 milliseconds99.75% &lt;= 26 milliseconds99.90% &lt;= 27 milliseconds99.95% &lt;= 28 milliseconds100.00% &lt;= 30 milliseconds49776.01 requests per secondSPOP: 0.00SPOP: 46968.09SPOP: 41507.52SPOP: 37956.69SPOP: 40148.79SPOP: 38308.35SPOP: 37963.13SPOP: 39410.68SPOP: 39633.40SPOP: 38861.61====== SPOP ====== 100000 requests completed in 2.54 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 3 milliseconds0.44% &lt;= 4 milliseconds1.17% &lt;= 5 milliseconds2.73% &lt;= 6 milliseconds5.39% &lt;= 7 milliseconds8.72% &lt;= 8 milliseconds13.43% &lt;= 9 milliseconds20.22% &lt;= 10 milliseconds27.72% &lt;= 11 milliseconds36.48% &lt;= 12 milliseconds46.30% &lt;= 13 milliseconds55.81% &lt;= 14 milliseconds63.46% &lt;= 15 milliseconds69.16% &lt;= 16 milliseconds74.49% &lt;= 17 milliseconds78.85% &lt;= 18 milliseconds82.19% &lt;= 19 milliseconds85.55% &lt;= 20 milliseconds87.73% &lt;= 21 milliseconds89.88% &lt;= 22 milliseconds91.31% &lt;= 23 milliseconds92.50% &lt;= 24 milliseconds93.28% &lt;= 25 milliseconds93.82% &lt;= 26 milliseconds94.76% &lt;= 27 milliseconds95.52% &lt;= 28 milliseconds96.31% &lt;= 29 milliseconds97.22% &lt;= 30 milliseconds97.57% &lt;= 31 milliseconds97.86% &lt;= 32 milliseconds98.00% &lt;= 33 milliseconds98.18% &lt;= 34 milliseconds98.36% &lt;= 35 milliseconds98.57% &lt;= 36 milliseconds98.75% &lt;= 37 milliseconds98.95% &lt;= 38 milliseconds99.20% &lt;= 39 milliseconds99.38% &lt;= 40 milliseconds99.47% &lt;= 41 milliseconds99.61% &lt;= 42 milliseconds99.72% &lt;= 43 milliseconds99.77% &lt;= 44 milliseconds99.80% &lt;= 45 milliseconds99.83% &lt;= 46 milliseconds99.84% &lt;= 47 milliseconds99.84% &lt;= 48 milliseconds99.89% &lt;= 49 milliseconds99.93% &lt;= 50 milliseconds99.96% &lt;= 51 milliseconds99.96% &lt;= 54 milliseconds99.98% &lt;= 55 milliseconds100.00% &lt;= 55 milliseconds39385.59 requests per secondLPUSH (needed to benchmark LRANGE): 0.00LPUSH (needed to benchmark LRANGE): 40350.00LPUSH (needed to benchmark LRANGE): 43356.16LPUSH (needed to benchmark LRANGE): 39123.54LPUSH (needed to benchmark LRANGE): 36397.67LPUSH (needed to benchmark LRANGE): 37048.40LPUSH (needed to benchmark LRANGE): 35118.49LPUSH (needed to benchmark LRANGE): 35086.59LPUSH (needed to benchmark LRANGE): 34418.49LPUSH (needed to benchmark LRANGE): 33290.24LPUSH (needed to benchmark LRANGE): 33927.59LPUSH (needed to benchmark LRANGE): 33270.23LPUSH (needed to benchmark LRANGE): 32143.14====== LPUSH (needed to benchmark LRANGE) ====== 100000 requests completed in 3.16 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 3 milliseconds0.01% &lt;= 4 milliseconds0.01% &lt;= 5 milliseconds0.15% &lt;= 6 milliseconds0.29% &lt;= 7 milliseconds0.53% &lt;= 8 milliseconds1.07% &lt;= 9 milliseconds1.90% &lt;= 10 milliseconds3.40% &lt;= 11 milliseconds5.49% &lt;= 12 milliseconds7.96% &lt;= 13 milliseconds10.60% &lt;= 14 milliseconds14.91% &lt;= 15 milliseconds20.34% &lt;= 16 milliseconds24.59% &lt;= 17 milliseconds30.83% &lt;= 18 milliseconds36.47% &lt;= 19 milliseconds42.94% &lt;= 20 milliseconds48.69% &lt;= 21 milliseconds52.65% &lt;= 22 milliseconds56.48% &lt;= 23 milliseconds59.96% &lt;= 24 milliseconds63.13% &lt;= 25 milliseconds66.65% &lt;= 26 milliseconds69.27% &lt;= 27 milliseconds71.21% &lt;= 28 milliseconds73.74% &lt;= 29 milliseconds76.12% &lt;= 30 milliseconds78.21% &lt;= 31 milliseconds80.02% &lt;= 32 milliseconds81.45% &lt;= 33 milliseconds83.16% &lt;= 34 milliseconds85.16% &lt;= 35 milliseconds86.69% &lt;= 36 milliseconds87.77% &lt;= 37 milliseconds88.80% &lt;= 38 milliseconds89.97% &lt;= 39 milliseconds90.75% &lt;= 40 milliseconds91.38% &lt;= 41 milliseconds92.17% &lt;= 42 milliseconds93.18% &lt;= 43 milliseconds93.55% &lt;= 44 milliseconds93.84% &lt;= 45 milliseconds94.40% &lt;= 46 milliseconds94.86% &lt;= 47 milliseconds95.40% &lt;= 48 milliseconds95.70% &lt;= 49 milliseconds96.27% &lt;= 50 milliseconds96.64% &lt;= 51 milliseconds97.06% &lt;= 52 milliseconds97.36% &lt;= 53 milliseconds97.51% &lt;= 54 milliseconds97.67% &lt;= 55 milliseconds97.94% &lt;= 56 milliseconds98.12% &lt;= 57 milliseconds98.23% &lt;= 58 milliseconds98.26% &lt;= 59 milliseconds98.27% &lt;= 60 milliseconds98.30% &lt;= 61 milliseconds98.41% &lt;= 62 milliseconds98.51% &lt;= 63 milliseconds98.60% &lt;= 64 milliseconds98.74% &lt;= 65 milliseconds98.89% &lt;= 66 milliseconds99.02% &lt;= 67 milliseconds99.16% &lt;= 68 milliseconds99.21% &lt;= 69 milliseconds99.35% &lt;= 70 milliseconds99.45% &lt;= 71 milliseconds99.47% &lt;= 72 milliseconds99.53% &lt;= 73 milliseconds99.61% &lt;= 74 milliseconds99.66% &lt;= 78 milliseconds99.68% &lt;= 79 milliseconds99.71% &lt;= 80 milliseconds99.74% &lt;= 81 milliseconds99.78% &lt;= 84 milliseconds99.87% &lt;= 85 milliseconds99.95% &lt;= 86 milliseconds99.99% &lt;= 87 milliseconds100.00% &lt;= 91 milliseconds31665.61 requests per secondLRANGE_100 (first 100 elements): 0.00LRANGE_100 (first 100 elements): 16503.68LRANGE_100 (first 100 elements): 16992.54LRANGE_100 (first 100 elements): 16684.67LRANGE_100 (first 100 elements): 17255.86LRANGE_100 (first 100 elements): 16290.59LRANGE_100 (first 100 elements): 15484.75LRANGE_100 (first 100 elements): 14759.02LRANGE_100 (first 100 elements): 15009.30LRANGE_100 (first 100 elements): 14440.08LRANGE_100 (first 100 elements): 14616.45LRANGE_100 (first 100 elements): 14677.33LRANGE_100 (first 100 elements): 14453.56LRANGE_100 (first 100 elements): 14703.44LRANGE_100 (first 100 elements): 14967.08LRANGE_100 (first 100 elements): 15217.75LRANGE_100 (first 100 elements): 15423.78LRANGE_100 (first 100 elements): 15582.29LRANGE_100 (first 100 elements): 15879.39LRANGE_100 (first 100 elements): 16118.90LRANGE_100 (first 100 elements): 16300.07LRANGE_100 (first 100 elements): 16454.85LRANGE_100 (first 100 elements): 16486.72====== LRANGE_100 (first 100 elements) ====== 100000 requests completed in 6.04 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 14 milliseconds0.02% &lt;= 15 milliseconds0.06% &lt;= 16 milliseconds0.30% &lt;= 17 milliseconds0.60% &lt;= 18 milliseconds1.57% &lt;= 19 milliseconds2.61% &lt;= 20 milliseconds4.04% &lt;= 21 milliseconds5.86% &lt;= 22 milliseconds8.25% &lt;= 23 milliseconds11.08% &lt;= 24 milliseconds15.03% &lt;= 25 milliseconds19.80% &lt;= 26 milliseconds24.77% &lt;= 27 milliseconds29.86% &lt;= 28 milliseconds35.47% &lt;= 29 milliseconds40.37% &lt;= 30 milliseconds45.40% &lt;= 31 milliseconds50.50% &lt;= 32 milliseconds55.05% &lt;= 33 milliseconds59.34% &lt;= 34 milliseconds62.12% &lt;= 35 milliseconds64.71% &lt;= 36 milliseconds67.20% &lt;= 37 milliseconds69.43% &lt;= 38 milliseconds71.26% &lt;= 39 milliseconds72.84% &lt;= 40 milliseconds74.64% &lt;= 41 milliseconds76.11% &lt;= 42 milliseconds77.42% &lt;= 43 milliseconds78.57% &lt;= 44 milliseconds79.94% &lt;= 45 milliseconds81.80% &lt;= 46 milliseconds83.39% &lt;= 47 milliseconds85.25% &lt;= 48 milliseconds86.78% &lt;= 49 milliseconds88.46% &lt;= 50 milliseconds90.03% &lt;= 51 milliseconds91.47% &lt;= 52 milliseconds92.51% &lt;= 53 milliseconds93.28% &lt;= 54 milliseconds94.00% &lt;= 55 milliseconds94.62% &lt;= 56 milliseconds95.25% &lt;= 57 milliseconds95.74% &lt;= 58 milliseconds96.01% &lt;= 59 milliseconds96.38% &lt;= 60 milliseconds96.77% &lt;= 61 milliseconds97.07% &lt;= 62 milliseconds97.47% &lt;= 63 milliseconds97.76% &lt;= 64 milliseconds98.07% &lt;= 65 milliseconds98.31% &lt;= 66 milliseconds98.47% &lt;= 67 milliseconds98.61% &lt;= 68 milliseconds98.78% &lt;= 69 milliseconds98.94% &lt;= 70 milliseconds99.06% &lt;= 71 milliseconds99.13% &lt;= 72 milliseconds99.16% &lt;= 73 milliseconds99.27% &lt;= 74 milliseconds99.37% &lt;= 75 milliseconds99.47% &lt;= 76 milliseconds99.62% &lt;= 77 milliseconds99.69% &lt;= 78 milliseconds99.71% &lt;= 79 milliseconds99.82% &lt;= 80 milliseconds99.87% &lt;= 81 milliseconds99.92% &lt;= 82 milliseconds99.95% &lt;= 83 milliseconds99.97% &lt;= 84 milliseconds99.99% &lt;= 85 milliseconds100.00% &lt;= 86 milliseconds100.00% &lt;= 87 milliseconds100.00% &lt;= 87 milliseconds16564.52 requests per secondLRANGE_300 (first 300 elements): 0.00LRANGE_300 (first 300 elements): 5424.24LRANGE_300 (first 300 elements): 4953.49LRANGE_300 (first 300 elements): 5237.59LRANGE_300 (first 300 elements): 4992.38LRANGE_300 (first 300 elements): 5170.24LRANGE_300 (first 300 elements): 5340.90LRANGE_300 (first 300 elements): 5202.19LRANGE_300 (first 300 elements): 5245.88LRANGE_300 (first 300 elements): 5216.19LRANGE_300 (first 300 elements): 5155.18LRANGE_300 (first 300 elements): 5167.99LRANGE_300 (first 300 elements): 5142.78LRANGE_300 (first 300 elements): 5137.88LRANGE_300 (first 300 elements): 5133.38LRANGE_300 (first 300 elements): 5219.28LRANGE_300 (first 300 elements): 5178.15LRANGE_300 (first 300 elements): 5163.39LRANGE_300 (first 300 elements): 5213.09LRANGE_300 (first 300 elements): 5157.33LRANGE_300 (first 300 elements): 5158.23LRANGE_300 (first 300 elements): 5179.05LRANGE_300 (first 300 elements): 5215.59LRANGE_300 (first 300 elements): 5177.84LRANGE_300 (first 300 elements): 5132.08LRANGE_300 (first 300 elements): 5106.08LRANGE_300 (first 300 elements): 5170.38LRANGE_300 (first 300 elements): 5236.09LRANGE_300 (first 300 elements): 5272.49LRANGE_300 (first 300 elements): 5272.96LRANGE_300 (first 300 elements): 5295.94LRANGE_300 (first 300 elements): 5306.22LRANGE_300 (first 300 elements): 5268.58LRANGE_300 (first 300 elements): 5224.52LRANGE_300 (first 300 elements): 5232.12LRANGE_300 (first 300 elements): 5186.02LRANGE_300 (first 300 elements): 5121.18LRANGE_300 (first 300 elements): 5129.83LRANGE_300 (first 300 elements): 5091.03LRANGE_300 (first 300 elements): 5100.93LRANGE_300 (first 300 elements): 5127.19LRANGE_300 (first 300 elements): 5157.95LRANGE_300 (first 300 elements): 5129.84LRANGE_300 (first 300 elements): 5170.22LRANGE_300 (first 300 elements): 5203.57LRANGE_300 (first 300 elements): 5235.13LRANGE_300 (first 300 elements): 5266.25LRANGE_300 (first 300 elements): 5297.99LRANGE_300 (first 300 elements): 5329.12LRANGE_300 (first 300 elements): 5359.23LRANGE_300 (first 300 elements): 5383.11LRANGE_300 (first 300 elements): 5410.87LRANGE_300 (first 300 elements): 5435.52LRANGE_300 (first 300 elements): 5456.57LRANGE_300 (first 300 elements): 5481.72LRANGE_300 (first 300 elements): 5501.63LRANGE_300 (first 300 elements): 5514.00LRANGE_300 (first 300 elements): 5523.20LRANGE_300 (first 300 elements): 5499.37LRANGE_300 (first 300 elements): 5517.71LRANGE_300 (first 300 elements): 5492.07====== LRANGE_300 (first 300 elements) ====== 100000 requests completed in 18.19 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 10 milliseconds0.01% &lt;= 15 milliseconds0.01% &lt;= 16 milliseconds0.02% &lt;= 17 milliseconds0.03% &lt;= 18 milliseconds0.04% &lt;= 19 milliseconds0.05% &lt;= 24 milliseconds0.05% &lt;= 25 milliseconds0.07% &lt;= 26 milliseconds0.08% &lt;= 27 milliseconds0.09% &lt;= 31 milliseconds0.09% &lt;= 32 milliseconds0.10% &lt;= 33 milliseconds0.10% &lt;= 34 milliseconds0.11% &lt;= 38 milliseconds0.11% &lt;= 39 milliseconds0.12% &lt;= 40 milliseconds0.12% &lt;= 41 milliseconds0.12% &lt;= 42 milliseconds0.12% &lt;= 44 milliseconds0.13% &lt;= 45 milliseconds0.15% &lt;= 46 milliseconds0.16% &lt;= 47 milliseconds0.20% &lt;= 49 milliseconds0.21% &lt;= 50 milliseconds0.23% &lt;= 51 milliseconds0.24% &lt;= 52 milliseconds0.25% &lt;= 53 milliseconds0.27% &lt;= 54 milliseconds0.27% &lt;= 55 milliseconds0.29% &lt;= 56 milliseconds0.30% &lt;= 57 milliseconds0.36% &lt;= 58 milliseconds0.48% &lt;= 59 milliseconds0.56% &lt;= 60 milliseconds0.59% &lt;= 61 milliseconds0.75% &lt;= 62 milliseconds1.01% &lt;= 63 milliseconds1.46% &lt;= 64 milliseconds1.86% &lt;= 65 milliseconds2.59% &lt;= 66 milliseconds3.60% &lt;= 67 milliseconds5.26% &lt;= 68 milliseconds7.57% &lt;= 69 milliseconds9.84% &lt;= 70 milliseconds12.41% &lt;= 71 milliseconds15.03% &lt;= 72 milliseconds17.77% &lt;= 73 milliseconds20.65% &lt;= 74 milliseconds23.67% &lt;= 75 milliseconds26.44% &lt;= 76 milliseconds29.34% &lt;= 77 milliseconds32.20% &lt;= 78 milliseconds35.59% &lt;= 79 milliseconds39.14% &lt;= 80 milliseconds42.72% &lt;= 81 milliseconds46.16% &lt;= 82 milliseconds48.95% &lt;= 83 milliseconds51.74% &lt;= 84 milliseconds54.20% &lt;= 85 milliseconds56.20% &lt;= 86 milliseconds58.07% &lt;= 87 milliseconds59.51% &lt;= 88 milliseconds60.61% &lt;= 89 milliseconds61.36% &lt;= 90 milliseconds62.18% &lt;= 91 milliseconds62.93% &lt;= 92 milliseconds63.65% &lt;= 93 milliseconds64.32% &lt;= 94 milliseconds65.02% &lt;= 95 milliseconds65.87% &lt;= 96 milliseconds66.80% &lt;= 97 milliseconds67.74% &lt;= 98 milliseconds68.70% &lt;= 99 milliseconds69.62% &lt;= 100 milliseconds70.54% &lt;= 101 milliseconds71.44% &lt;= 102 milliseconds72.12% &lt;= 103 milliseconds72.83% &lt;= 104 milliseconds73.39% &lt;= 105 milliseconds73.95% &lt;= 106 milliseconds74.51% &lt;= 107 milliseconds75.08% &lt;= 108 milliseconds75.57% &lt;= 109 milliseconds76.17% &lt;= 110 milliseconds76.73% &lt;= 111 milliseconds77.24% &lt;= 112 milliseconds77.79% &lt;= 113 milliseconds78.32% &lt;= 114 milliseconds78.97% &lt;= 115 milliseconds79.68% &lt;= 116 milliseconds80.32% &lt;= 117 milliseconds80.95% &lt;= 118 milliseconds81.56% &lt;= 119 milliseconds82.16% &lt;= 120 milliseconds82.76% &lt;= 121 milliseconds83.24% &lt;= 122 milliseconds83.53% &lt;= 123 milliseconds83.80% &lt;= 124 milliseconds84.23% &lt;= 125 milliseconds84.75% &lt;= 126 milliseconds85.28% &lt;= 127 milliseconds85.80% &lt;= 128 milliseconds86.35% &lt;= 129 milliseconds86.87% &lt;= 130 milliseconds87.31% &lt;= 131 milliseconds87.84% &lt;= 132 milliseconds88.46% &lt;= 133 milliseconds89.07% &lt;= 134 milliseconds89.72% &lt;= 135 milliseconds90.25% &lt;= 136 milliseconds90.76% &lt;= 137 milliseconds91.29% &lt;= 138 milliseconds91.68% &lt;= 139 milliseconds92.05% &lt;= 140 milliseconds92.33% &lt;= 141 milliseconds92.61% &lt;= 142 milliseconds92.94% &lt;= 143 milliseconds93.24% &lt;= 144 milliseconds93.59% &lt;= 145 milliseconds93.93% &lt;= 146 milliseconds94.29% &lt;= 147 milliseconds94.67% &lt;= 148 milliseconds95.05% &lt;= 149 milliseconds95.42% &lt;= 150 milliseconds95.72% &lt;= 151 milliseconds96.06% &lt;= 152 milliseconds96.31% &lt;= 153 milliseconds96.51% &lt;= 154 milliseconds96.76% &lt;= 155 milliseconds96.92% &lt;= 156 milliseconds97.06% &lt;= 157 milliseconds97.18% &lt;= 158 milliseconds97.31% &lt;= 159 milliseconds97.43% &lt;= 160 milliseconds97.57% &lt;= 161 milliseconds97.73% &lt;= 162 milliseconds97.88% &lt;= 163 milliseconds98.00% &lt;= 164 milliseconds98.12% &lt;= 165 milliseconds98.29% &lt;= 166 milliseconds98.41% &lt;= 167 milliseconds98.58% &lt;= 168 milliseconds98.67% &lt;= 169 milliseconds98.74% &lt;= 170 milliseconds98.83% &lt;= 171 milliseconds98.91% &lt;= 172 milliseconds98.97% &lt;= 173 milliseconds99.04% &lt;= 174 milliseconds99.10% &lt;= 175 milliseconds99.16% &lt;= 176 milliseconds99.21% &lt;= 177 milliseconds99.29% &lt;= 178 milliseconds99.36% &lt;= 179 milliseconds99.43% &lt;= 180 milliseconds99.49% &lt;= 181 milliseconds99.54% &lt;= 182 milliseconds99.58% &lt;= 183 milliseconds99.62% &lt;= 184 milliseconds99.65% &lt;= 185 milliseconds99.70% &lt;= 186 milliseconds99.73% &lt;= 187 milliseconds99.75% &lt;= 188 milliseconds99.77% &lt;= 189 milliseconds99.77% &lt;= 190 milliseconds99.77% &lt;= 191 milliseconds99.78% &lt;= 192 milliseconds99.81% &lt;= 193 milliseconds99.82% &lt;= 194 milliseconds99.83% &lt;= 195 milliseconds99.84% &lt;= 196 milliseconds99.85% &lt;= 197 milliseconds99.85% &lt;= 198 milliseconds99.86% &lt;= 199 milliseconds99.87% &lt;= 200 milliseconds99.88% &lt;= 201 milliseconds99.89% &lt;= 202 milliseconds99.91% &lt;= 203 milliseconds99.92% &lt;= 204 milliseconds99.94% &lt;= 205 milliseconds99.95% &lt;= 206 milliseconds99.96% &lt;= 207 milliseconds99.96% &lt;= 208 milliseconds99.98% &lt;= 209 milliseconds99.98% &lt;= 210 milliseconds99.98% &lt;= 211 milliseconds99.98% &lt;= 212 milliseconds99.99% &lt;= 213 milliseconds100.00% &lt;= 214 milliseconds100.00% &lt;= 215 milliseconds5496.62 requests per secondLRANGE_500 (first 450 elements): 0.00LRANGE_500 (first 450 elements): 3957.45LRANGE_500 (first 450 elements): 4221.52LRANGE_500 (first 450 elements): 3893.62LRANGE_500 (first 450 elements): 4178.99LRANGE_500 (first 450 elements): 4379.11LRANGE_500 (first 450 elements): 4388.22LRANGE_500 (first 450 elements): 4402.96LRANGE_500 (first 450 elements): 4332.26LRANGE_500 (first 450 elements): 4405.73LRANGE_500 (first 450 elements): 4532.40LRANGE_500 (first 450 elements): 4614.05LRANGE_500 (first 450 elements): 4638.36LRANGE_500 (first 450 elements): 4652.15LRANGE_500 (first 450 elements): 4710.54LRANGE_500 (first 450 elements): 4640.12LRANGE_500 (first 450 elements): 4660.82LRANGE_500 (first 450 elements): 4706.78LRANGE_500 (first 450 elements): 4749.03LRANGE_500 (first 450 elements): 4764.20LRANGE_500 (first 450 elements): 4734.31LRANGE_500 (first 450 elements): 4709.53LRANGE_500 (first 450 elements): 4745.56LRANGE_500 (first 450 elements): 4751.44LRANGE_500 (first 450 elements): 4764.13LRANGE_500 (first 450 elements): 4741.03LRANGE_500 (first 450 elements): 4748.73LRANGE_500 (first 450 elements): 4739.17LRANGE_500 (first 450 elements): 4735.25LRANGE_500 (first 450 elements): 4713.63LRANGE_500 (first 450 elements): 4708.21LRANGE_500 (first 450 elements): 4720.78LRANGE_500 (first 450 elements): 4708.30LRANGE_500 (first 450 elements): 4687.53LRANGE_500 (first 450 elements): 4711.45LRANGE_500 (first 450 elements): 4721.83LRANGE_500 (first 450 elements): 4744.80LRANGE_500 (first 450 elements): 4765.81LRANGE_500 (first 450 elements): 4784.21LRANGE_500 (first 450 elements): 4805.87LRANGE_500 (first 450 elements): 4824.25LRANGE_500 (first 450 elements): 4835.78LRANGE_500 (first 450 elements): 4834.69LRANGE_500 (first 450 elements): 4844.05LRANGE_500 (first 450 elements): 4848.40LRANGE_500 (first 450 elements): 4865.84LRANGE_500 (first 450 elements): 4892.78LRANGE_500 (first 450 elements): 4906.77LRANGE_500 (first 450 elements): 4924.66LRANGE_500 (first 450 elements): 4935.96LRANGE_500 (first 450 elements): 4925.82LRANGE_500 (first 450 elements): 4932.85LRANGE_500 (first 450 elements): 4941.81LRANGE_500 (first 450 elements): 4953.68LRANGE_500 (first 450 elements): 4965.87LRANGE_500 (first 450 elements): 4979.25LRANGE_500 (first 450 elements): 4990.95LRANGE_500 (first 450 elements): 5000.78LRANGE_500 (first 450 elements): 5005.47LRANGE_500 (first 450 elements): 5022.85LRANGE_500 (first 450 elements): 5037.54LRANGE_500 (first 450 elements): 5045.18LRANGE_500 (first 450 elements): 5050.72LRANGE_500 (first 450 elements): 5060.99LRANGE_500 (first 450 elements): 5071.47LRANGE_500 (first 450 elements): 5079.39LRANGE_500 (first 450 elements): 5089.05LRANGE_500 (first 450 elements): 5090.76====== LRANGE_500 (first 450 elements) ====== 100000 requests completed in 19.63 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 17 milliseconds0.02% &lt;= 18 milliseconds0.02% &lt;= 22 milliseconds0.03% &lt;= 23 milliseconds0.03% &lt;= 54 milliseconds0.04% &lt;= 55 milliseconds0.04% &lt;= 56 milliseconds0.04% &lt;= 60 milliseconds0.04% &lt;= 61 milliseconds0.06% &lt;= 62 milliseconds0.09% &lt;= 63 milliseconds0.10% &lt;= 64 milliseconds0.12% &lt;= 65 milliseconds0.15% &lt;= 66 milliseconds0.15% &lt;= 67 milliseconds0.16% &lt;= 68 milliseconds0.18% &lt;= 69 milliseconds0.28% &lt;= 70 milliseconds0.33% &lt;= 71 milliseconds0.40% &lt;= 72 milliseconds0.47% &lt;= 73 milliseconds0.56% &lt;= 74 milliseconds0.67% &lt;= 75 milliseconds0.92% &lt;= 76 milliseconds1.15% &lt;= 77 milliseconds1.53% &lt;= 78 milliseconds2.10% &lt;= 79 milliseconds3.06% &lt;= 80 milliseconds4.14% &lt;= 81 milliseconds5.29% &lt;= 82 milliseconds6.86% &lt;= 83 milliseconds9.29% &lt;= 84 milliseconds12.26% &lt;= 85 milliseconds16.79% &lt;= 86 milliseconds21.10% &lt;= 87 milliseconds26.00% &lt;= 88 milliseconds30.20% &lt;= 89 milliseconds34.38% &lt;= 90 milliseconds38.26% &lt;= 91 milliseconds41.93% &lt;= 92 milliseconds45.39% &lt;= 93 milliseconds48.84% &lt;= 94 milliseconds52.24% &lt;= 95 milliseconds55.75% &lt;= 96 milliseconds58.86% &lt;= 97 milliseconds61.85% &lt;= 98 milliseconds64.58% &lt;= 99 milliseconds66.92% &lt;= 100 milliseconds69.27% &lt;= 101 milliseconds71.26% &lt;= 102 milliseconds72.89% &lt;= 103 milliseconds74.40% &lt;= 104 milliseconds75.64% &lt;= 105 milliseconds76.74% &lt;= 106 milliseconds77.69% &lt;= 107 milliseconds78.39% &lt;= 108 milliseconds79.11% &lt;= 109 milliseconds80.00% &lt;= 110 milliseconds80.96% &lt;= 111 milliseconds81.76% &lt;= 112 milliseconds82.41% &lt;= 113 milliseconds83.33% &lt;= 114 milliseconds84.41% &lt;= 115 milliseconds85.41% &lt;= 116 milliseconds86.24% &lt;= 117 milliseconds86.79% &lt;= 118 milliseconds87.41% &lt;= 119 milliseconds87.93% &lt;= 120 milliseconds88.46% &lt;= 121 milliseconds88.90% &lt;= 122 milliseconds89.35% &lt;= 123 milliseconds89.79% &lt;= 124 milliseconds90.12% &lt;= 125 milliseconds90.48% &lt;= 126 milliseconds90.78% &lt;= 127 milliseconds91.08% &lt;= 128 milliseconds91.37% &lt;= 129 milliseconds91.65% &lt;= 130 milliseconds91.91% &lt;= 131 milliseconds92.25% &lt;= 132 milliseconds92.64% &lt;= 133 milliseconds92.95% &lt;= 134 milliseconds93.20% &lt;= 135 milliseconds93.50% &lt;= 136 milliseconds93.73% &lt;= 137 milliseconds93.90% &lt;= 138 milliseconds94.12% &lt;= 139 milliseconds94.36% &lt;= 140 milliseconds94.66% &lt;= 141 milliseconds94.92% &lt;= 142 milliseconds95.15% &lt;= 143 milliseconds95.33% &lt;= 144 milliseconds95.51% &lt;= 145 milliseconds95.69% &lt;= 146 milliseconds95.89% &lt;= 147 milliseconds96.14% &lt;= 148 milliseconds96.36% &lt;= 149 milliseconds96.55% &lt;= 150 milliseconds96.75% &lt;= 151 milliseconds96.83% &lt;= 152 milliseconds96.93% &lt;= 153 milliseconds97.08% &lt;= 154 milliseconds97.23% &lt;= 155 milliseconds97.32% &lt;= 156 milliseconds97.42% &lt;= 157 milliseconds97.52% &lt;= 158 milliseconds97.58% &lt;= 159 milliseconds97.66% &lt;= 160 milliseconds97.77% &lt;= 161 milliseconds97.86% &lt;= 162 milliseconds97.95% &lt;= 163 milliseconds98.06% &lt;= 164 milliseconds98.17% &lt;= 165 milliseconds98.26% &lt;= 166 milliseconds98.34% &lt;= 167 milliseconds98.41% &lt;= 168 milliseconds98.49% &lt;= 169 milliseconds98.53% &lt;= 170 milliseconds98.56% &lt;= 171 milliseconds98.57% &lt;= 172 milliseconds98.59% &lt;= 173 milliseconds98.61% &lt;= 174 milliseconds98.64% &lt;= 175 milliseconds98.69% &lt;= 176 milliseconds98.72% &lt;= 177 milliseconds98.76% &lt;= 178 milliseconds98.80% &lt;= 179 milliseconds98.83% &lt;= 180 milliseconds98.86% &lt;= 181 milliseconds98.88% &lt;= 182 milliseconds98.91% &lt;= 183 milliseconds98.92% &lt;= 184 milliseconds98.96% &lt;= 185 milliseconds98.99% &lt;= 186 milliseconds99.01% &lt;= 187 milliseconds99.02% &lt;= 188 milliseconds99.03% &lt;= 189 milliseconds99.04% &lt;= 190 milliseconds99.06% &lt;= 191 milliseconds99.08% &lt;= 192 milliseconds99.10% &lt;= 193 milliseconds99.12% &lt;= 194 milliseconds99.13% &lt;= 195 milliseconds99.15% &lt;= 196 milliseconds99.18% &lt;= 197 milliseconds99.20% &lt;= 198 milliseconds99.24% &lt;= 199 milliseconds99.25% &lt;= 200 milliseconds99.27% &lt;= 201 milliseconds99.29% &lt;= 202 milliseconds99.33% &lt;= 203 milliseconds99.36% &lt;= 204 milliseconds99.37% &lt;= 205 milliseconds99.38% &lt;= 206 milliseconds99.39% &lt;= 207 milliseconds99.42% &lt;= 208 milliseconds99.44% &lt;= 209 milliseconds99.46% &lt;= 210 milliseconds99.47% &lt;= 211 milliseconds99.48% &lt;= 212 milliseconds99.49% &lt;= 213 milliseconds99.51% &lt;= 214 milliseconds99.51% &lt;= 217 milliseconds99.52% &lt;= 218 milliseconds99.54% &lt;= 219 milliseconds99.56% &lt;= 220 milliseconds99.57% &lt;= 221 milliseconds99.60% &lt;= 222 milliseconds99.62% &lt;= 223 milliseconds99.64% &lt;= 224 milliseconds99.65% &lt;= 225 milliseconds99.66% &lt;= 226 milliseconds99.66% &lt;= 227 milliseconds99.67% &lt;= 228 milliseconds99.68% &lt;= 229 milliseconds99.70% &lt;= 230 milliseconds99.72% &lt;= 231 milliseconds99.73% &lt;= 232 milliseconds99.75% &lt;= 233 milliseconds99.78% &lt;= 234 milliseconds99.80% &lt;= 235 milliseconds99.82% &lt;= 236 milliseconds99.84% &lt;= 237 milliseconds99.86% &lt;= 238 milliseconds99.87% &lt;= 239 milliseconds99.88% &lt;= 240 milliseconds99.89% &lt;= 241 milliseconds99.90% &lt;= 242 milliseconds99.91% &lt;= 243 milliseconds99.91% &lt;= 244 milliseconds99.92% &lt;= 245 milliseconds99.92% &lt;= 246 milliseconds99.92% &lt;= 247 milliseconds99.92% &lt;= 248 milliseconds99.93% &lt;= 249 milliseconds99.94% &lt;= 250 milliseconds99.94% &lt;= 251 milliseconds99.95% &lt;= 252 milliseconds99.96% &lt;= 253 milliseconds99.97% &lt;= 254 milliseconds99.98% &lt;= 255 milliseconds99.99% &lt;= 256 milliseconds100.00% &lt;= 256 milliseconds5095.02 requests per secondLRANGE_600 (first 600 elements): 0.00LRANGE_600 (first 600 elements): 4141.48LRANGE_600 (first 600 elements): 4243.24LRANGE_600 (first 600 elements): 4312.63LRANGE_600 (first 600 elements): 4386.51LRANGE_600 (first 600 elements): 4402.65LRANGE_600 (first 600 elements): 4368.17LRANGE_600 (first 600 elements): 4401.50LRANGE_600 (first 600 elements): 4338.44LRANGE_600 (first 600 elements): 4319.55LRANGE_600 (first 600 elements): 4305.49LRANGE_600 (first 600 elements): 4296.12LRANGE_600 (first 600 elements): 4283.91LRANGE_600 (first 600 elements): 4288.89LRANGE_600 (first 600 elements): 4273.34LRANGE_600 (first 600 elements): 4269.54LRANGE_600 (first 600 elements): 4265.88LRANGE_600 (first 600 elements): 4287.48LRANGE_600 (first 600 elements): 4290.81LRANGE_600 (first 600 elements): 4297.72LRANGE_600 (first 600 elements): 4314.72LRANGE_600 (first 600 elements): 4320.29LRANGE_600 (first 600 elements): 4310.55LRANGE_600 (first 600 elements): 4299.28LRANGE_600 (first 600 elements): 4285.23LRANGE_600 (first 600 elements): 4263.42LRANGE_600 (first 600 elements): 4261.43LRANGE_600 (first 600 elements): 4245.66LRANGE_600 (first 600 elements): 4230.27LRANGE_600 (first 600 elements): 4232.99LRANGE_600 (first 600 elements): 4221.29LRANGE_600 (first 600 elements): 4183.69LRANGE_600 (first 600 elements): 4175.06LRANGE_600 (first 600 elements): 4187.09LRANGE_600 (first 600 elements): 4193.20LRANGE_600 (first 600 elements): 4196.37LRANGE_600 (first 600 elements): 4206.07LRANGE_600 (first 600 elements): 4209.42LRANGE_600 (first 600 elements): 4214.56LRANGE_600 (first 600 elements): 4220.65LRANGE_600 (first 600 elements): 4223.18LRANGE_600 (first 600 elements): 4218.68LRANGE_600 (first 600 elements): 4207.86LRANGE_600 (first 600 elements): 4177.80LRANGE_600 (first 600 elements): 4168.32LRANGE_600 (first 600 elements): 4162.25LRANGE_600 (first 600 elements): 4119.64LRANGE_600 (first 600 elements): 4115.85LRANGE_600 (first 600 elements): 4109.69LRANGE_600 (first 600 elements): 4105.83LRANGE_600 (first 600 elements): 4102.90LRANGE_600 (first 600 elements): 4096.54LRANGE_600 (first 600 elements): 4091.36LRANGE_600 (first 600 elements): 4089.24LRANGE_600 (first 600 elements): 4090.83LRANGE_600 (first 600 elements): 4089.75LRANGE_600 (first 600 elements): 4098.27LRANGE_600 (first 600 elements): 4108.65LRANGE_600 (first 600 elements): 4107.48LRANGE_600 (first 600 elements): 4105.40LRANGE_600 (first 600 elements): 4108.24LRANGE_600 (first 600 elements): 4110.35LRANGE_600 (first 600 elements): 4113.45LRANGE_600 (first 600 elements): 4108.82LRANGE_600 (first 600 elements): 4113.01LRANGE_600 (first 600 elements): 4111.77LRANGE_600 (first 600 elements): 4103.83LRANGE_600 (first 600 elements): 4096.90LRANGE_600 (first 600 elements): 4079.53LRANGE_600 (first 600 elements): 4070.41LRANGE_600 (first 600 elements): 4045.94LRANGE_600 (first 600 elements): 4045.43LRANGE_600 (first 600 elements): 4016.12LRANGE_600 (first 600 elements): 3998.59LRANGE_600 (first 600 elements): 3995.39LRANGE_600 (first 600 elements): 3989.65LRANGE_600 (first 600 elements): 3972.99LRANGE_600 (first 600 elements): 3973.29LRANGE_600 (first 600 elements): 3977.72====== LRANGE_600 (first 600 elements) ====== 100000 requests completed in 25.14 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 13 milliseconds0.01% &lt;= 26 milliseconds0.01% &lt;= 27 milliseconds0.03% &lt;= 28 milliseconds0.03% &lt;= 30 milliseconds0.03% &lt;= 31 milliseconds0.04% &lt;= 32 milliseconds0.05% &lt;= 33 milliseconds0.06% &lt;= 36 milliseconds0.06% &lt;= 37 milliseconds0.07% &lt;= 38 milliseconds0.07% &lt;= 39 milliseconds0.08% &lt;= 44 milliseconds0.09% &lt;= 45 milliseconds0.10% &lt;= 46 milliseconds0.11% &lt;= 47 milliseconds0.12% &lt;= 48 milliseconds0.13% &lt;= 49 milliseconds0.14% &lt;= 50 milliseconds0.14% &lt;= 57 milliseconds0.14% &lt;= 58 milliseconds0.15% &lt;= 59 milliseconds0.16% &lt;= 60 milliseconds0.17% &lt;= 61 milliseconds0.19% &lt;= 62 milliseconds0.21% &lt;= 63 milliseconds0.22% &lt;= 64 milliseconds0.23% &lt;= 65 milliseconds0.23% &lt;= 66 milliseconds0.24% &lt;= 67 milliseconds0.24% &lt;= 68 milliseconds0.25% &lt;= 69 milliseconds0.25% &lt;= 73 milliseconds0.25% &lt;= 74 milliseconds0.27% &lt;= 75 milliseconds0.29% &lt;= 76 milliseconds0.31% &lt;= 77 milliseconds0.33% &lt;= 78 milliseconds0.34% &lt;= 79 milliseconds0.36% &lt;= 80 milliseconds0.38% &lt;= 81 milliseconds0.41% &lt;= 82 milliseconds0.43% &lt;= 83 milliseconds0.44% &lt;= 84 milliseconds0.46% &lt;= 85 milliseconds0.47% &lt;= 86 milliseconds0.49% &lt;= 87 milliseconds0.50% &lt;= 88 milliseconds0.55% &lt;= 89 milliseconds0.59% &lt;= 90 milliseconds0.60% &lt;= 91 milliseconds0.62% &lt;= 92 milliseconds0.67% &lt;= 93 milliseconds0.69% &lt;= 94 milliseconds0.75% &lt;= 95 milliseconds0.80% &lt;= 96 milliseconds0.83% &lt;= 97 milliseconds0.97% &lt;= 98 milliseconds1.20% &lt;= 99 milliseconds1.50% &lt;= 100 milliseconds1.96% &lt;= 101 milliseconds2.40% &lt;= 102 milliseconds3.13% &lt;= 103 milliseconds3.92% &lt;= 104 milliseconds4.64% &lt;= 105 milliseconds5.48% &lt;= 106 milliseconds6.44% &lt;= 107 milliseconds7.57% &lt;= 108 milliseconds8.89% &lt;= 109 milliseconds10.33% &lt;= 110 milliseconds11.91% &lt;= 111 milliseconds13.74% &lt;= 112 milliseconds15.95% &lt;= 113 milliseconds18.46% &lt;= 114 milliseconds21.35% &lt;= 115 milliseconds24.79% &lt;= 116 milliseconds28.63% &lt;= 117 milliseconds32.30% &lt;= 118 milliseconds36.02% &lt;= 119 milliseconds39.60% &lt;= 120 milliseconds42.84% &lt;= 121 milliseconds45.72% &lt;= 122 milliseconds48.35% &lt;= 123 milliseconds50.83% &lt;= 124 milliseconds53.04% &lt;= 125 milliseconds55.24% &lt;= 126 milliseconds57.48% &lt;= 127 milliseconds59.84% &lt;= 128 milliseconds62.22% &lt;= 129 milliseconds64.55% &lt;= 130 milliseconds66.92% &lt;= 131 milliseconds69.03% &lt;= 132 milliseconds71.10% &lt;= 133 milliseconds73.06% &lt;= 134 milliseconds74.73% &lt;= 135 milliseconds76.11% &lt;= 136 milliseconds77.48% &lt;= 137 milliseconds78.60% &lt;= 138 milliseconds79.64% &lt;= 139 milliseconds80.58% &lt;= 140 milliseconds81.37% &lt;= 141 milliseconds82.08% &lt;= 142 milliseconds82.76% &lt;= 143 milliseconds83.50% &lt;= 144 milliseconds84.18% &lt;= 145 milliseconds84.92% &lt;= 146 milliseconds85.58% &lt;= 147 milliseconds86.16% &lt;= 148 milliseconds86.74% &lt;= 149 milliseconds87.37% &lt;= 150 milliseconds87.88% &lt;= 151 milliseconds88.35% &lt;= 152 milliseconds88.73% &lt;= 153 milliseconds89.11% &lt;= 154 milliseconds89.50% &lt;= 155 milliseconds89.88% &lt;= 156 milliseconds90.16% &lt;= 157 milliseconds90.41% &lt;= 158 milliseconds90.68% &lt;= 159 milliseconds90.95% &lt;= 160 milliseconds91.18% &lt;= 161 milliseconds91.41% &lt;= 162 milliseconds91.63% &lt;= 163 milliseconds91.82% &lt;= 164 milliseconds92.08% &lt;= 165 milliseconds92.35% &lt;= 166 milliseconds92.56% &lt;= 167 milliseconds92.80% &lt;= 168 milliseconds93.06% &lt;= 169 milliseconds93.30% &lt;= 170 milliseconds93.47% &lt;= 171 milliseconds93.65% &lt;= 172 milliseconds93.82% &lt;= 173 milliseconds93.94% &lt;= 174 milliseconds94.03% &lt;= 175 milliseconds94.16% &lt;= 176 milliseconds94.31% &lt;= 177 milliseconds94.45% &lt;= 178 milliseconds94.56% &lt;= 179 milliseconds94.71% &lt;= 180 milliseconds94.84% &lt;= 181 milliseconds94.98% &lt;= 182 milliseconds95.17% &lt;= 183 milliseconds95.36% &lt;= 184 milliseconds95.57% &lt;= 185 milliseconds95.72% &lt;= 186 milliseconds95.84% &lt;= 187 milliseconds95.99% &lt;= 188 milliseconds96.11% &lt;= 189 milliseconds96.23% &lt;= 190 milliseconds96.33% &lt;= 191 milliseconds96.42% &lt;= 192 milliseconds96.50% &lt;= 193 milliseconds96.56% &lt;= 194 milliseconds96.64% &lt;= 195 milliseconds96.72% &lt;= 196 milliseconds96.82% &lt;= 197 milliseconds96.92% &lt;= 198 milliseconds97.00% &lt;= 199 milliseconds97.12% &lt;= 200 milliseconds97.25% &lt;= 201 milliseconds97.36% &lt;= 202 milliseconds97.46% &lt;= 203 milliseconds97.57% &lt;= 204 milliseconds97.62% &lt;= 205 milliseconds97.70% &lt;= 206 milliseconds97.79% &lt;= 207 milliseconds97.87% &lt;= 208 milliseconds97.94% &lt;= 209 milliseconds98.03% &lt;= 210 milliseconds98.09% &lt;= 211 milliseconds98.18% &lt;= 212 milliseconds98.25% &lt;= 213 milliseconds98.31% &lt;= 214 milliseconds98.45% &lt;= 215 milliseconds98.61% &lt;= 216 milliseconds98.71% &lt;= 217 milliseconds98.83% &lt;= 218 milliseconds98.94% &lt;= 219 milliseconds99.00% &lt;= 220 milliseconds99.06% &lt;= 221 milliseconds99.12% &lt;= 222 milliseconds99.17% &lt;= 223 milliseconds99.25% &lt;= 224 milliseconds99.30% &lt;= 225 milliseconds99.37% &lt;= 226 milliseconds99.41% &lt;= 227 milliseconds99.45% &lt;= 228 milliseconds99.49% &lt;= 229 milliseconds99.54% &lt;= 230 milliseconds99.58% &lt;= 231 milliseconds99.61% &lt;= 232 milliseconds99.65% &lt;= 233 milliseconds99.69% &lt;= 234 milliseconds99.71% &lt;= 235 milliseconds99.75% &lt;= 236 milliseconds99.77% &lt;= 237 milliseconds99.79% &lt;= 238 milliseconds99.81% &lt;= 239 milliseconds99.83% &lt;= 240 milliseconds99.85% &lt;= 241 milliseconds99.86% &lt;= 242 milliseconds99.89% &lt;= 243 milliseconds99.91% &lt;= 244 milliseconds99.93% &lt;= 245 milliseconds99.94% &lt;= 246 milliseconds99.95% &lt;= 247 milliseconds99.96% &lt;= 248 milliseconds99.97% &lt;= 249 milliseconds99.97% &lt;= 250 milliseconds99.97% &lt;= 251 milliseconds99.97% &lt;= 252 milliseconds99.98% &lt;= 253 milliseconds99.98% &lt;= 254 milliseconds99.99% &lt;= 255 milliseconds100.00% &lt;= 256 milliseconds100.00% &lt;= 256 milliseconds3977.72 requests per secondMSET (10 keys): 0.00MSET (10 keys): 17769.78MSET (10 keys): 18903.59MSET (10 keys): 19545.57MSET (10 keys): 17979.59MSET (10 keys): 17032.03MSET (10 keys): 16139.05MSET (10 keys): 16069.64MSET (10 keys): 16504.64MSET (10 keys): 16250.43MSET (10 keys): 16364.31MSET (10 keys): 16976.48MSET (10 keys): 17136.78MSET (10 keys): 17587.36MSET (10 keys): 17400.22MSET (10 keys): 17532.25MSET (10 keys): 17647.90MSET (10 keys): 17896.02MSET (10 keys): 18023.20MSET (10 keys): 17993.77MSET (10 keys): 18080.70MSET (10 keys): 18315.56====== MSET (10 keys) ====== 100000 requests completed in 5.44 seconds 1000 parallel clients 3 bytes payload keep alive: 10.00% &lt;= 16 milliseconds0.02% &lt;= 17 milliseconds0.02% &lt;= 18 milliseconds0.06% &lt;= 19 milliseconds0.11% &lt;= 20 milliseconds0.55% &lt;= 21 milliseconds0.71% &lt;= 22 milliseconds0.92% &lt;= 23 milliseconds1.25% &lt;= 24 milliseconds1.72% &lt;= 25 milliseconds2.18% &lt;= 26 milliseconds3.26% &lt;= 27 milliseconds4.12% &lt;= 28 milliseconds5.34% &lt;= 29 milliseconds6.62% &lt;= 30 milliseconds8.05% &lt;= 31 milliseconds11.39% &lt;= 32 milliseconds13.92% &lt;= 33 milliseconds16.55% &lt;= 34 milliseconds19.13% &lt;= 35 milliseconds22.32% &lt;= 36 milliseconds25.54% &lt;= 37 milliseconds28.70% &lt;= 38 milliseconds31.06% &lt;= 39 milliseconds33.18% &lt;= 40 milliseconds35.37% &lt;= 41 milliseconds37.44% &lt;= 42 milliseconds39.52% &lt;= 43 milliseconds41.78% &lt;= 44 milliseconds43.92% &lt;= 45 milliseconds46.27% &lt;= 46 milliseconds48.48% &lt;= 47 milliseconds51.10% &lt;= 48 milliseconds53.91% &lt;= 49 milliseconds56.62% &lt;= 50 milliseconds58.84% &lt;= 51 milliseconds61.03% &lt;= 52 milliseconds63.40% &lt;= 53 milliseconds66.00% &lt;= 54 milliseconds68.37% &lt;= 55 milliseconds69.81% &lt;= 56 milliseconds71.07% &lt;= 57 milliseconds72.50% &lt;= 58 milliseconds74.15% &lt;= 59 milliseconds75.02% &lt;= 60 milliseconds76.12% &lt;= 61 milliseconds76.95% &lt;= 62 milliseconds77.87% &lt;= 63 milliseconds78.84% &lt;= 64 milliseconds80.20% &lt;= 65 milliseconds81.69% &lt;= 66 milliseconds82.63% &lt;= 67 milliseconds84.13% &lt;= 68 milliseconds85.93% &lt;= 69 milliseconds87.18% &lt;= 70 milliseconds88.80% &lt;= 71 milliseconds89.60% &lt;= 72 milliseconds90.18% &lt;= 73 milliseconds90.97% &lt;= 74 milliseconds91.49% &lt;= 75 milliseconds91.96% &lt;= 76 milliseconds92.36% &lt;= 77 milliseconds92.64% &lt;= 78 milliseconds92.98% &lt;= 79 milliseconds93.33% &lt;= 80 milliseconds93.74% &lt;= 81 milliseconds94.36% &lt;= 82 milliseconds94.85% &lt;= 83 milliseconds95.25% &lt;= 84 milliseconds95.72% &lt;= 85 milliseconds96.08% &lt;= 86 milliseconds96.68% &lt;= 87 milliseconds97.07% &lt;= 88 milliseconds97.46% &lt;= 89 milliseconds97.89% &lt;= 90 milliseconds98.19% &lt;= 91 milliseconds98.39% &lt;= 92 milliseconds98.66% &lt;= 93 milliseconds98.92% &lt;= 94 milliseconds99.05% &lt;= 95 milliseconds99.23% &lt;= 96 milliseconds99.37% &lt;= 97 milliseconds99.46% &lt;= 98 milliseconds99.50% &lt;= 100 milliseconds99.50% &lt;= 101 milliseconds99.51% &lt;= 102 milliseconds99.52% &lt;= 103 milliseconds99.53% &lt;= 106 milliseconds99.53% &lt;= 107 milliseconds99.66% &lt;= 108 milliseconds99.72% &lt;= 109 milliseconds99.78% &lt;= 110 milliseconds99.84% &lt;= 113 milliseconds99.85% &lt;= 114 milliseconds99.86% &lt;= 118 milliseconds99.86% &lt;= 119 milliseconds99.88% &lt;= 120 milliseconds99.92% &lt;= 121 milliseconds99.92% &lt;= 124 milliseconds99.92% &lt;= 125 milliseconds99.96% &lt;= 126 milliseconds99.97% &lt;= 129 milliseconds100.00% &lt;= 129 milliseconds18395.88 requests per second]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Laravel 5.x进阶笔记（二）]]></title>
    <url>%2F2017%2F08%2F01%2Fp%2FLaravel5.x%E8%BF%9B%E9%98%B6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[在浏览器中展示创建的Article列表在浏览器中展示创建的Article数据，需要使用Http请求Web服务器，Web服务器接收请求并将Article数据在视图页面中展示，即MVC。完成上述操作，离不开Laravel的路由功能，所谓路由就是请求的控制转发。Laravel把所有的请求地址预先配置在路由列表文件中learnlaravel5/app/Http/routes.php。描述路由的基本格式是： 1Route::get(&apos;/home&apos;, &apos;HomeController@index&apos;); Route类（\Illuminate\Support\Facades\Route）是Laravel框架提供的路由配置工具类，get标识http请求的Method（get/post/put/patch…），get函数的第一个参数是url格式定义，第二个参数用@分割成两部分，第一部分是Http请求的Controller类，第二部分标识Controller的具体执行方法，具体看HomeController类的内容描述： 123456789101112131415161718192021222324252627282930&lt;?phpnamespace App\Http\Controllers;use App\Article;use App\Http\Requests;use Illuminate\Http\Request;class HomeController extends \Illuminate\Routing\Controller&#123; /** * Create a new controller instance. * * @return void */ public function __construct() &#123; $this-&gt;middleware(&apos;auth&apos;); &#125; /** * Show the application dashboard. * * @return \Illuminate\Http\Response */ public function index() &#123; return view(&apos;home&apos;)-&gt;withArticles(Article::all());; &#125;&#125; _construct()是默认构造函数，函数体使用middleware(‘auth’)中间件，描述访问这个Controller需要登录验证。index()方法则是在路由中指定的执行方法，返回值是一个视图层对象view，视图的目录位置是模板文件learnlaravel5/resources/views/home.blade.php，view(‘home’)中的home与模板文件的第一个关键字相同，index函数指定了返回的视图文件，并把Article数据通过Eloquent方法取出，使用withArticles方法携带返回到视图页面，此处withArticles标识在页面可以通过articles对象取得所有返回的数据，依次类推如果是withAcls,则在页面上使用acls对象获取返回的数据。home.blade.php示例（php模板语法自行解决）： 12345678910111213141516171819202122232425@extends(&apos;layouts.app&apos;)@section(&apos;content&apos;) &lt;div id=&quot;title&quot; style=&quot;text-align: center;&quot;&gt; &lt;h1&gt;Learn Laravel 5&lt;/h1&gt; &lt;div style=&quot;padding: 5px; font-size: 16px;&quot;&gt;Learn Laravel 5&lt;/div&gt; &lt;/div&gt; &lt;hr&gt; &lt;div id=&quot;content&quot;&gt; &lt;ul&gt; @foreach ($articles as $article) &lt;li style=&quot;margin: 50px 0;&quot;&gt; &lt;div class=&quot;title&quot;&gt; &lt;a href=&quot;&#123;&#123; url(&apos;article/&apos;.$article-&gt;id) &#125;&#125;&quot;&gt; &lt;h4&gt;&#123;&#123; $article-&gt;title &#125;&#125;&lt;/h4&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;body&quot;&gt; &lt;p&gt;&#123;&#123; $article-&gt;body &#125;&#125;&lt;/p&gt; &lt;/div&gt; &lt;/li&gt; @endforeach &lt;/ul&gt; &lt;/div&gt;@endsection 通过浏览器输入http://localhost:1024/home，就能看到数据加载到页面。]]></content>
      <tags>
        <tag>PHP</tag>
        <tag>Laravel5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Laravel 5.x进阶笔记（一）]]></title>
    <url>%2F2017%2F08%2F01%2Fp%2FLaravel5.x%E8%BF%9B%E9%98%B6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[默认条件MacOS，&gt;=PHP5.4，翻墙环境（默认创建的项目会请求*.google.com）懂得PHP基础知识和MVC的基本架构，开发环境已经安装Laravel所需的命令环境。 配置composer中国镜像，使得本地开发环境下载依赖速度更快。1composer config -g repo.packagist composer https://packagist.phpcomposer.com 创建Laravel项目进入项目的工作空间目录 1cd ~/workspace 创建Laravel项目1composer -vvv create-project laravel/laravel learnlaravel5 5.2.31 项目创建完成，进入刚刚创建的项目，并使用PHP内置server服务启动项目，访问页面验证。 12cd learnlaravel5/publicphp -S 0.0.0.0:1024 页面效果使用1024端口访问的目的是Unix系统的动态端口的开始端口，不需要进行端口配置，就可以启动监听。如果单纯要学习Laravel或者PHP，请不要使用apache或者nginx这样的web服务器启动项目，对学习本身带来影响。 启用Laravel针对PHP5.4版本的trait特性实现用户注册登录的功能，实现简单的用户权限管理12cd .. ##cd ~/workspace/learnlaravel5php artisan make:auth 不需要停止php -S即可直接访问localhost:1024/login查看生成登录页面 Mysql创建数据库laravel5，用户名和密码均为root1create database laravel5 修改learnlaravel5项目的配置文件12cd ~/workspace/learnlaravel5open .env ##.env文件不存在，需要拷贝一份.env.example 修改配置项 123456DB_CONNECTION=mysqlDB_HOST=127.0.0.1DB_PORT=3306DB_DATABASE=laravel5DB_USERNAME=rootDB_PASSWORD=root 使用laravel默认的数据库描述文件创建表执行命令创建表 1php artisan migrate migrate命令要执行的php描述文件是learnlaravel5/database/migrations下的文件 2014_10_12_000000_create_users_table.php 2014_10_12_100000_create_password_resets_table.php再查看一下laravel5数据库中是否创建了这两个描述文件中的表，如果已经创建，说明命令执行成功，就可以用注册页面注册用户了，赶快试一试。使用 Laravel 的“葵花宝典”：EloquentEloquent是Laravel定义的Model基类，只要Larave创建Model就会继承Eloquent类，此时创建的Model就具有了十个异常强大的函数，从此想干啥事儿都是一行代码就搞定，创建一个Article的Model体验一下： 1php artisan make:model Article 创建Article对应的数据库描述文件 1php artisan make:migration create_article_table 打开刚刚创建的数据库迁移描述文件 1open learnlaravel5/database/migration/*_create_article_table.php 修改文件的内容为： 123456789101112131415161718192021222324252627282930313233343536&lt;?phpuse Illuminate\Database\Schema\Blueprint;use Illuminate\Database\Migrations\Migration;class CreateArticleTable extends Migration&#123; /** * Run the migrations. * * 创建文章表 * * @return void */ public function up() &#123; Schema::create(&apos;articles&apos;, function($table) &#123; $table-&gt;increments(&apos;id&apos;); $table-&gt;string(&apos;title&apos;); $table-&gt;text(&apos;body&apos;)-&gt;nullable(); $table-&gt;integer(&apos;user_id&apos;); $table-&gt;timestamps(); &#125;); &#125; /** * Reverse the migrations. * * @return void */ public function down() &#123; // &#125;&#125; 再次使用数据库迁移命令创建articles表 1php artisan migrate 执行完后，在laravel5的数据库中会创建出articles的表结构。使用Seeder往数据库中插入默认数据。首先创建一个Seeder，例如： 1php artisan make:seeder ArticleSeeder 在learnlaravel5/database/seeds/下打开并修改刚刚创建的ArticleSeeder.php文件，默认的run函数体为空，为其添加创建Article的数据脚本。 1234567891011public function run() &#123; DB::table(&apos;articles&apos;)-&gt;delete(); for ($i=0;$i&lt;10;$i++)&#123; \App\Article::create([ &apos;title&apos;=&gt;&apos;Title&apos;.$i, &apos;body&apos;=&gt;&apos;Body&apos;.$i, &apos;user_id&apos;=&gt;1 ]); &#125; &#125; 把ArticleSeeder注册到项目的数据库执行环境中。打开learnlaravel5/database/seeds/DatabaseSeeder.php，并修改run函数。 1234public function run() &#123; $this-&gt;call(ArticleSeeder::class); &#125; 由于learnlaravel5/database目录不能被composer自动加载，所有需要执行命令让这个目录加载待composer环境，保证代码能找到ArticleSeedr这个类。 1composer dump-autoload 然后再执行 1php artisan db:seed 执行完，此时在数据库laravel5的article表中会产生10条数据。 下一节再讲解如何展示插入的数据 &gt;&gt;&gt;Laravel 5.x进阶笔记（二）]]></content>
      <tags>
        <tag>PHP</tag>
        <tag>Laravel5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Laravel 5.x进阶笔记（二）]]></title>
    <url>%2F2017%2F08%2F01%2Fp%2FLaravel5.x%E8%BF%9B%E9%98%B6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"></content>
      <tags>
        <tag>PHP</tag>
        <tag>Laravel5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker清理命令]]></title>
    <url>%2F2017%2F05%2F21%2FDocker%E6%B8%85%E7%90%86%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[杀死所有正在运行的容器 docker kill $(docker ps -a -q) 删除所有已经停止的容器 docker rm $(docker ps -a -q) 删除所有未打 dangling 标签的镜像 docker rmi $(docker images -q -f dangling=true) 删除所有镜像 docker rmi $(docker images -q) 为这些命令创建别名 ~/.bash_aliases COMMOND 杀死所有正在运行的容器. alias dockerkill=&#39;docker kill $(docker ps -a -q)&#39; 删除所有已经停止的容器. alias dockercleanc=&#39;docker rm $(docker ps -a -q)&#39; 删除所有未打标签的镜像. alias dockercleani=&#39;docker rmi $(docker images -q -f dangling=true)&#39; 删除所有已经停止的容器和未打标签的镜像. alias dockerclean=&#39;dockercleanc || true &amp;&amp; dockercleani&#39;]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用wait(),notify()配合synchronized实现自定义阻塞队列MyBlockingQueue]]></title>
    <url>%2F2016%2F04%2F21%2F%E4%BD%BF%E7%94%A8wait-notify-%E9%85%8D%E5%90%88synchronized%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97MyBlockingQueue%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118package cn.klxx.multithread;import java.util.LinkedList;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;/** * @author ForwardLee * @description 使用synchronized关键字，wait()和notify()方法实现阻塞队列 */public class MyBlockingQueue &#123; //队列容器 private final LinkedList&lt;Object&gt; list = new LinkedList&lt;Object&gt;(); //队列大小,使用AtomicInteger可以保证在多线程获取队列大小时线程安全 private AtomicInteger count = new AtomicInteger(0); //队列最大长度 private int maxSize; //队列最小长度 private int minSize = 0; //实现业务模型的锁 private Object lock = new Object(); public MyBlockingQueue(int length)&#123; this.maxSize = length; &#125; /** * @description 队列存数据 * @param obj */ public void putObj(Object obj)&#123; synchronized (lock)&#123; if (count.get()==maxSize) &#123; try &#123; lock.wait();//队列长度已经达到最大值，不能再继续存储数据，则线程处于等待状态，直至队列收到有可用位置的通知。 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; list.push(obj); count.incrementAndGet(); lock.notify();//如果已经有线程等待获取队列数据，此时就通知取数据线程有数据了。 System.out.println(&quot;存入对象：&quot;+obj); &#125; &#125; /** * @description 获取队列中的数据 * @return 返回队列的第一个元素 */ public Object getObject()&#123; Object retVal = null; synchronized (lock)&#123; if (count.get()==0) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; retVal = list.getFirst(); count.decrementAndGet(); lock.notify();//队列长度小于maxSize了，有存储空间了，就通知存储线程可以放入数据了。 System.out.println(&quot;取出对象：&quot;+retVal); &#125; return retVal; &#125; public static void main(String[] args) &#123; final MyBlockingQueue myBlockingQuene = new MyBlockingQueue(5); new Thread(new Runnable() &#123; @Override public void run() &#123; Object var1 = myBlockingQuene.getObject(); &#125; &#125;,&quot;t1&quot;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; myBlockingQuene.putObj(&quot;abc1&quot;); myBlockingQuene.putObj(&quot;abc2&quot;); myBlockingQuene.putObj(&quot;abc3&quot;); myBlockingQuene.putObj(&quot;abc4&quot;); myBlockingQuene.putObj(&quot;abc5&quot;); myBlockingQuene.putObj(&quot;abc6&quot;); System.out.println(&quot;此时队列长度L1=&quot;+myBlockingQuene.count.get()); myBlockingQuene.putObj(&quot;abc7&quot;);//这个对象放不了，线程就处于等待状态了。 &#125; &#125;,&quot;t2&quot;).start(); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(new Runnable() &#123; @Override public void run() &#123; Object var2 = myBlockingQuene.getObject();//t2线程中的abc7对象此时才能放入队列。 System.out.println(&quot;此时队列长度L2=&quot;+myBlockingQuene.count.get()); &#125; &#125;,&quot;t3&quot;).start(); &#125;&#125;]]></content>
      <tags>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CountDownLatch多线程性能]]></title>
    <url>%2F2016%2F04%2F19%2Fjava%2FCountDownLatch%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%80%A7%E8%83%BD%2F</url>
    <content type="text"><![CDATA[cn.klxx.multithread;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129import java.util.ArrayList;import java.util.List;import java.util.concurrent.CountDownLatch;/** * Object方法wait()、notify()两个方法需要配合sychronized关键字使用， * wait()方法释放锁，notify()方法占有锁，所以要先调用wait()方法，再调用notify()方法 */public class MyCountDownLatch &#123;static List list = new ArrayList();public void addString(String a)&#123; list.add(a); System.out.println(&quot;list add string!&quot;);&#125;public int getSize()&#123; return list.size();&#125;static final Object lock = new Object(); public static void main(String[] args) &#123; final MyCountDownLatch myCountDownLatch = new MyCountDownLatch(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; synchronized (lock)&#123; for (int i = 0; i &lt; 10; i++) &#123; myCountDownLatch.addString(&quot;abc&quot;); Thread.sleep(500); if(list.size()==5)&#123; lock.notify(); System.out.println(&quot;发出通知...&quot;); &#125; &#125; System.out.println(&quot;执行完这句话释放锁...&quot;); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,&quot;t1&quot;) ; Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; synchronized (lock) &#123; if (list.size()!=5) &#123; lock.wait();//此时t2处于等待中，不再继续往下执行代码，直到收到通知才继续执行 &#125; System.out.println(&quot;t2 收到停止通知&quot;); throw new RuntimeException(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;,&quot;t2&quot;) ; /** * t2必须先启动，因为wait方法不会占有锁，不影响t1执行；如果t1先启动会占有锁，而t2则不能使用锁无法执行。 * 用wait和notify配合关键字sychronized关键字，使用等notify所在的sychronized关键字代码块执行完， * wait所在的sychronized代码块才会继续执行，这样的最大问题就是收到通知的时间严重滞后于发送通知的时间。 * 所以要借助CountDownLatch来解决这个问题，CountDownLatch使用countDown方法发送完通知后也会继续执行， * 但是await方法会马上收到通知继续执行代码，避免了收到通知的严重滞后问题。 */ t2.start(); t1.start(); try &#123; Thread.sleep(10000); list.clear(); System.out.println(&quot;-----------------------------&quot;); System.out.println(&quot;list.size:&quot;+list.size()); System.out.println(&quot;-----------------------------&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; final CountDownLatch countDownLatch = new CountDownLatch(1); Thread t3 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; for (int i = 0; i &lt; 10; i++) &#123; list.add(&quot;abc&quot;); System.out.println(&quot;list add string!&quot;); Thread.sleep(500); if (list.size()==5) &#123; System.out.println(&quot;发送通知&quot;); countDownLatch.countDown(); &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;t3&quot;); Thread t4 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; if (list.size()!=5) &#123; System.out.println(&quot;t4 等待...&quot;); countDownLatch.await(); &#125; System.out.println(&quot;list.size:&quot;+list.size()+&quot; ,t4 停止&quot;); throw new RuntimeException(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;t4&quot;); t3.start(); t4.start(); &#125;&#125;]]></content>
      <tags>
        <tag>JAVA</tag>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDKBlockingQueue使用]]></title>
    <url>%2F2016%2F04%2F10%2FJDKBlockingQueue%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140package cn.klxx.multithread;import org.junit.Test;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.LinkedBlockingDeque;import java.util.concurrent.PriorityBlockingQueue;import java.util.concurrent.SynchronousQueue;/** * @description JDK实现的阻塞和无阻塞队列测试 * @author ForwardLee */public class JDKBlockingQueueTest &#123; /** * 有界阻塞队列 */ @Test public void testArrayBlockingQueue() throws Exception &#123; ArrayBlockingQueue&lt;String&gt; arrayBlockingQueue = new ArrayBlockingQueue&lt;String&gt;(5); arrayBlockingQueue.add(&quot;a&quot;); arrayBlockingQueue.add(&quot;b&quot;); arrayBlockingQueue.add(&quot;c&quot;); arrayBlockingQueue.add(&quot;d&quot;); arrayBlockingQueue.add(&quot;e&quot;); arrayBlockingQueue.add(&quot;f&quot;); &#125; /** * @descripiton 可以是有界阻塞队列，也可以是无解阻塞队列，关键看实例化队列对象用的构造方法是否初始化队列大小 */ @Test public void testLinkedBlockingDeque() throws Exception &#123; LinkedBlockingDeque&lt;String&gt; linkedBlockingDeque = new LinkedBlockingDeque&lt;String&gt;();//new LinkedBlockingDeque&lt;String&gt;(2) linkedBlockingDeque.add(&quot;a&quot;); linkedBlockingDeque.offer(&quot;b&quot;); linkedBlockingDeque.add(&quot;c&quot; ); &#125; /** * @description 同步阻塞队列，主要应用场景在于多线程之间的线程切换，例如线程池的实现。 * 不能直接往队列中存数据。要往队列中存数据，前提是必须要有一个线程等待取数据 * Executors.newCachedThreadPool() * * Creates a thread pool that creates new threads as needed, but * will reuse previously constructed threads when they are * available, and uses the provided * ThreadFactory to create new threads when needed. * &#123;@param threadFactory the factory to use when creating new threads * @return the newly created thread pool * @throws NullPointerException if threadFactory is null&#125; * * public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) &#123; * return new ThreadPoolExecutor(0, Integer.MAX_VALUE, * 60L, TimeUnit.SECONDS, * new SynchronousQueue&lt;Runnable&gt;(), * threadFactory); * &#125; */ @Test public void testSynchronousQueue() throws Exception &#123; final SynchronousQueue&lt;String&gt; synchronousQueue = new SynchronousQueue&lt;String&gt;(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; String peek = synchronousQueue.take(); System.out.println(peek); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); new Thread(new Runnable() &#123; public void run() &#123; synchronousQueue.add(&quot;a&quot;); &#125; &#125;).start(); &#125; /** * PriorityBlockingQueue是带优先级的无界阻塞队列，每次出队都返回优先级最高的元素， * 是二叉树最小堆的实现，研究过数组方式存放最小堆节点的都知道，直接遍历队列元素是无序的。 * 队列存储的对象必须实现Comparable接口 * @throws Exception */ @Test public void testPriorityBlockingQueue() throws Exception &#123; PriorityBlockingQueue&lt;ObjectWithComparable&gt; priorityBlockingQueue= new PriorityBlockingQueue&lt;ObjectWithComparable&gt;(); ObjectWithComparable o1 = new ObjectWithComparable(); o1.setId(3); priorityBlockingQueue.add(o1); ObjectWithComparable o2 = new ObjectWithComparable(); o2.setId(4); priorityBlockingQueue.add(o2); ObjectWithComparable o3 = new ObjectWithComparable(); o3.setId(2); priorityBlockingQueue.add(o3); for (ObjectWithComparable objectWithComparable : priorityBlockingQueue) &#123; System.out.println(priorityBlockingQueue.take()); &#125; &#125; private class ObjectWithComparable implements Comparable&lt;ObjectWithComparable&gt;&#123; private int id; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; @Override public int compareTo(ObjectWithComparable o) &#123; return this.id&gt;o.id? 1:(this.id&lt;o.id? -1:0); &#125; @Override public String toString() &#123; return &quot;ObjectWithComparable&#123;&quot; + &quot;id=&quot; + id + &apos;&#125;&apos;; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>JAVA</tag>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github仓库创建与管理]]></title>
    <url>%2F2015%2F12%2F03%2FGithub%E4%BB%93%E5%BA%93%E5%88%9B%E5%BB%BA%E4%B8%8E%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[github使用手册【转】http://www.runoob.com/w3cnote/git-guide.html Step1：在github网站创建仓库创建完，找到仓库地址：https://github.com/forworldlee/abc.git Step2：同步仓库在本地创建一个abc的目录，或者从github上克隆abc仓库到本地目录。代码同步 1234567cd abcgit initgit add README.mdgit commit -m &quot;first commit&quot;git remote add origin git@github.com:forworldlee/abc.gitgit push -u origin master 本地目录中已经存在文件，可以使用git add . 代替git add README.md增加所有文件。]]></content>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis主从复制丢失数据问题]]></title>
    <url>%2F2015%2F01%2F20%2FRedis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E4%B8%A2%E5%A4%B1%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[master重要配置min-slaves-to-write 1min-slaves-max-lag 10 两个配置一起解释，至少有一个slave节点的数据同步延迟不能超过10s，否则master将暂停接收新数据的写入操作。这样能保证在主从复制过程中出现问题，master节点丢失的数据量可控。 场景一：主从异步复制导致的数据丢失场景：master接收到新数据，还没有把新数据异步传输给slave，此时master宕机，那么master在内存中接收到的新数据将会丢失。 场景二：哨兵机制下（Sentinel）集群主从节点发生脑裂场景：master节点与slave节点断开网络连接，但是master的client端依然与master正常通信，此时数据还一直往master节点写入，哨兵（Sentinel）认为master已经挂掉，哨兵会进行主从转换，选举出一个slave节点转换为master节点接收数据，而原来的master在恢复网络连接后会变成slave从新的master中同步数据，那么这就导致原来master在断开与slave的连接后，接收到client端的新数据被丢弃，造成数据丢失。 解决方案依赖以上两个配置，控制数据丢失的数量。如果发生master暂停写入新数据，那么redis的client端可以进行如下操作，保证服务的可用性：（1）client端做降级处理，把数据写入本地磁盘，等master能够写操作时写入master；然后对client对外提供服务的接口做降级处理，减少client接收到的请求数量，降低redis的并发写入量。（2）client端将数据写入异步消息队列，每隔一段时间把数据从消息队列中取出，再写回到master当中。]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis在CentOS6上安装并配置主从]]></title>
    <url>%2F2015%2F01%2F05%2FRedis%E5%9C%A8CentOS6%E4%B8%8A%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AE%E4%B8%BB%E4%BB%8E%2F</url>
    <content type="text"><![CDATA[安装Redisstep1：tcl工具包安装 1yum install -y install tcl step2：Redis安装 1234wget http://download.redis.io/releases/redis-2.8.13.tar.gztar -zvxf redis-2.8.13.tar.gzcd redis-2.8.13make &amp; make install Redis启动和主从配置1234mkdir /etc/redismkdir -pv /var/redis/6379mv redis.conf /etc/redis/6379.confvi /etc/redis/6379.conf 单机启动配置 12345678910#守护进程启动daemonize yes#进程id文件pidfile /var/run/redis_6379.pid#数据文件目录dir /var/redis/6379#redis节点绑定ip访问bind 192.168.1.212#redis节点访问端口port 6379 设置redis随系统自启动 12cp utils/redis_init_script /etc/init.d/redis_6379vi /etc/init.d/redis_6379 在redis_6379文件顶部添加 123#chkconfig: 2345 90 10#description: Redis is a persistent key-value databasechkconfig redis_6379 on 多节点主从配置 1234567891011121314151617181920212223242526#master节点信息slaveof 192.168.1.210 6379#master节点连接密码masterauth redis-pass#slave节点只读slave-read-only yes#slave节点连接master节点进行full synchronizaton操作时，mastr的rdb快照文件直接写入slave socket，不写入磁盘，也就是无磁盘化复制。无磁盘化配置no，磁盘化配置yes。repl-diskless-sync no#master延迟n秒后，进行无磁盘化full synchronization操作，目的是等待更多的slave节点连接master进行全量同步复制。全量数据的RDB文件一旦在master节点缓存内创建完成，就不再接收新的slave，直至本次全量同步完成。repl-diskless-sync-delay 5#主从复制的超时设置，超时则停止复制，根据RDB数据大小、硬件IO、带宽设置超时时间。默认60s。repl-timeout 60 #backlog用在slave如果断开了与master之间的连接，这时master记录一部分要写入slave的数据和offset位置，当slave重连master后，不需要进行full resync操作，只需要从slave断开连接丢失的数据开始同步即可。默认缓冲大小1M。repl-backlog-size 1mb#至少有3个slave节点数据同步延时少于10s，否则master将暂停写操作。此配置主要用来保证主从异步复制数据和redis集群脑裂时，确保数据不丢失的配置方案。配合master在不能写入数据时进行异步写队列或者磁盘，保证数据丢失在可控范围内。min-slaves-to-write 3min-slaves-max-lag 10#redis节点连接密码requirepass redis-pass#开启AOF数据持久化appendonly yes#每秒进行一次从os cache写aof文件操作appendfsync everysec#aof文件rewrite操作时机配置，当aof的文件大小已经超过64m的100%，也就是128m时，就进行rewrite aof文件。auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 开发环境设置防火墙或者关闭防火墙 1service iptables stop 启动、访问、测试 12345678910111213141516171819202122cd /etc/init.d./redis_6379 startredis-cli -h 192.168.1.212 -a redis-pass192.168.1.212:6379&gt; info replication# Replicationrole:slavemaster_host:192.168.1.210master_port:6379master_link_status:upmaster_last_io_seconds_ago:8master_sync_in_progress:0slave_repl_offset:180178228slave_priority:100slave_read_only:1connected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
</search>
